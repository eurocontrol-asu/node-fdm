{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>   A physics-guided Neural Ordinary Differential Equation (Neural ODE) framework for aircraft flight dynamics. </p> <p> Get Started View Examples </p>"},{"location":"#at-a-glance","title":"\ud83c\udfaf At a Glance","text":"<p>node-fdm bridges the gap between deep learning and aeronautics. It allows you to compose hybrid dynamical models by stacking physical principles, analytical features, and neural networks.</p> <p>The diagram below illustrates the standard architecture used for ADS-B data (OpenSky 2025), where an analytical layer pre-processes physical features before feeding them into a neural network:</p> <pre><code>graph LR\n    subgraph Inputs [\"System Inputs\"]\n        direction TB\n        X((State x))\n        U((Control u))\n        E((Context e))\n    end\n\n    subgraph Core [\"Core Blocks\"]\n        direction LR\n        B1[Analytical Layer]\n        B2[Neural Net Layer]\n    end\n\n    subgraph Solver [\"Temporal Integration\"]\n        direction TB\n        DX((Derivative dx/dt))\n        ODE[ODE Solver]\n    end\n\n    %% Connexions (Ordre strict pour l'index linkStyle)\n    %% Index 0, 1, 2\n    X --&gt; B1\n    U --&gt; B1\n    E --&gt; B1\n    %% Index 3\n    B1 --&gt; B2\n    %% Index 4\n    B2 --&gt; DX\n    %% Index 5\n    DX --&gt; ODE\n\n    %% Index 6 : Feedback Loop (Cible pour le style rouge)\n    ODE -.-&gt;|Loss| X\n\n    %% Styles\n    classDef cInput fill:#9ECAE9,stroke:#333,stroke-width:2px,color:black\n    classDef cControl fill:#FF9D98,stroke:#333,stroke-width:2px,color:black\n    classDef cContext fill:#88D27A,stroke:#333,stroke-width:2px,color:black\n    classDef cAnalytics fill:#F2CF5B,stroke:#333,stroke-width:2px,color:black\n    classDef cNeural fill:#83BCB6,stroke:#333,stroke-width:2px,color:black\n    classDef cDerivative fill:#D6A5C9,stroke:#333,stroke-width:2px,color:black\n\n    class X cInput\n    class U cControl\n    class E cContext\n    class B1,ODE cAnalytics\n    class B2 cNeural\n    class DX cDerivative\n\n    %% Application du style rouge sur le lien d'index 6\n    linkStyle 6 stroke:red,stroke-width:2px,stroke-dasharray: 5 5,color:red</code></pre>"},{"location":"#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Reconstruct Trajectories: Generate coherent flight paths from ADS-B or QAR data.</li> <li>Physics-Aware: Simulate behavior using latent dynamics constrained by aeronautical laws.</li> <li>Ready-to-Use: Includes architectures for OpenSky 2025 and QAR.</li> <li>Benchmark Ready: Compare directly against physical models like BADA.</li> </ul>"},{"location":"#workflow-navigation","title":"\ud83d\ude80 Workflow &amp; Navigation","text":"<p>Follow the pipelines mirrored in the repository layout.</p> <ul> <li> <p> Quickstart</p> <p>Get started with the essentials.</p> <ul> <li>Installation</li> <li>Core Concepts</li> <li>Pipelines Overview</li> </ul> </li> <li> <p> How to</p> <p>Configure and customize your project.</p> <ul> <li>Configure Project</li> <li>Create Architecture</li> <li>Train a Model</li> <li>Run Inference</li> <li>Contribute</li> </ul> </li> <li> <p> API Reference</p> <p>Technical documentation for developers.</p> <ul> <li>Overview</li> <li>Architectures </li> <li>Trainer &amp; Predictor</li> <li>Data &amp; Models</li> </ul> </li> </ul>"},{"location":"#quick-install","title":"\u26a1 Quick Install","text":"<p>You can install the core package directly via pip:</p> <pre><code>pip install node-fdm\n# Or for editable research mode:\npip install -e .[dev]\n</code></pre> <p>Legal Notice</p> <p>This project is intended for research purposes only.</p> <p>This project is distributed under the EUPL-1.2 license with specific EUROCONTROL amendments. It must not be used as a regulatory or operational tool under any circumstances. See <code>AMENDMENT_TO_EUPL_license.md</code> for details.</p>"},{"location":"howto/configure_params/","title":"\u2699\ufe0f Configure Project Paths and Options","text":"<p>Runtime settings are centralized in YAML files, defining project paths, data scope, and feature flags per pipeline.</p>"},{"location":"howto/configure_params/#configuration-files","title":"\ud83d\udcc2 Configuration Files","text":"<p>All configuration settings are defined per pipeline. Edit the file corresponding to your use case:</p> <ul> <li>\ud83d\udce1 OpenSky 2025: <code>scripts/opensky/config.yaml</code></li> <li>\u2708\ufe0f QAR (Private): <code>scripts/qar/config.yaml</code></li> </ul>"},{"location":"howto/configure_params/#configuration-structure-example","title":"\ud83d\udcdd Configuration Structure Example","text":"<p>This example shows the primary fields in the OpenSky configuration.</p> scripts/opensky/config.yaml<pre><code>paths:\n  data_dir: \"/path/to/data\"\n  download_dir: \"downloaded_parquet\"\n  preprocess_dir: \"preprocessed_parquet\"\n  # ... (more directories)\n  era5_cache_dir: \"era5_cache\"\n\nera5_features:\n  - u_component_of_wind\n  - v_component_of_wind\n  - temperature\n\ntypecodes:\n  - A320\n  - A20N\n  # ...\n\nbada:\n  bada_4_2_dir: \"/path/to/BADA/4.2.1\"\n</code></pre>"},{"location":"howto/configure_params/#key-parameters-and-best-practices","title":"\ud83d\udd11 Key Parameters and Best Practices","text":"Section Parameter Type Best Practice / Description Paths <code>data_dir</code> Path Crucial: Keep this path absolute. All subfolders (<code>download_dir</code>, <code>models_dir</code>, etc.) are resolved relative to this root. Paths <code>era5_cache_dir</code> Path Path for local cache of meteorological fields. Setting this prevents re-downloading large files. Scope <code>typecodes</code> List Single Source: Adjust aircraft type scope here, not by modifying pipeline scripts. BADA <code>bada_4_2_dir</code> Path Set this only if you plan to run baseline evaluation (<code>07_bada_prediction.py</code>). ERA5 <code>era5_features</code> List Defines the specific meteorological fields (wind components, temperature) to be used as exogenous inputs. <p>QAR Pipeline Variations</p> <p>The QAR configuration is minimal. It typically only retains essential paths (<code>data_dir</code>, <code>predicted_dir</code>, <code>models_dir</code>), <code>typecodes</code>, and options for parallel processing (<code>computing.default_cpu_count</code>).</p> <p>Directory Existence</p> <p>Ensure your main directories exist before running data downloads or preprocessing scripts. <pre><code>mkdir -p /path/to/data/\n</code></pre></p>"},{"location":"howto/configure_params/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<ul> <li>Create an Architecture: Now that paths are configured, learn how to build the model's core components.</li> </ul>"},{"location":"howto/contribute/","title":"Contributing to node-fdm","text":"<p>Thank you for your interest in contributing to node-fdm!</p> <p>We aim to provide a robust, physics-guided framework for flight dynamics research. Whether you are a researcher correcting a physical formula, a data scientist adding a new preprocessing hook, or a developer fixing a bug, your help is welcome.</p> <p>We welcome contributions in several forms:</p> <ol> <li>Bug Reports &amp; Issues</li> <li>Documentation Improvements</li> <li>New Architectures (Adapting the model to new aircraft or datasets)</li> <li>Core Improvements (Solvers, Physics Layers)</li> </ol>"},{"location":"howto/contribute/#bug-reports","title":"\ud83d\udc1b Bug Reports","text":"<p>Please file bug reports on the GitHub Issue Tracker.</p> <p>When filing a report, please include:</p> <ul> <li>Description: A clear summary of the issue.</li> <li>Context: Which pipeline were you running? (e.g., OpenSky 2025 or QAR).</li> <li>Configuration: Relevant parts of your <code>config.yaml</code> (especially <code>model_config</code>).</li> <li>Logs: The full traceback or error message.</li> <li>Environment: Your Python version, OS, and whether you are using GPU/CUDA.</li> </ul> <p>Note on Data Privacy: If the error occurs on a private QAR dataset, do not upload the data samples. Try to reproduce the issue with synthetic data or the public OpenSky sample if possible.</p>"},{"location":"howto/contribute/#development-workflow","title":"\ud83d\udee0\ufe0f Development Workflow","text":"<p>To contribute code, you need a local development environment.</p> <p>1. Fork and Clone Fork the repository on GitHub, then clone your fork locally: <pre><code>git clone [https://github.com/](https://github.com/)&lt;your-username&gt;/node-fdm.git\ncd node-fdm\n</code></pre></p> <p>2. Install in Editable Mode We recommend using a virtual environment (venv, conda, or uv). Install the package with all development dependencies: <pre><code>pip install -e .[all]\n</code></pre></p> <p>3. Run Tests Ensure the current codebase is stable before making changes. <pre><code>pytest tests/\n</code></pre></p>"},{"location":"howto/contribute/#pull-requests-pr","title":"\ud83d\udce5 Pull Requests (PR)","text":"<ol> <li>Create a new branch for your feature or fix: <code>git checkout -b feature/my-new-architecture</code>.</li> <li>Make your changes and commit them with a clear, descriptive message.</li> <li>Push to your fork and submit a Pull Request against the <code>main</code> branch of <code>eurocontrol-asu/node-fdm</code>.</li> <li>In the PR description, reference any related Issues (e.g., \"Fixes #42\").</li> </ol>"},{"location":"howto/contribute/#contributing-new-architectures","title":"\ud83c\udfd7\ufe0f Contributing New Architectures","text":"<p>The most common way to extend node-fdm is by adding support for a new aircraft type or a new data source. We call these Architectures.</p> <p>Instead of modifying the core engine, you should create a self-contained module in <code>node_fdm/architectures/</code>.</p> <p>Steps to contribute an architecture: 1.  Duplicate a Template: Copy <code>opensky_2025</code> (for sparse data) or <code>qar</code> (for rich data) into a new folder. 2.  Define Columns: Update <code>columns.py</code> to map your specific data inputs to SI units. 3.  Implement Logic: Adjust <code>flight_process.py</code> (cleaning) and <code>model.py</code> (layer stack). 4.  Register: Add your architecture to <code>node_fdm/architectures/mapping.py</code>.</p> <p>\ud83d\udcd8 Documentation Reference: Please strictly follow the Create an Architecture guide in the documentation to ensure your contribution is compatible with the <code>ODETrainer</code>.</p>"},{"location":"howto/contribute/#documentation","title":"\ud83d\udcda Documentation","text":"<p>Documentation is built with MkDocs Material. If you modify code, please update the docstrings (we use <code>mkdocstrings</code> to auto-generate API references).</p> <p>To preview documentation changes locally: <pre><code>pip install mkdocs-material mkdocstrings[python]\nmkdocs build --clean\nmkdocs serve\n</code></pre> Then open <code>http://127.0.0.1:8000/node-fdm/</code> in your browser.</p>"},{"location":"howto/contribute/#style-guide","title":"\ud83c\udfa8 Style Guide","text":"<p>We do not want to be overly strict, but consistency helps review. * Python: Follow PEP 8. We recommend running <code>ruff</code> or <code>black</code> before committing. * Type Hints: Please use Python type hints (<code>def func(df: pd.DataFrame) -&gt; torch.Tensor:</code>) for all core functions. * Existing Code: Avoid reformatting unrelated existing code, as this makes diffs harder to read.</p>"},{"location":"howto/contribute/#license-proprietary-data","title":"\u2696\ufe0f License &amp; Proprietary Data","text":"<ul> <li>License: Contributions are accepted under the EUPL-1.2 license.</li> <li>Proprietary Data: Never commit QAR files, BADA model files, or any other proprietary data to the repository. The <code>.gitignore</code> is set up to exclude <code>data/</code>, but please be vigilant.</li> </ul>"},{"location":"howto/create_architecture/","title":"\ud83c\udfd7\ufe0f Create a New Architecture","text":"<p>Follow these steps to add and register a new architecture to the framework (see README guidance).</p>"},{"location":"howto/create_architecture/#1-copy-a-skeleton","title":"1. Copy a Skeleton","text":"<p>Start by duplicating an existing architecture folder into <code>node_fdm/architectures/&lt;your_arch&gt;</code>.</p> <ul> <li>Minimal start: Copy <code>node_fdm/architectures/opensky_2025</code>.</li> <li>Advanced stack: Copy <code>node_fdm/architectures/qar</code> (includes stacked layers).</li> </ul> <p>Required Files: * <code>columns.py</code> * <code>flight_process.py</code> * <code>model.py</code> * <code>__init__.py</code> (plus any extra layer files).</p>"},{"location":"howto/create_architecture/#2-declare-columns-columnspy","title":"2. Declare Columns (<code>columns.py</code>)","text":"<p>Define the variable groups using <code>utils.data.column.Column</code>. Ensure you specify units to avoid silent scale bugs.</p> <ul> <li><code>X_COLS</code>: State variables (ODE inputs/outputs).</li> <li><code>U_COLS</code>: Control variables.</li> <li><code>E0_COLS</code>: Environmental inputs.</li> <li><code>E1_COLS</code>: Derived outputs.</li> <li><code>DX_COLS</code>: Derivatives predicted by the ODE layer.</li> </ul> <p>Target Matching</p> <p>Make sure your derivative columns (<code>DX_COLS</code>) exactly match the ODE targets you want the model to learn.</p>"},{"location":"howto/create_architecture/#3-custom-preprocessing-flight_processpy","title":"3. Custom Preprocessing (<code>flight_process.py</code>)","text":"<p>Implement the data preparation logic.</p> <ul> <li><code>flight_processing(df)</code>: Augment raw data (e.g., add derived columns, apply smoothing). Expose specific configs here (e.g., <code>selected_param_config</code>).</li> <li><code>segment_filtering(df, start_idx, seq_len)</code> (Optional): Reject bad segments (e.g., segments with large distance jumps) before they reach the trainer.</li> </ul>"},{"location":"howto/create_architecture/#4-wire-the-model-modelpy","title":"4. Wire the Model (<code>model.py</code>)","text":"<p>This is where you define the architecture stack.</p> <ol> <li>Build Column Lists: Build <code>X_COLS</code>, <code>U_COLS</code>, <code>E0_COLS</code>, <code>E1_COLS</code>, <code>DX_COLS</code>, and <code>MODEL_COLS</code>.</li> <li>Define Layers: Define layers (e.g., a physics/feature layer, then an ODE/data layer) and assemble <code>ARCHITECTURE</code> as a list of layer specs.</li> </ol> <p>Column Ordering</p> <p>Set <code>MODEL_COLS</code> carefully. It must match the ordering expected by <code>FlightProcessor</code> and <code>SeqDataset</code>.</p>"},{"location":"howto/create_architecture/#5-add-custom-layers","title":"5. Add Custom Layers","text":"<p>If your model requires specific physics or feature logic: 1.  Place the files in your architecture folder (e.g., <code>trajectory_layer.py</code>, <code>engine_layer.py</code>). 2.  Export them via <code>__init__.py</code> if external imports are needed.</p>"},{"location":"howto/create_architecture/#6-register-the-name","title":"6. Register the Name","text":"<p>To make your architecture discoverable, edit <code>architectures/mapping.py</code>.</p> <ol> <li>Add your unique key to <code>valid_names</code>.</li> <li>Ensure <code>get_architecture_module</code> correctly imports your <code>columns</code>, <code>flight_process</code>, and <code>model</code> modules.</li> </ol>"},{"location":"howto/create_architecture/#7-test-a-tiny-run","title":"7. Test a Tiny Run","text":"<p>Before launching a full training job, verify the integration:</p> <ul> <li>Data preparation: Prepare a minimal processed dataset conforming to your new columns.</li> <li>Training loop: Run a short training with <code>ODETrainer</code> (using small <code>seq_len</code> and <code>batch_size</code>) to check tensor shapes and statistics.</li> <li>Inference flow: Verify <code>NodeFDMPredictor</code> using your <code>MODEL_COLS</code> and <code>flight_processing</code>.</li> </ul>"},{"location":"howto/create_architecture/#pro-tips","title":"\ud83d\udca1 Pro Tips","text":"<p>Best Practices</p> <ul> <li>Consistency: Keep column names consistent between preprocessing and model definitions.</li> <li>Units: Use <code>Column</code> units definition strictly to prevent silent scale bugs.</li> <li>Config: Update the relevant pipeline <code>config.yaml</code> (paths, <code>typecodes</code>) if your dataset layout differs from the standard pipelines.</li> </ul>"},{"location":"howto/create_architecture/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<ul> <li>Train a Model: Now that your architecture is registered, train it.</li> </ul>"},{"location":"howto/run_inference/","title":"\ud83d\udd2e Run Inference","text":"<p>Once a model is trained, the <code>NodeFDMPredictor</code> allows you to perform trajectory rollouts (simulations) on new data. It reconstructs the flight path by integrating the Neural ODE over time.</p>"},{"location":"howto/run_inference/#requirements","title":"\ud83d\udccb Requirements","text":"<p>To run inference, the predictor needs three things:</p> <ol> <li>Architecture Definition: The <code>MODEL_COLS</code> and <code>flight_processing</code> hooks to prepare data exactly as the model expects.</li> <li>Model Artifacts: A directory containing <code>meta.json</code> (for scaling/hyperparams) and the layer weights (<code>*.pt</code>).</li> <li>Input Data: A processed parquet file or DataFrame compatible with the architecture.</li> </ol>"},{"location":"howto/run_inference/#single-flight-inference","title":"\u26a1 Single Flight Inference","text":"<p>This script demonstrates how to load a trained model and predict a single trajectory.</p> scripts/predict_single.py<pre><code>import json\nimport pandas as pd\nimport yaml\nfrom pathlib import Path\n\nfrom node_fdm.predictor import NodeFDMPredictor\nfrom node_fdm.data.flight_processor import FlightProcessor\nfrom node_fdm.architectures import mapping\n\n# 1. Setup Paths &amp; Config\n# Assumes running from repo root\nwith open(\"scripts/opensky/config.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\npaths = cfg[\"paths\"]\nprocess_dir = Path(paths[\"data_dir\"]) / paths[\"process_dir\"]\nmodels_dir = Path(paths[\"data_dir\"]) / paths[\"models_dir\"]\n\n# 2. Identify the Architecture\n# We read meta.json first to know WHICH architecture to load\ntarget_model_folder = models_dir / \"opensky_2025_A320\"  # Adjust folder name\nmeta_path = target_model_folder / \"meta.json\"\n\nif not meta_path.exists():\n    raise FileNotFoundError(f\"Missing meta.json in {target_model_folder}\")\n\nmeta = json.loads(meta_path.read_text())\narch_name = meta[\"architecture_name\"]\ntime_step = meta.get(\"step\", 4.0)\n\n# 3. Load Architecture Modules\n# Dynamically fetch columns and hooks based on the name found in meta.json\n_, model_cols, hooks = mapping.get_architecture_from_name(arch_name)\ncustom_processing_fn, _ = hooks\n\n# 4. Prepare Input Data\n# Load a raw parquet file and apply the architecture's preprocessing\nflight_path = process_dir / \"A320\" / \"sample_flight.parquet\"\nraw_df = pd.read_parquet(flight_path)\n\nprocessor = FlightProcessor(\n    model_cols, \n    custom_processing_fn=custom_processing_fn\n)\n# Returns a standardized Flight object\nprocessed_flight = processor.process_flight(raw_df)\n\n# 5. Initialize Predictor\npredictor = NodeFDMPredictor(\n    model_cols=model_cols,\n    model_path=target_model_folder,\n    dt=time_step,\n    device=\"cuda:0\",  # Use \"cpu\" if GPU is unavailable\n)\n\n# 6. Run Prediction\n# Integrates the ODE and returns a DataFrame with predictions\npred_df = predictor.predict_flight(processed_flight)\n\nprint(f\"Prediction complete. Shape: {pred_df.shape}\")\nprint(pred_df.head())\n</code></pre>"},{"location":"howto/run_inference/#output-format","title":"\ud83d\udcca Output Format","text":"<p>The <code>predict_flight</code> method returns a DataFrame containing: * Original Columns: All input columns required by the model. * Predicted Columns: Columns prefixed with <code>pred_</code> (e.g., <code>pred_alt</code>, <code>pred_tas</code>, <code>pred_lat</code>, <code>pred_lon</code>).</p> <p>Visualization</p> <p>You can immediately plot <code>alt</code> vs <code>pred_alt</code> to verify the model's accuracy on this specific flight.</p>"},{"location":"howto/run_inference/#batch-processing","title":"\ud83d\udd04 Batch Processing","text":"<p>For processing an entire test set, it is inefficient to re-initialize the <code>NodeFDMPredictor</code> loop inside the loop. Instead, initialize it once and iterate over your file list.</p> <pre><code># Load Split File\nsplit_df = pd.read_csv(process_dir / \"dataset_split.csv\")\ntest_files = split_df[split_df.split == \"test\"].filepath.tolist()\n\nresults = []\n\nfor rel_path in test_files:\n    # Construct full path\n    fpath = process_dir / rel_path\n\n    # Process\n    raw_df = pd.read_parquet(fpath)\n    flight_obj = processor.process_flight(raw_df)\n\n    # Predict (Reuse the predictor instance!)\n    pred = predictor.predict_flight(flight_obj)\n\n    # Save or Analyze\n    output_path = fpath.with_name(f\"{fpath.stem}_pred.parquet\")\n    pred.to_parquet(output_path)\n</code></pre>"},{"location":"howto/train_model/","title":"\ud83c\udfcb\ufe0f Train a Model","text":"<p>The <code>ODETrainer</code> class is the engine of node-fdm. It manages data loading, the Neural ODE integration loop, loss calculation, and checkpointing.</p>"},{"location":"howto/train_model/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<p>Before starting the training loop, ensure you have:</p> <ol> <li>Processed Data: Parquet files generated by the preprocessing pipeline (see Pipelines).</li> <li>Split File: A <code>dataset_split.csv</code> containing at least <code>filepath</code>, <code>aircraft_type</code>, and <code>split</code> columns.</li> <li>Registered Architecture: The architecture name (e.g., <code>opensky_2025</code>) must be registered in <code>architectures.mapping</code>.</li> </ol>"},{"location":"howto/train_model/#minimal-training-script","title":"\u26a1 Minimal Training Script","text":"<p>This script loads the configuration, filters the dataset for a specific aircraft type (e.g., A320), and launches the training.</p> scripts/train_custom.py<pre><code>import pandas as pd\nimport yaml\nfrom pathlib import Path\nfrom node_fdm.ode_trainer import ODETrainer\n\n# 1. Load Project Configuration\n# Assumes running from repository root\nwith open(\"scripts/opensky/config.yaml\") as f:\n    cfg = yaml.safe_load(f)\n\npaths = cfg[\"paths\"]\nprocess_dir = Path(paths[\"data_dir\"]) / paths[\"process_dir\"]\nmodels_dir = Path(paths[\"data_dir\"]) / paths[\"models_dir\"]\n\n# 2. Select Data Scope\nTARGET_ACFT = \"A320\"\nTARGET_ARCH = \"opensky_2025\"  # Must match architectures.mapping.valid_names\n\n# Load the split file and filter for the target aircraft\nsplit_df = pd.read_csv(process_dir / \"dataset_split.csv\")\ndata_df = split_df[split_df.aircraft_type == TARGET_ACFT]\n\n# 3. Define Hyperparameters\n# These controls how data is sliced for the ODE\ntrain_config = dict(\n    architecture_name=TARGET_ARCH,\n    model_name=f\"{TARGET_ARCH}_{TARGET_ACFT}\",\n    step=4,                       # Sampling period (seconds)\n    shift=60,                     # Stride between windows\n    seq_len=60,                   # Context window length (steps)\n    lr=1e-3,                      # Learning Rate\n    weight_decay=1e-4,\n    model_params=[3, 2, 48],      # Architecture-specific args (see model.py)\n    loading_args=(False, False),  # (Load Weights, Load Optimizer)\n    batch_size=512,\n    num_workers=4,\n)\n\n# 4. Initialize Trainer\ntrainer = ODETrainer(\n    data_df=data_df,\n    model_config=train_config,\n    model_dir=models_dir,\n    num_workers=train_config[\"num_workers\"],\n    load_parallel=True,           # Use multi-CPU\n)\n\n# 5. Execute Training Loop\ntrainer.train(\n    epochs=10,\n    batch_size=train_config[\"batch_size\"],\n    val_batch_size=10_000,\n    method=\"euler\",               # Solver: \"euler\" or \"rk4\"\n    alpha_dict=None               # Optional loss balancing\n)\n</code></pre>"},{"location":"howto/train_model/#configuration-reference","title":"\u2699\ufe0f Configuration Reference","text":""},{"location":"howto/train_model/#data-slicing-model_config","title":"Data Slicing (<code>model_config</code>)","text":"Parameter Description Typical Value <code>step</code> The time delta (\\(dt\\)) between two consecutive data points. <code>4</code> (OpenSky) or <code>1</code> (QAR) <code>seq_len</code> Number of time steps fed to the ODE integration. <code>60</code> to <code>120</code> <code>shift</code> Sliding window stride. Lower = more overlap but more data. <code>60</code> (Non-overlapping)"},{"location":"howto/train_model/#solver-options-trainertrain","title":"Solver Options (<code>trainer.train</code>)","text":"Parameter Description <code>method</code> The integration method. <code>euler</code> is faster but less precise. <code>rk4</code> (Runge-Kutta 4) is more stable but 4x slower. <code>alpha_dict</code> A dictionary mapping column names to loss weights. Useful if one variable (e.g., altitude) dominates the loss."},{"location":"howto/train_model/#outputs-artifacts","title":"\ud83d\udcbe Outputs &amp; Artifacts","text":"<p>Upon completion, the trainer generates the following structure in your <code>models_dir</code>:</p> <pre><code>models/opensky_2025_A320/\n\u251c\u2500\u2500 meta.json                # \u26a0\ufe0f CRITICAL: Contains scaling stats &amp; hyperparams\n\u251c\u2500\u2500 training_losses.csv      # Log of Train/Val loss per epoch\n\u251c\u2500\u2500 training_curve.png       # Visualization of convergence\n\u251c\u2500\u2500 trajectory.pt            # Checkpoint for Physics/Trajectory layers\n\u2514\u2500\u2500 data_ode.pt             # Checkpoint for Neural/Data layers\n</code></pre> <p>Do not delete meta.json</p> <p>The <code>meta.json</code> file stores the mean and standard deviation of the training data. The inference engine (<code>NodeFDMPredictor</code>) requires this file to normalize new data exactly as the model expects.</p>"},{"location":"howto/train_model/#advanced-tips","title":"\ud83d\udca1 Advanced Tips","text":"Resuming TrainingLoss BalancingMulti-Architecture Loops <p>To restart training from an existing checkpoint, update the <code>loading_args</code> in your config:</p> <pre><code>model_config = dict(\n    # ...\n    # (Load Weights=True, Load Optimizer State=True)\n    loading_args=(True, True),\n)\n</code></pre> <p>If your model struggles to learn specific dynamic variables (e.g., Vertical Speed <code>vz</code>), you can increase their weight in the loss function:</p> <pre><code># Penalize errors on 'vz' 5x more than other variables\ntrainer.train(\n    # ...\n    alpha_dict={\"vz\": 5.0}\n)\n</code></pre> <p>You can train multiple models sequentially by iterating over the architecture names and dataset subsets:</p> <pre><code>for arch in [\"opensky_2025\", \"my_custom_arch\"]:\n    # Update config\n    train_config[\"architecture_name\"] = arch\n    # Re-initialize trainer...\n</code></pre>"},{"location":"howto/train_model/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<ul> <li>Run Inference: Use your trained model to generate trajectory rollouts.</li> </ul>"},{"location":"quickstart/concepts/","title":"\ud83e\udde0 Core Concepts","text":"<p>This section introduces the fundamental building blocks of node-fdm. Understanding these concepts will help you navigate architectures, preprocess data, and extend the framework with your own models.</p>"},{"location":"quickstart/concepts/#column-groups","title":"\ud83d\udd22 Column Groups","text":"<p>Every architecture organizes its input and output features into standardized column groups. These groups define the information flow inside the Neural ODE.</p> <pre><code>graph LR\n    subgraph Inputs\n        direction TB\n        X(X_COLS&lt;br&gt;State)\n        U(U_COLS&lt;br&gt;Control)\n        E0(E0_COLS&lt;br&gt;Context)\n    end\n\n    subgraph Output\n        DX(DX_COLS&lt;br&gt;Derivatives)\n    end\n\n    Model[Architecture&lt;br&gt;Layers]\n\n    X &amp; U &amp; E0 --&gt; Model --&gt; DX\n\n    %% Styling matches your Index palette\n    classDef state fill:#9ECAE9,stroke:#333,stroke-width:2px,color:black;\n    classDef control fill:#FF9D98,stroke:#333,stroke-width:2px,color:black;\n    classDef context fill:#88D27A,stroke:#333,stroke-width:2px,color:black;\n    classDef derivative fill:#D6A5C9,stroke:#333,stroke-width:2px,color:black;\n    classDef model fill:#fff,stroke:#555,stroke-width:1px,stroke-dasharray: 5 5;\n\n    class X state;\n    class U control;\n    class E0 context;\n    class DX derivative;\n    class Model model;</code></pre> Group Variable Type Description <code>X_COLS</code> State Flight variables integrated by the ODE (e.g., altitude, speed). <code>U_COLS</code> Control Pilot inputs, FMS selections, or active controls. <code>E0_COLS</code> Environmental Exogenous inputs like wind, temperature, or static distances. <code>E_COLS</code> Derived Intermediate features calculated by physics layers (e.g., Mach number). <code>DX_COLS</code> Derivatives The target outputs predicted by the ODE layer (e.g., <code>dalt</code>, <code>dvz</code>)."},{"location":"quickstart/concepts/#architecture-stack","title":"\ud83c\udfd7\ufe0f Architecture Stack","text":"<p>An architecture in node-fdm is not just a neural network; it is a stack of components defined in <code>model.py</code>.</p> <p>The Stack</p> <p>Architecture = Physics Layers + Neural Layers</p> <ol> <li>Physics/Feature Layers: Deterministic layers that compute derived quantities (e.g., <code>TrajectoryLayer</code>, <code>EngineLayer</code>).</li> <li>Structured Layers: The Neural ODE components (<code>StructuredLayer</code>) that predict the final derivatives (<code>DX_COLS</code>).</li> </ol> <p>The <code>model.py</code> file defines the explicit order of these layers and how column groups are mapped between them.</p>"},{"location":"quickstart/concepts/#processing-hooks","title":"\ud83d\udd27 Processing Hooks","text":"<p>Architectures are self-contained: they define not only the model but also how the data must be prepared. This is handled via two specific hooks:</p> <pre><code>graph LR\n    Raw[Raw Data] --&gt; Hook1\n\n    subgraph Architecture Definition\n        direction TB\n        Hook1[[flight_processing]]\n        Hook2[[segment_filtering]]\n    end\n\n    Hook1 --&gt;|Augmented Data| Hook2\n    Hook2 --&gt;|Clean Data| Train[Training Set]\n\n    style Hook1 fill:#FFF9C4,stroke:#FBC02D\n    style Hook2 fill:#FFF9C4,stroke:#FBC02D</code></pre> <ul> <li><code>flight_processing</code>: Augments raw data before training (e.g., computing <code>alt_diff</code>, smoothing signals, adding derived physics).</li> <li><code>segment_filtering</code>: Removes poor-quality segments or invalid training examples based on domain-specific rules.</li> </ul>"},{"location":"quickstart/concepts/#normalization-statistics","title":"\ud83d\udcd0 Normalization &amp; Statistics","text":"<p>The <code>SeqDataset</code> class handles data normalization automatically to ensure consistent training and inference.</p> <p>Automated Features</p> <ul> <li>Statistics: Computes mean and standard deviation for every column.</li> <li>Robust Scaling: Applies outlier-robust scaling (clipped at 99.5%).</li> <li>Metadata: Saves all statistics to <code>meta.json</code>, ensuring the inference pipeline uses the exact same scaling as the training pipeline.</li> </ul>"},{"location":"quickstart/concepts/#registration-mechanism","title":"\ud83e\udde9 Registration Mechanism","text":"<p>For the <code>ODETrainer</code> or <code>NodeFDMPredictor</code> to utilize your custom architecture, it must be discoverable via the central registry.</p> <p>Registry Location</p> <p>The mapping is defined in: <code>node_fdm/architectures/mapping.py</code></p> <p>This file resolves a simple string identifier (e.g., <code>\"opensky_2025\"</code>) into a configuration dictionary that links the three essential components of your architecture:</p> <pre><code>graph LR\n    ID[String Name&lt;br&gt;'opensky_2025'] --&gt; Map{mapping.py}\n\n    Map --&gt;|Resolves to| Config[Configuration Dict]\n\n    Config --&gt; Cols[Columns Definition]\n    Config --&gt; Hooks[Processing Hooks]\n    Config --&gt; Class[Model Class]\n\n    style Map fill:#f9f,stroke:#333</code></pre>"},{"location":"quickstart/concepts/#registration-example","title":"Registration Example","text":"<p>Inside <code>mapping.py</code>, the registration looks like this:</p> node_fdm/architectures/mapping.py<pre><code>AVAILABLE_ARCHITECTURES = {\n    \"opensky_2025\": {\n        \"columns\": OpenSkyColumns,     # (1)\n        \"hooks\": {                     # (2)\n            \"flight\": process_flight,\n            \"segment\": filter_segment\n        },\n        \"model_class\": OpenSkyModel    # (3)\n    },\n    # Your custom architecture here...\n}\n</code></pre> <ol> <li>Column Definitions: Defines <code>X_COLS</code>, <code>U_COLS</code>, etc.</li> <li>Processing Hooks: The functions used to preprocess raw data.</li> <li>Layer Stack: The Python class defining the Neural ODE layers.</li> </ol>"},{"location":"quickstart/concepts/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<ul> <li>Pipelines Overview: Now that you understand the core building blocks, see how they fit together in a complete workflow.</li> </ul>"},{"location":"quickstart/installation/","title":"\u2699\ufe0f Installation","text":"<p>This page explains how to install node-fdm, configure optional dependencies, and set up the directory structure required for the data pipelines.</p>"},{"location":"quickstart/installation/#prerequisites","title":"\ud83e\udde9 Prerequisites","text":"<p>Before installing, ensure your environment meets the following requirements:</p> <ul> <li>Python 3.11+</li> <li>OpenSky Trino access (Required only if you plan to run the full OpenSky 2025 data collection pipeline).</li> </ul> <p>BADA 4.2 Model Files</p> <p>Support for the BADA 4.2 physical model is optional but recommended for benchmarking.</p> <ul> <li>You must obtain the model files separately (due to licensing).</li> <li>You will need to set their location in the relevant <code>config.yaml</code> later.</li> </ul>"},{"location":"quickstart/installation/#install-the-package","title":"\ud83d\udce6 Install the Package","text":"<p>Choose the installation method that matches your needs.</p> Standard User (PyPI)Contributor (Source) <p>Recommended for running existing pipelines and training models.</p> <p>1. Core Installation Install the core library: <pre><code>pip install node-fdm\n</code></pre></p> <p>2. Optional Dependencies (Recommended) To install support for traffic data processing, fast meteorology, and visualization: <pre><code># Quotes are often required for shell compatibility\npip install 'node-fdm[all]'\n</code></pre></p> <p>3. BADA Baseline Support (Optional) The <code>pybada</code> wrapper has restrictive dependencies. Use these specific commands to force installation: <pre><code>pip install pybada --ignore-requires-python --no-deps\npip install simplekml 'xlsxwriter&gt;=3.2.5'\n</code></pre></p> <p>Recommended if you plan to modify the code or create custom architectures.</p> <p>1. Clone the repository <pre><code>git clone [https://github.com/eurocontrol-asu/node-fdm.git](https://github.com/eurocontrol-asu/node-fdm.git)\ncd node-fdm\n</code></pre></p> <p>2. Editable Installation Install the package in editable mode along with all development dependencies: <pre><code>pip install -e .[all]\n</code></pre></p>"},{"location":"quickstart/installation/#configuration-directories","title":"\ud83d\udcc1 Configuration &amp; Directories","text":"<p>node-fdm relies on configuration files to locate data and artifacts. You must configure these paths before running a pipeline.</p> <p>Where to configure</p> <p>Edit the configuration file specific to your target pipeline:</p> <ul> <li>\ud83d\udcc2 OpenSky: <code>scripts/opensky/config.yaml</code></li> <li>\ud83d\udcc2 QAR: <code>scripts/qar/config.yaml</code></li> </ul> Parameter Description Requirement <code>paths.data_dir</code> The root directory for all data artifacts. Required <code>paths.era5_cache_dir</code> Local cache directory for meteorological fields. Required <code>bada.bada_4_2_dir</code> Path to the folder containing BADA 4.2 files. Optional <code>paths.download_dir</code> Destination for raw downloaded data. Auto-managed <code>paths.models_dir</code> Directory where trained models are saved. Auto-managed"},{"location":"quickstart/installation/#verification","title":"\u2714\ufe0f Verification","text":"<p>Run this quick check to verify that <code>node_fdm</code> and <code>torch</code> are correctly installed and importable.</p> <p>```python import torch import node_fdm import sys</p> <p>print(f\"Python version: {sys.version.split()[0]}\") print(f\"Torch version:  {torch.version}\") print(\"\u2705 node_fdm import successful\")</p> <p>Next Step</p> <p>Once installation is verified, head to the Core Concepts to understand how node-fdm works.</p>"},{"location":"quickstart/pipelines/","title":"\u26a1 Quickstart: End-to-End Pipelines","text":"<p>This guide provides a complete overview of how to run node-fdm end-to-end. It covers the abstract workflow used by all architectures and provides a step-by-step walkthrough of the OpenSky 2025 reference implementation.</p> <p>Configuration</p> <p>All paths assume you are at the repository root. Each pipeline ships its own configuration file:</p> <ul> <li><code>scripts/opensky/config.yaml</code></li> <li><code>scripts/qar/config.yaml</code></li> </ul>"},{"location":"quickstart/pipelines/#general-pattern","title":"\ud83d\udd04 General Pattern","text":"<p>Regardless of the data source (ADS-B or QAR), every architecture follows this 7-step logic.</p> <pre><code>graph LR\n    subgraph Prep [1. Data Preparation]\n        direction TB\n        S1[Collect &amp; Map] --&gt; S2[Decode &amp; Clean]\n        S2 --&gt; S3[Feature Enrichment]\n        S3 --&gt; S4[Dataset Split]\n    end\n\n    subgraph Learn [2. Learning]\n        direction TB\n        S5[Train ODETrainer]\n    end\n\n    subgraph Eval [3. Deployment]\n        direction TB\n        S6[Inference] --&gt; S7[Evaluation &amp; Viz]\n    end\n\n    Prep --&gt; Learn --&gt; Eval\n\n    classDef phase fill:#f9f9f9,stroke:#333,stroke-width:1px;\n    class Prep,Learn,Eval phase;</code></pre> <ol> <li>Collect and prepare raw data: Ensure inputs map to the architecture\u2019s <code>Column</code> definitions.</li> <li>Decode, resample, and clean: Build consistent time steps and remove invalid segments.</li> <li>Feature enrichment: Add environmental inputs (e.g., ERA5) and compute derived physics quantities.</li> <li>Dataset splitting: Generate train/val/test lists pointing to processed parquet files.</li> <li>Training: Run <code>ODETrainer</code>, loading the specific <code>model_params</code> from <code>model.py</code>.</li> <li>Inference: Load checkpoints with <code>NodeFDMPredictor</code> to generate trajectory rollouts.</li> <li>Evaluation: Compute metrics (MAE/MAPE) and generate comparison plots.</li> </ol>"},{"location":"quickstart/pipelines/#opensky-2025-ads-b-pipeline","title":"\ud83d\udce1 OpenSky 2025 (ADS-B) Pipeline","text":"<p>This reference pipeline processes public ADS-B data. The scripts are located in <code>scripts/opensky/</code>.</p> Phase 1: Data PreparationPhase 2: TrainingPhase 3: Inference &amp; Eval <p>These steps fetch raw data and transform it into enriched, training-ready tensors.</p> <p>1. Aircraft Sampling Builds the database of target aircraft types. <pre><code>python scripts/opensky/01_aircraft_list.py\n</code></pre> * Input: Trino SQL connection. * Output: <code>data/aircraft_db.csv</code>.</p> <p>2. Download Raw Data Fetches flight history and extended tables. <pre><code>python scripts/opensky/02_download_data.py\n</code></pre> * Output: <code>data/downloaded_parquet/</code>.</p> <p>3. Decode &amp; Resample Decodes specific BDS messages, filters short flights, and resamples to 4s. <pre><code>python scripts/opensky/03_preprocess_data.py\n</code></pre> * Note: Handles ADEP/ADES distance computation.</p> <p>4. Enrichment Injects ERA5 weather data and smooths signals. <pre><code>python scripts/opensky/04_weather_spd_process_data.py\n</code></pre> * Output: Enriched files in <code>data/processed_flights/&lt;TYPECODE&gt;/</code>.</p> <p>Train the Neural ODE using the architecture defined in <code>node_fdm.architectures.opensky_2025</code>.</p> <p>5. Train Model <pre><code>python scripts/opensky/05_training.py\n</code></pre> * Action: Uses <code>ODETrainer</code>. * Output: Checkpoints saved to <code>models/opensky_&lt;TYPECODE&gt;/</code>.</p> <p>Generate predictions and benchmark against BADA.</p> <p>6. Inference (Rollouts) <pre><code>python scripts/opensky/06_flight_prediction.py\n</code></pre> * Output: <code>data/predicted_flights/&lt;TYPECODE&gt;/</code>.</p> <p>7. Baselines &amp; Metrics Run these scripts in order to assess performance:</p> <ul> <li><code>07_bada_prediction.py</code>: Computes physical baseline (Requires BADA 4.2 files).</li> <li><code>08_visualize_predictions.py</code>: Generates overlays (Ground Truth vs Model vs BADA).</li> <li><code>09_performance_aggregation.py</code>: Computes MAE/MAPE metrics per flight phase.</li> <li><code>10_dataset_stats.py</code>: Generates coverage statistics.</li> </ul>"},{"location":"quickstart/pipelines/#general-tips","title":"\ud83d\udca1 General Tips","text":"<p>Single Source of Truth</p> <p>Always use the pipeline's <code>config.yaml</code> to define paths, typecodes, and shared parameters. Do not hardcode paths in scripts.</p> <p>Caching</p> <p>Ensure <code>data/era5_cache</code> exists. Meteorological data download is slow; caching prevents repeated downloads of the same ERA5 fields.</p> <p>Hardware Optimization</p> <p>If you face memory issues, adjust the following in <code>model_config</code>:</p> <ul> <li>Decrease <code>batch_size</code>.</li> <li>Decrease <code>num_workers</code>.</li> <li>Adjust <code>seq_len</code> (sequence length).</li> </ul>"},{"location":"quickstart/pipelines/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<p>Now that you understand the core building blocks, start applying them by following the step-by-step development workflow:</p> <ul> <li>1. Configure Project: Set up paths, typecodes, and key hyperparameters for your pipeline.</li> <li>2. Create an Architecture: Define your custom model's column groups, preprocessing hooks, and layer stack.</li> <li>3. Train a Model: Launch the learning process using the <code>ODETrainer</code>.</li> <li>4. Run Inference: Generate trajectory rollouts and evaluate the model's performance.</li> </ul>"},{"location":"reference/architectures/","title":"\ud83c\udfd7\ufe0f Architectures API","text":"<p>The <code>node_fdm.architectures</code> namespace contains the blueprint definitions for specific flight dynamics problems.</p> <p>It serves two main purposes: Registration (mapping string names to Python objects) and Implementation (defining the column groups, preprocessing logic, and model stacks for specific datasets like OpenSky or QAR).</p>"},{"location":"reference/architectures/#registry","title":"\ud83e\udde9 Registry","text":"<p>The mapping module is the central lookup table. It allows the <code>ODETrainer</code> and <code>Predictor</code> to instantiate the correct classes based on a configuration string.</p>"},{"location":"reference/architectures/#node_fdm.architectures.mapping","title":"<code>mapping</code>","text":"<p>Helpers to dynamically load and assemble architecture components.</p>"},{"location":"reference/architectures/#node_fdm.architectures.mapping.get_architecture_from_name","title":"<code>get_architecture_from_name(architecture_name)</code>","text":"<p>Return architecture definition, model columns, and custom functions by name.</p> <p>Parameters:</p> Name Type Description Default <code>architecture_name</code> <code>str</code> <p>Name of the architecture to load.</p> required <p>Returns:</p> Type Description <code>Tuple[Any, Any, Any]</code> <p>Tuple of (architecture layers, model columns, custom functions).</p> Source code in <code>src/node_fdm/architectures/mapping.py</code> <pre><code>def get_architecture_from_name(architecture_name: str) -&gt; Tuple[Any, Any, Any]:\n    \"\"\"Return architecture definition, model columns, and custom functions by name.\n\n    Args:\n        architecture_name: Name of the architecture to load.\n\n    Returns:\n        Tuple of (architecture layers, model columns, custom functions).\n    \"\"\"\n    architecture_dict = get_architecture_module(architecture_name)\n    architecture = architecture_dict[\"model\"].ARCHITECTURE\n    model_cols = architecture_dict[\"model\"].MODEL_COLS\n    custom_fn = architecture_dict[\"custom_fn\"]\n    return architecture, model_cols, custom_fn\n</code></pre>"},{"location":"reference/architectures/#node_fdm.architectures.mapping.get_architecture_module","title":"<code>get_architecture_module(name)</code>","text":"<p>Dynamically import only the architecture requested.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Architecture package name to load.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing imported column, model, and custom function modules.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided architecture name is not supported.</p> Source code in <code>src/node_fdm/architectures/mapping.py</code> <pre><code>def get_architecture_module(name: str) -&gt; Dict[str, Any]:\n    \"\"\"Dynamically import only the architecture requested.\n\n    Args:\n        name: Architecture package name to load.\n\n    Returns:\n        Dictionary containing imported column, model, and custom function modules.\n\n    Raises:\n        ValueError: If the provided architecture name is not supported.\n    \"\"\"\n    valid_names = [\"opensky_2025\", \"qar\"]\n\n    if name not in valid_names:\n        raise ValueError(f\"Unknown architecture '{name}'. Valid names: {valid_names}\")\n\n    module_root = f\"node_fdm.architectures.{name}\"\n\n    columns = importlib.import_module(f\"{module_root}.columns\")\n    flight_process = importlib.import_module(f\"{module_root}.flight_process\")\n    model = importlib.import_module(f\"{module_root}.model\")\n\n    return {\n        \"columns\": columns,\n        \"custom_fn\": (\n            flight_process.flight_processing,\n            flight_process.segment_filtering,\n        ),\n        \"model\": model,\n    }\n</code></pre>"},{"location":"reference/architectures/#node_fdm.architectures.mapping.get_architecture_params_from_meta","title":"<code>get_architecture_params_from_meta(meta_path)</code>","text":"<p>Load architecture parameters and stats from a meta JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>meta_path</code> <code>str</code> <p>Path to the meta JSON file.</p> required <p>Returns:</p> Type Description <code>Tuple[Any, Any, Any, Dict[Any, Any]]</code> <p>Tuple containing architecture, model columns, model parameters, and stats dictionary.</p> Source code in <code>src/node_fdm/architectures/mapping.py</code> <pre><code>def get_architecture_params_from_meta(\n    meta_path: str,\n) -&gt; Tuple[Any, Any, Any, Dict[Any, Any]]:\n    \"\"\"Load architecture parameters and stats from a meta JSON file.\n\n    Args:\n        meta_path: Path to the meta JSON file.\n\n    Returns:\n        Tuple containing architecture, model columns, model parameters, and stats dictionary.\n    \"\"\"\n    with open(meta_path, \"r\") as f:\n        meta = json.load(f)\n\n    architecture, model_cols, _ = get_architecture_from_name(meta[\"architecture_name\"])\n    x_cols, u_cols, e0_cols, e_cols, _ = model_cols\n    deriv_cols = [col.derivative for col in x_cols]\n    model_cols2 = [x_cols, u_cols, e0_cols, e_cols, deriv_cols]\n\n    all_cols_dict = {str(col): col for cols in model_cols2 for col in cols}\n    stats_dict = {\n        all_cols_dict[str_col]: stats for str_col, stats in meta[\"stats_dict\"].items()\n    }\n\n    return architecture, model_cols, meta[\"model_params\"], stats_dict\n</code></pre>"},{"location":"reference/architectures/#opensky-2025","title":"\ud83d\udce1 OpenSky 2025","text":"<p>This is the reference implementation for public ADS-B data. It defines a physics-informed architecture capable of handling noisy surveillance data.</p>"},{"location":"reference/architectures/#columns-definition","title":"Columns Definition","text":"<p>Defines the input/output variables (State, Control, Environment) and their units.</p>"},{"location":"reference/architectures/#node_fdm.architectures.opensky_2025.columns","title":"<code>columns</code>","text":"<p>Column definitions and unit mappings for the OpenSky 2025 architecture.</p>"},{"location":"reference/architectures/#processing-hooks","title":"Processing Hooks","text":"<p>Functions to clean, smooth, and augment raw ADS-B data.</p>"},{"location":"reference/architectures/#node_fdm.architectures.opensky_2025.flight_process","title":"<code>flight_process</code>","text":"<p>Pre-processing utilities for OpenSky 2025 flight data.</p>"},{"location":"reference/architectures/#node_fdm.architectures.opensky_2025.flight_process.flight_processing","title":"<code>flight_processing(df)</code>","text":"<p>Prepare OpenSky flight data by computing altitude differences.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing flight measurements.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with altitude difference column added.</p> Source code in <code>src/node_fdm/architectures/opensky_2025/flight_process.py</code> <pre><code>def flight_processing(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Prepare OpenSky flight data by computing altitude differences.\n\n    Args:\n        df: Input DataFrame containing flight measurements.\n\n    Returns:\n        DataFrame with altitude difference column added.\n    \"\"\"\n    df[col_alt_diff] = df[col_alt_sel] - df[col_alt]\n\n    df[col_vz_sel] = df[col_vz_sel].fillna(0.0)\n    df[col_mach_sel] = df[col_mach_sel].fillna(0.0)\n    df[col_cas_sel] = df[col_cas_sel].fillna(0.0)\n\n    return df\n</code></pre>"},{"location":"reference/architectures/#node_fdm.architectures.opensky_2025.flight_process.segment_filtering","title":"<code>segment_filtering(f, start_idx, seq_len)</code>","text":"<p>Check whether a segment meets distance variation thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>DataFrame</code> <p>DataFrame containing flight measurements.</p> required <code>start_idx</code> <code>int</code> <p>Starting index of the segment to evaluate.</p> required <code>seq_len</code> <code>int</code> <p>Length of the segment to evaluate.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the segment stays within distance thresholds, otherwise False.</p> Source code in <code>src/node_fdm/architectures/opensky_2025/flight_process.py</code> <pre><code>def segment_filtering(f: pd.DataFrame, start_idx: int, seq_len: int) -&gt; bool:\n    \"\"\"Check whether a segment meets distance variation thresholds.\n\n    Args:\n        f: DataFrame containing flight measurements.\n        start_idx: Starting index of the segment to evaluate.\n        seq_len: Length of the segment to evaluate.\n\n    Returns:\n        True if the segment stays within distance thresholds, otherwise False.\n    \"\"\"\n    dist_diff = f[col_dist].diff(1)\n    seg = dist_diff.iloc[start_idx : start_idx + seq_len]\n    condition = len(seg[(seg &lt; LOW_THR) | (seg &gt; UPPER_THR)]) == 0\n    return condition\n</code></pre>"},{"location":"reference/architectures/#model-stack","title":"Model Stack","text":"<p>The assembly of the Neural ODE, connecting physics layers with the learned derivative layer.</p>"},{"location":"reference/architectures/#node_fdm.architectures.opensky_2025.model","title":"<code>model</code>","text":"<p>Layer configuration and column grouping for the OpenSky 2025 architecture.</p>"},{"location":"reference/architectures/#trajectory-layer","title":"Trajectory Layer","text":"<p>A specialized physics layer that computes derived kinematic variables.</p>"},{"location":"reference/architectures/#node_fdm.architectures.opensky_2025.trajectory_layer","title":"<code>trajectory_layer</code>","text":"<p>Torch module for computing basic trajectory features for OpenSky 2025 data.</p>"},{"location":"reference/architectures/#node_fdm.architectures.opensky_2025.trajectory_layer.TrajectoryLayer","title":"<code>TrajectoryLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Compute trajectory outputs such as vertical speed, Mach, and calibrated airspeed.</p> Source code in <code>src/node_fdm/architectures/opensky_2025/trajectory_layer.py</code> <pre><code>class TrajectoryLayer(nn.Module):\n    \"\"\"Compute trajectory outputs such as vertical speed, Mach, and calibrated airspeed.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the trajectory layer with base configuration.\"\"\"\n        super().__init__()\n        self.alpha = 40\n\n    def forward(self, x: Mapping[Any, torch.Tensor]) -&gt; Dict[Any, torch.Tensor]:\n        \"\"\"Compute derived trajectory quantities from the provided inputs.\n\n        Args:\n            x: Mapping from column identifiers to input tensors.\n\n        Returns:\n            Dictionary of derived tensors keyed by their column identifiers.\n        \"\"\"\n        output_dict = {}\n        for col in [col_tas, col_gamma, col_alt, col_long_wind_spd]:\n            x[col] = torch.nan_to_num(x[col], nan=0.0, posinf=1e6, neginf=-1e6)\n            x[col] = torch.clamp(x[col], min=-1e6, max=1e6)\n\n        tas = x[col_tas]\n        gamma = x[col_gamma]\n        long_wind = x[col_long_wind_spd]\n        alt = x[col_alt]\n\n        output_dict[col_vz] = tas * torch.sin(gamma)\n\n        temp = isa_temperature_torch(alt)\n\n        a = torch.sqrt(torch.clamp(gamma_ratio * R * temp, min=1e-6, max=1e8))\n\n        mach = tas / torch.clamp(a, min=1e-6, max=1e8)\n        output_dict[col_mach] = mach\n\n        output_dict[col_gs] = tas - long_wind\n\n        p = isa_pressure_torch(alt)\n\n        pt_over_p = torch.pow(\n            torch.clamp(1 + (gamma_ratio - 1) / 2 * mach**2, min=1e-6, max=1e6),\n            gamma_ratio / (gamma_ratio - 1),\n        )\n\n        qc_p0 = (torch.clamp(p, min=1.0) / p0) * (pt_over_p - 1.0)\n        qc_p0 = torch.clamp(qc_p0, min=-0.999, max=1e6)\n\n        CAS_term = torch.clamp(qc_p0 + 1.0, min=1e-8, max=1e6)\n        CAS = a0 * torch.sqrt(\n            (2.0 / (gamma_ratio - 1.0))\n            * (CAS_term ** ((gamma_ratio - 1.0) / gamma_ratio) - 1.0)\n        )\n        CAS = torch.nan_to_num(CAS, nan=0.0, posinf=1e4, neginf=0.0)\n        output_dict[col_cas] = CAS\n\n        # --- Reference differences ---\n        ref_alt = x[col_alt_sel]\n\n        alt_diff = ref_alt - alt\n\n        output_dict[col_alt_diff] = torch.nan_to_num(alt_diff, nan=0.0)\n\n        return output_dict\n</code></pre>"},{"location":"reference/architectures/#node_fdm.architectures.opensky_2025.trajectory_layer.TrajectoryLayer.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the trajectory layer with base configuration.</p> Source code in <code>src/node_fdm/architectures/opensky_2025/trajectory_layer.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the trajectory layer with base configuration.\"\"\"\n    super().__init__()\n    self.alpha = 40\n</code></pre>"},{"location":"reference/architectures/#node_fdm.architectures.opensky_2025.trajectory_layer.TrajectoryLayer.forward","title":"<code>forward(x)</code>","text":"<p>Compute derived trajectory quantities from the provided inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Mapping[Any, Tensor]</code> <p>Mapping from column identifiers to input tensors.</p> required <p>Returns:</p> Type Description <code>Dict[Any, Tensor]</code> <p>Dictionary of derived tensors keyed by their column identifiers.</p> Source code in <code>src/node_fdm/architectures/opensky_2025/trajectory_layer.py</code> <pre><code>def forward(self, x: Mapping[Any, torch.Tensor]) -&gt; Dict[Any, torch.Tensor]:\n    \"\"\"Compute derived trajectory quantities from the provided inputs.\n\n    Args:\n        x: Mapping from column identifiers to input tensors.\n\n    Returns:\n        Dictionary of derived tensors keyed by their column identifiers.\n    \"\"\"\n    output_dict = {}\n    for col in [col_tas, col_gamma, col_alt, col_long_wind_spd]:\n        x[col] = torch.nan_to_num(x[col], nan=0.0, posinf=1e6, neginf=-1e6)\n        x[col] = torch.clamp(x[col], min=-1e6, max=1e6)\n\n    tas = x[col_tas]\n    gamma = x[col_gamma]\n    long_wind = x[col_long_wind_spd]\n    alt = x[col_alt]\n\n    output_dict[col_vz] = tas * torch.sin(gamma)\n\n    temp = isa_temperature_torch(alt)\n\n    a = torch.sqrt(torch.clamp(gamma_ratio * R * temp, min=1e-6, max=1e8))\n\n    mach = tas / torch.clamp(a, min=1e-6, max=1e8)\n    output_dict[col_mach] = mach\n\n    output_dict[col_gs] = tas - long_wind\n\n    p = isa_pressure_torch(alt)\n\n    pt_over_p = torch.pow(\n        torch.clamp(1 + (gamma_ratio - 1) / 2 * mach**2, min=1e-6, max=1e6),\n        gamma_ratio / (gamma_ratio - 1),\n    )\n\n    qc_p0 = (torch.clamp(p, min=1.0) / p0) * (pt_over_p - 1.0)\n    qc_p0 = torch.clamp(qc_p0, min=-0.999, max=1e6)\n\n    CAS_term = torch.clamp(qc_p0 + 1.0, min=1e-8, max=1e6)\n    CAS = a0 * torch.sqrt(\n        (2.0 / (gamma_ratio - 1.0))\n        * (CAS_term ** ((gamma_ratio - 1.0) / gamma_ratio) - 1.0)\n    )\n    CAS = torch.nan_to_num(CAS, nan=0.0, posinf=1e4, neginf=0.0)\n    output_dict[col_cas] = CAS\n\n    # --- Reference differences ---\n    ref_alt = x[col_alt_sel]\n\n    alt_diff = ref_alt - alt\n\n    output_dict[col_alt_diff] = torch.nan_to_num(alt_diff, nan=0.0)\n\n    return output_dict\n</code></pre>"},{"location":"reference/data/","title":"\ud83d\udcbd Data Pipeline API","text":"<p>The <code>node_fdm.data</code> namespace handles the transformation of raw flight records into training-ready tensors.</p> <p>Its primary responsibilities include applying architecture-specific preprocessing hooks via the Flight Processor, normalizing features using robust statistics, and managing efficient sequence loading from disk through the Dataset and Loader utilities.</p>"},{"location":"reference/data/#class-reference","title":"\ud83d\udcd8 Class Reference","text":""},{"location":"reference/data/#flight-processor","title":"Flight Processor","text":""},{"location":"reference/data/#node_fdm.data.flight_processor","title":"<code>flight_processor</code>","text":"<p>Flight preprocessing pipeline for converting raw data into model-ready columns.</p>"},{"location":"reference/data/#node_fdm.data.flight_processor.FlightProcessor","title":"<code>FlightProcessor</code>","text":"<p>Flexible flight data processor with a customizable post-processing hook.</p> Source code in <code>src/node_fdm/data/flight_processor.py</code> <pre><code>class FlightProcessor:\n    \"\"\"Flexible flight data processor with a customizable post-processing hook.\"\"\"\n\n    def __init__(\n        self,\n        model_cols: Tuple[Any, Any, Any, Any, Any],\n        custom_processing_fn: Optional[Callable[[Any], Any]] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the processor with model column configuration and hooks.\n\n        Args:\n            model_cols: Tuple of model column groups (state, control, env, etc.).\n            custom_processing_fn: Optional callable applied after base processing; uses Any for flexibility with DataFrame-like inputs.\n        \"\"\"\n        (\n            self.x_cols,\n            self.u_cols,\n            self.e0_cols,\n            self.e_cols,\n            self.dx_cols,\n        ) = model_cols\n        self.dx_cols = [col.derivative for col in self.x_cols]\n        self.custom_processing_fn = custom_processing_fn\n\n    # ------------------------------------------------------------------\n    def process_flight(self, df: Any) -&gt; DataFrameWrapper:\n        \"\"\"Run the main flight preprocessing pipeline.\n\n        Args:\n            df: DataFrame-like object containing raw flight data. Uses Any for flexibility across wrappers.\n\n        Returns:\n            Processed DataFrameWrapper filtered to model-relevant columns.\n        \"\"\"\n\n        df = DataFrameWrapper(df)\n\n        for col in Column.get_all():\n            raw_col = col.raw_name\n            gold_col = col.col_name\n            if raw_col is not None and raw_col in df.columns:\n                df[gold_col] = col.unit.convert(df[raw_col])\n\n        for col in self.x_cols:\n            df[col.derivative] = df[col].diff(1).bfill()\n\n        if self.custom_processing_fn is not None:\n            df = self.custom_processing_fn(df)\n\n        return df[self.x_cols + self.u_cols + self.e0_cols + self.e_cols + self.dx_cols]\n</code></pre>"},{"location":"reference/data/#node_fdm.data.flight_processor.FlightProcessor.__init__","title":"<code>__init__(model_cols, custom_processing_fn=None)</code>","text":"<p>Initialize the processor with model column configuration and hooks.</p> <p>Parameters:</p> Name Type Description Default <code>model_cols</code> <code>Tuple[Any, Any, Any, Any, Any]</code> <p>Tuple of model column groups (state, control, env, etc.).</p> required <code>custom_processing_fn</code> <code>Optional[Callable[[Any], Any]]</code> <p>Optional callable applied after base processing; uses Any for flexibility with DataFrame-like inputs.</p> <code>None</code> Source code in <code>src/node_fdm/data/flight_processor.py</code> <pre><code>def __init__(\n    self,\n    model_cols: Tuple[Any, Any, Any, Any, Any],\n    custom_processing_fn: Optional[Callable[[Any], Any]] = None,\n) -&gt; None:\n    \"\"\"Initialize the processor with model column configuration and hooks.\n\n    Args:\n        model_cols: Tuple of model column groups (state, control, env, etc.).\n        custom_processing_fn: Optional callable applied after base processing; uses Any for flexibility with DataFrame-like inputs.\n    \"\"\"\n    (\n        self.x_cols,\n        self.u_cols,\n        self.e0_cols,\n        self.e_cols,\n        self.dx_cols,\n    ) = model_cols\n    self.dx_cols = [col.derivative for col in self.x_cols]\n    self.custom_processing_fn = custom_processing_fn\n</code></pre>"},{"location":"reference/data/#node_fdm.data.flight_processor.FlightProcessor.process_flight","title":"<code>process_flight(df)</code>","text":"<p>Run the main flight preprocessing pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame-like object containing raw flight data. Uses Any for flexibility across wrappers.</p> required <p>Returns:</p> Type Description <code>DataFrameWrapper</code> <p>Processed DataFrameWrapper filtered to model-relevant columns.</p> Source code in <code>src/node_fdm/data/flight_processor.py</code> <pre><code>def process_flight(self, df: Any) -&gt; DataFrameWrapper:\n    \"\"\"Run the main flight preprocessing pipeline.\n\n    Args:\n        df: DataFrame-like object containing raw flight data. Uses Any for flexibility across wrappers.\n\n    Returns:\n        Processed DataFrameWrapper filtered to model-relevant columns.\n    \"\"\"\n\n    df = DataFrameWrapper(df)\n\n    for col in Column.get_all():\n        raw_col = col.raw_name\n        gold_col = col.col_name\n        if raw_col is not None and raw_col in df.columns:\n            df[gold_col] = col.unit.convert(df[raw_col])\n\n    for col in self.x_cols:\n        df[col.derivative] = df[col].diff(1).bfill()\n\n    if self.custom_processing_fn is not None:\n        df = self.custom_processing_fn(df)\n\n    return df[self.x_cols + self.u_cols + self.e0_cols + self.e_cols + self.dx_cols]\n</code></pre>"},{"location":"reference/data/#dataset","title":"Dataset","text":""},{"location":"reference/data/#node_fdm.data.dataset","title":"<code>dataset</code>","text":"<p>Dataset utilities for loading and segmenting flight data sequences.</p>"},{"location":"reference/data/#node_fdm.data.dataset.SeqDataset","title":"<code>SeqDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Sequence dataset that loads flight segments for model training.</p> Source code in <code>src/node_fdm/data/dataset.py</code> <pre><code>class SeqDataset(Dataset):\n    \"\"\"Sequence dataset that loads flight segments for model training.\"\"\"\n\n    def __init__(\n        self,\n        flights_path_list: Sequence[str],\n        model_cols: Tuple[Any, Any, Any, Any, Any],\n        seq_len: int = 60,\n        shift: int = 60,\n        n_jobs: int = 35,\n        load_parallel: bool = True,\n        custom_fn: Tuple[\n            Optional[Callable[[pd.DataFrame], pd.DataFrame]],\n            Optional[Callable[..., bool]],\n        ] = (None, None),\n    ) -&gt; None:\n        \"\"\"Initialize the dataset with flight paths and model column definitions.\n\n        Args:\n            flights_path_list: Iterable of flight parquet file paths.\n            model_cols: Tuple containing model column groups (state, control, env, etc.).\n            seq_len: Sequence length to extract from each flight.\n            shift: Step size when sliding the sequence window.\n            n_jobs: Number of parallel workers to use when loading flights.\n            load_parallel: Whether to load flights concurrently.\n            custom_fn: Tuple of optional processing and segment-filtering callables.\n        \"\"\"\n        self.flights_path_list = flights_path_list\n        self.shift = shift\n        self.seq_len = seq_len\n        self.x_cols, self.u_cols, self.e0_cols, self.e_cols, self.dx_cols = model_cols\n        self.deriv_cols = [col.derivative for col in self.x_cols]\n        self.model_cols = model_cols\n        self.load_parallel = load_parallel\n        self.n_jobs = n_jobs\n        custom_processing_fn, custom_segment_filtering_fn = custom_fn\n        self.processor = FlightProcessor(\n            model_cols, custom_processing_fn=custom_processing_fn\n        )\n        self.custom_segment_filtering_fn = custom_segment_filtering_fn\n        self.init_flight_date()\n\n    def init_flight_date(self) -&gt; None:\n        \"\"\"Load all flights, build sequence cache, and compute aggregate statistics.\n\n        Populates internal sequence list and per-column statistics used for normalization.\n        \"\"\"\n        if self.load_parallel:\n            results = Parallel(n_jobs=self.n_jobs)(\n                delayed(self.process_one_flight)(\n                    flight,\n                )\n                for flight in tqdm(self.flights_path_list, desc=\"Loading flights\")\n            )\n        else:\n            results = [\n                self.process_one_flight(flight)\n                for flight in tqdm(self.flights_path_list, desc=\"Loading flights\")\n            ]\n\n        self.sequences = []\n        for seqs in results:\n            self.sequences.extend(seqs)\n\n        all_data = np.concatenate(\n            [\n                np.concatenate([seq[0], seq[1], seq[2], seq[3]], axis=1)\n                for seq in self.sequences\n            ],\n            axis=0,\n        )\n\n        all_cols = (\n            self.x_cols + self.u_cols + self.e0_cols + self.e_cols + self.deriv_cols\n        )\n\n        self.stats_dict = dict()\n\n        for i, col in enumerate(all_cols):\n            vals = all_data[:, i].astype(float)\n            self.stats_dict[col] = {\n                \"mean\": vals.mean(),\n                \"std\": vals.std() + 1e-6,\n                \"max\": np.percentile(np.abs(vals), 99.5),\n            }\n\n    def process_one_flight(\n        self, flight_path: str\n    ) -&gt; List[Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]]:\n        \"\"\"Process a single flight file into clean, nan-free sequences.\n\n        Args:\n            flight_path: Path to a flight parquet file.\n\n        Returns:\n            List of tuples containing state, control, environment, and derivative arrays.\n        \"\"\"\n        f = self.read_flight(flight_path)\n        seqs = []\n        N = len(f)\n        if N &gt; self.seq_len:\n            x_seq = f[self.x_cols].values.astype(np.float32)\n            u_seq = f[self.u_cols].values.astype(np.float32)\n            e_seq = f[self.e0_cols + self.e_cols].values.astype(np.float32)\n            dx_seq = f[self.deriv_cols].values.astype(np.float32)\n\n            for start in range(0, N - self.seq_len + 1, self.shift):\n                custom_segment_filtering_bool = True\n                if self.custom_segment_filtering_fn is not None:\n                    custom_segment_filtering_bool = self.custom_segment_filtering_fn(\n                        f, start, self.seq_len\n                    )\n                nans = sum(\n                    [\n                        np.isnan(seq[start : start + self.seq_len]).sum()\n                        for seq in [x_seq, u_seq, e_seq, dx_seq]\n                    ]\n                )\n                if (custom_segment_filtering_bool) &amp; (nans == 0):\n                    seqs.append(\n                        (\n                            x_seq[start : start + self.seq_len],\n                            u_seq[start : start + self.seq_len],\n                            e_seq[start : start + self.seq_len],\n                            dx_seq[start : start + self.seq_len],\n                        )\n                    )\n        return seqs\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return number of available sequences.\n\n        Returns:\n            Count of cached flight sequences.\n        \"\"\"\n        return len(self.sequences)\n\n    def __getitem__(\n        self, idx: int\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Return tensors for a specific sequence index.\n\n        Args:\n            idx: Index of the sequence to retrieve.\n\n        Returns:\n            Tuple of tensors for state, control, environment, and derivative slices.\n        \"\"\"\n        x_seq, u_seq, e_seq, dxdt_seq = self.sequences[idx]\n        return (\n            torch.tensor(x_seq, dtype=torch.float32),\n            torch.tensor(u_seq, dtype=torch.float32),\n            torch.tensor(e_seq, dtype=torch.float32),\n            torch.tensor(dxdt_seq, dtype=torch.float32),\n        )\n\n    def read_flight(self, flight_path: str) -&gt; pd.DataFrame:\n        \"\"\"Read a flight parquet file and apply base processing.\n\n        Args:\n            flight_path: Path to a parquet file containing flight data.\n\n        Returns:\n            Processed DataFrame with standardized columns.\n        \"\"\"\n        f = pd.read_parquet(flight_path)\n        return self.processor.process_flight(f)\n\n    def get_full_flight(\n        self, flight_idx: int\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, pd.DataFrame]:\n        \"\"\"Return full arrays for a specific flight index.\n\n        Args:\n            flight_idx: Index of the flight in the provided flight list.\n\n        Returns:\n            Tuple of state, control, environment, derivative arrays, and the full DataFrame.\n        \"\"\"\n        flight_path = self.flights_path_list[flight_idx]\n        f = self.read_flight(flight_path)\n        x_seq = f[self.x_cols].values.astype(np.float32)\n        u_seq = f[self.u_cols].values.astype(np.float32)\n        e0_seq = f[self.e0_cols + self.e_cols].values.astype(np.float32)\n        dx_seq = f[self.deriv_cols].values.astype(np.float32)\n        return x_seq, u_seq, e0_seq, dx_seq, f\n</code></pre>"},{"location":"reference/data/#node_fdm.data.dataset.SeqDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Return tensors for a specific sequence index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the sequence to retrieve.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>Tuple of tensors for state, control, environment, and derivative slices.</p> Source code in <code>src/node_fdm/data/dataset.py</code> <pre><code>def __getitem__(\n    self, idx: int\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Return tensors for a specific sequence index.\n\n    Args:\n        idx: Index of the sequence to retrieve.\n\n    Returns:\n        Tuple of tensors for state, control, environment, and derivative slices.\n    \"\"\"\n    x_seq, u_seq, e_seq, dxdt_seq = self.sequences[idx]\n    return (\n        torch.tensor(x_seq, dtype=torch.float32),\n        torch.tensor(u_seq, dtype=torch.float32),\n        torch.tensor(e_seq, dtype=torch.float32),\n        torch.tensor(dxdt_seq, dtype=torch.float32),\n    )\n</code></pre>"},{"location":"reference/data/#node_fdm.data.dataset.SeqDataset.__init__","title":"<code>__init__(flights_path_list, model_cols, seq_len=60, shift=60, n_jobs=35, load_parallel=True, custom_fn=(None, None))</code>","text":"<p>Initialize the dataset with flight paths and model column definitions.</p> <p>Parameters:</p> Name Type Description Default <code>flights_path_list</code> <code>Sequence[str]</code> <p>Iterable of flight parquet file paths.</p> required <code>model_cols</code> <code>Tuple[Any, Any, Any, Any, Any]</code> <p>Tuple containing model column groups (state, control, env, etc.).</p> required <code>seq_len</code> <code>int</code> <p>Sequence length to extract from each flight.</p> <code>60</code> <code>shift</code> <code>int</code> <p>Step size when sliding the sequence window.</p> <code>60</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel workers to use when loading flights.</p> <code>35</code> <code>load_parallel</code> <code>bool</code> <p>Whether to load flights concurrently.</p> <code>True</code> <code>custom_fn</code> <code>Tuple[Optional[Callable[[DataFrame], DataFrame]], Optional[Callable[..., bool]]]</code> <p>Tuple of optional processing and segment-filtering callables.</p> <code>(None, None)</code> Source code in <code>src/node_fdm/data/dataset.py</code> <pre><code>def __init__(\n    self,\n    flights_path_list: Sequence[str],\n    model_cols: Tuple[Any, Any, Any, Any, Any],\n    seq_len: int = 60,\n    shift: int = 60,\n    n_jobs: int = 35,\n    load_parallel: bool = True,\n    custom_fn: Tuple[\n        Optional[Callable[[pd.DataFrame], pd.DataFrame]],\n        Optional[Callable[..., bool]],\n    ] = (None, None),\n) -&gt; None:\n    \"\"\"Initialize the dataset with flight paths and model column definitions.\n\n    Args:\n        flights_path_list: Iterable of flight parquet file paths.\n        model_cols: Tuple containing model column groups (state, control, env, etc.).\n        seq_len: Sequence length to extract from each flight.\n        shift: Step size when sliding the sequence window.\n        n_jobs: Number of parallel workers to use when loading flights.\n        load_parallel: Whether to load flights concurrently.\n        custom_fn: Tuple of optional processing and segment-filtering callables.\n    \"\"\"\n    self.flights_path_list = flights_path_list\n    self.shift = shift\n    self.seq_len = seq_len\n    self.x_cols, self.u_cols, self.e0_cols, self.e_cols, self.dx_cols = model_cols\n    self.deriv_cols = [col.derivative for col in self.x_cols]\n    self.model_cols = model_cols\n    self.load_parallel = load_parallel\n    self.n_jobs = n_jobs\n    custom_processing_fn, custom_segment_filtering_fn = custom_fn\n    self.processor = FlightProcessor(\n        model_cols, custom_processing_fn=custom_processing_fn\n    )\n    self.custom_segment_filtering_fn = custom_segment_filtering_fn\n    self.init_flight_date()\n</code></pre>"},{"location":"reference/data/#node_fdm.data.dataset.SeqDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return number of available sequences.</p> <p>Returns:</p> Type Description <code>int</code> <p>Count of cached flight sequences.</p> Source code in <code>src/node_fdm/data/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return number of available sequences.\n\n    Returns:\n        Count of cached flight sequences.\n    \"\"\"\n    return len(self.sequences)\n</code></pre>"},{"location":"reference/data/#node_fdm.data.dataset.SeqDataset.get_full_flight","title":"<code>get_full_flight(flight_idx)</code>","text":"<p>Return full arrays for a specific flight index.</p> <p>Parameters:</p> Name Type Description Default <code>flight_idx</code> <code>int</code> <p>Index of the flight in the provided flight list.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray, ndarray, DataFrame]</code> <p>Tuple of state, control, environment, derivative arrays, and the full DataFrame.</p> Source code in <code>src/node_fdm/data/dataset.py</code> <pre><code>def get_full_flight(\n    self, flight_idx: int\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, pd.DataFrame]:\n    \"\"\"Return full arrays for a specific flight index.\n\n    Args:\n        flight_idx: Index of the flight in the provided flight list.\n\n    Returns:\n        Tuple of state, control, environment, derivative arrays, and the full DataFrame.\n    \"\"\"\n    flight_path = self.flights_path_list[flight_idx]\n    f = self.read_flight(flight_path)\n    x_seq = f[self.x_cols].values.astype(np.float32)\n    u_seq = f[self.u_cols].values.astype(np.float32)\n    e0_seq = f[self.e0_cols + self.e_cols].values.astype(np.float32)\n    dx_seq = f[self.deriv_cols].values.astype(np.float32)\n    return x_seq, u_seq, e0_seq, dx_seq, f\n</code></pre>"},{"location":"reference/data/#node_fdm.data.dataset.SeqDataset.init_flight_date","title":"<code>init_flight_date()</code>","text":"<p>Load all flights, build sequence cache, and compute aggregate statistics.</p> <p>Populates internal sequence list and per-column statistics used for normalization.</p> Source code in <code>src/node_fdm/data/dataset.py</code> <pre><code>def init_flight_date(self) -&gt; None:\n    \"\"\"Load all flights, build sequence cache, and compute aggregate statistics.\n\n    Populates internal sequence list and per-column statistics used for normalization.\n    \"\"\"\n    if self.load_parallel:\n        results = Parallel(n_jobs=self.n_jobs)(\n            delayed(self.process_one_flight)(\n                flight,\n            )\n            for flight in tqdm(self.flights_path_list, desc=\"Loading flights\")\n        )\n    else:\n        results = [\n            self.process_one_flight(flight)\n            for flight in tqdm(self.flights_path_list, desc=\"Loading flights\")\n        ]\n\n    self.sequences = []\n    for seqs in results:\n        self.sequences.extend(seqs)\n\n    all_data = np.concatenate(\n        [\n            np.concatenate([seq[0], seq[1], seq[2], seq[3]], axis=1)\n            for seq in self.sequences\n        ],\n        axis=0,\n    )\n\n    all_cols = (\n        self.x_cols + self.u_cols + self.e0_cols + self.e_cols + self.deriv_cols\n    )\n\n    self.stats_dict = dict()\n\n    for i, col in enumerate(all_cols):\n        vals = all_data[:, i].astype(float)\n        self.stats_dict[col] = {\n            \"mean\": vals.mean(),\n            \"std\": vals.std() + 1e-6,\n            \"max\": np.percentile(np.abs(vals), 99.5),\n        }\n</code></pre>"},{"location":"reference/data/#node_fdm.data.dataset.SeqDataset.process_one_flight","title":"<code>process_one_flight(flight_path)</code>","text":"<p>Process a single flight file into clean, nan-free sequences.</p> <p>Parameters:</p> Name Type Description Default <code>flight_path</code> <code>str</code> <p>Path to a flight parquet file.</p> required <p>Returns:</p> Type Description <code>List[Tuple[ndarray, ndarray, ndarray, ndarray]]</code> <p>List of tuples containing state, control, environment, and derivative arrays.</p> Source code in <code>src/node_fdm/data/dataset.py</code> <pre><code>def process_one_flight(\n    self, flight_path: str\n) -&gt; List[Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]]:\n    \"\"\"Process a single flight file into clean, nan-free sequences.\n\n    Args:\n        flight_path: Path to a flight parquet file.\n\n    Returns:\n        List of tuples containing state, control, environment, and derivative arrays.\n    \"\"\"\n    f = self.read_flight(flight_path)\n    seqs = []\n    N = len(f)\n    if N &gt; self.seq_len:\n        x_seq = f[self.x_cols].values.astype(np.float32)\n        u_seq = f[self.u_cols].values.astype(np.float32)\n        e_seq = f[self.e0_cols + self.e_cols].values.astype(np.float32)\n        dx_seq = f[self.deriv_cols].values.astype(np.float32)\n\n        for start in range(0, N - self.seq_len + 1, self.shift):\n            custom_segment_filtering_bool = True\n            if self.custom_segment_filtering_fn is not None:\n                custom_segment_filtering_bool = self.custom_segment_filtering_fn(\n                    f, start, self.seq_len\n                )\n            nans = sum(\n                [\n                    np.isnan(seq[start : start + self.seq_len]).sum()\n                    for seq in [x_seq, u_seq, e_seq, dx_seq]\n                ]\n            )\n            if (custom_segment_filtering_bool) &amp; (nans == 0):\n                seqs.append(\n                    (\n                        x_seq[start : start + self.seq_len],\n                        u_seq[start : start + self.seq_len],\n                        e_seq[start : start + self.seq_len],\n                        dx_seq[start : start + self.seq_len],\n                    )\n                )\n    return seqs\n</code></pre>"},{"location":"reference/data/#node_fdm.data.dataset.SeqDataset.read_flight","title":"<code>read_flight(flight_path)</code>","text":"<p>Read a flight parquet file and apply base processing.</p> <p>Parameters:</p> Name Type Description Default <code>flight_path</code> <code>str</code> <p>Path to a parquet file containing flight data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Processed DataFrame with standardized columns.</p> Source code in <code>src/node_fdm/data/dataset.py</code> <pre><code>def read_flight(self, flight_path: str) -&gt; pd.DataFrame:\n    \"\"\"Read a flight parquet file and apply base processing.\n\n    Args:\n        flight_path: Path to a parquet file containing flight data.\n\n    Returns:\n        Processed DataFrame with standardized columns.\n    \"\"\"\n    f = pd.read_parquet(flight_path)\n    return self.processor.process_flight(f)\n</code></pre>"},{"location":"reference/data/#loader","title":"Loader","text":""},{"location":"reference/data/#node_fdm.data.loader","title":"<code>loader</code>","text":"<p>Helper for building train/validation datasets.</p>"},{"location":"reference/data/#node_fdm.data.loader.get_train_val_data","title":"<code>get_train_val_data(data_df, model_cols, shift=60, seq_len=60, custom_fn=(None, None), load_parallel=True, train_val_num=(5000, 500))</code>","text":"<p>Create training and validation datasets from a labeled file list.</p> <p>Parameters:</p> Name Type Description Default <code>data_df</code> <code>DataFrame</code> <p>DataFrame containing file paths with a <code>split</code> column.</p> required <code>model_cols</code> <p>Tuple containing model column groups.</p> required <code>shift</code> <code>int</code> <p>Window shift used when generating sequences.</p> <code>60</code> <code>seq_len</code> <code>int</code> <p>Sequence length for each sample.</p> <code>60</code> <code>custom_fn</code> <code>Tuple[Optional[Callable[[DataFrame], DataFrame]], Optional[Callable[..., bool]]]</code> <p>Tuple of optional processing and segment-filtering callables.</p> <code>(None, None)</code> <code>load_parallel</code> <code>bool</code> <p>Whether to load flights concurrently.</p> <code>True</code> <code>train_val_num</code> <code>Tuple[int, int]</code> <p>Maximum number of train and validation files to load.</p> <code>(5000, 500)</code> <p>Returns:</p> Type Description <code>Tuple[SeqDataset, SeqDataset]</code> <p>Tuple of training and validation SeqDataset instances.</p> Source code in <code>src/node_fdm/data/loader.py</code> <pre><code>def get_train_val_data(\n    data_df: pd.DataFrame,\n    model_cols,\n    shift: int = 60,\n    seq_len: int = 60,\n    custom_fn: Tuple[\n        Optional[Callable[[pd.DataFrame], pd.DataFrame]], Optional[Callable[..., bool]]\n    ] = (None, None),\n    load_parallel: bool = True,\n    train_val_num: Tuple[int, int] = (5000, 500),\n) -&gt; Tuple[SeqDataset, SeqDataset]:\n    \"\"\"Create training and validation datasets from a labeled file list.\n\n    Args:\n        data_df: DataFrame containing file paths with a `split` column.\n        model_cols: Tuple containing model column groups.\n        shift: Window shift used when generating sequences.\n        seq_len: Sequence length for each sample.\n        custom_fn: Tuple of optional processing and segment-filtering callables.\n        load_parallel: Whether to load flights concurrently.\n        train_val_num: Maximum number of train and validation files to load.\n\n    Returns:\n        Tuple of training and validation SeqDataset instances.\n    \"\"\"\n\n    train_files = data_df[data_df.split == \"train\"].filepath.tolist()\n    validation_files = data_df[data_df.split == \"val\"].filepath.tolist()\n\n    train_dataset = SeqDataset(\n        train_files[: train_val_num[0]],\n        model_cols,\n        seq_len=seq_len,\n        shift=shift,\n        custom_fn=custom_fn,\n        load_parallel=load_parallel,\n    )\n    val_dataset = SeqDataset(\n        validation_files[: train_val_num[1]],\n        model_cols,\n        seq_len=seq_len,\n        shift=shift,\n        custom_fn=custom_fn,\n        load_parallel=load_parallel,\n    )\n    return train_dataset, val_dataset\n</code></pre>"},{"location":"reference/models/","title":"\ud83e\udde0 Model Wrappers API","text":"<p>The <code>node_fdm.models</code> namespace provides the high-level PyTorch <code>nn.Module</code> wrappers that encapsulate the Neural ODE logic.</p> <p>These classes serve as the core mathematical engine of the framework. They handle the forward pass integration, the batch processing of trajectories, and the orchestration of the various layers defined in your architecture.</p>"},{"location":"reference/models/#class-reference","title":"\ud83d\udcd8 Class Reference","text":""},{"location":"reference/models/#flight-dynamics-model-base","title":"Flight Dynamics Model (Base)","text":"<p>The primary wrapper used during training. It manages the input encoding, connects to the ODE solver, and handles state reconstruction.</p>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model","title":"<code>flight_dynamics_model</code>","text":"<p>Neural flight dynamics model assembled from architecture layers.</p>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model.FlightDynamicsModel","title":"<code>FlightDynamicsModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Compute state derivatives using a layered flight dynamics architecture.</p> Source code in <code>src/node_fdm/models/flight_dynamics_model.py</code> <pre><code>class FlightDynamicsModel(nn.Module):\n    \"\"\"Compute state derivatives using a layered flight dynamics architecture.\"\"\"\n\n    def __init__(\n        self,\n        architecture: Sequence[Any],\n        stats_dict: Dict[Any, Dict[str, float]],\n        model_cols: Tuple[Any, Any, Any, Any, Any],\n        model_params: Sequence[int] = (2, 1, 48),\n    ) -&gt; None:\n        \"\"\"Initialize the model with architecture definition and statistics.\n\n        Args:\n            architecture: Iterable of layer definitions `(name, class, inputs, outputs, structured_flag)`.\n            stats_dict: Mapping from column to normalization/denormalization statistics.\n            model_cols: Tuple of model column groups (state, control, env, env_extra, derivatives).\n            model_params: Sequence defining backbone depth, head depth, and hidden width.\n        \"\"\"\n        super().__init__()\n        self.architecture = architecture\n        self.stats_dict = stats_dict\n        self.x_cols, self.u_cols, self.e0_cols, self.e_cols, self.dx_cols = model_cols\n        self.backbone_depth, self.head_depth, self.neurons_num = model_params\n        self.layers_dict = nn.ModuleDict({})\n        self.layers_name = []\n\n        for name, layer_class, input_cols, ouput_cols, structured in self.architecture:\n            self.layers_name.append(name)\n            if structured:\n                self.layers_dict[name] = self.create_structured_layer(\n                    input_cols,\n                    ouput_cols,\n                    layer_class=layer_class,\n                )\n            else:\n                self.layers_dict[name] = layer_class()\n\n    def reset_history(self):\n        \"\"\"Reset internal history buffers.\n\n        Clears stored layer outputs used for debugging or analysis between runs.\n        \"\"\"\n        self.history = {}\n\n    def create_structured_layer(\n        self,\n        input_cols: Sequence[Any],\n        output_cols: Sequence[Any],\n        layer_class: Any = StructuredLayer,\n    ) -&gt; nn.Module:\n        \"\"\"Build a structured layer with normalization and denormalization stats.\n\n        Args:\n            input_cols: Columns consumed by the layer.\n            output_cols: Columns produced by the layer.\n            layer_class: Layer implementation to instantiate.\n\n        Returns:\n            Configured structured layer instance.\n        \"\"\"\n        input_stats = [\n            {\n                col.col_name: self.stats_dict[col][metric]\n                for col in input_cols\n                if col.normalize_mode is not None\n            }\n            for metric in [\"mean\", \"std\"]\n        ]\n        output_stats = [\n            {\n                col.col_name: self.stats_dict[col][metric]\n                for col in output_cols\n                if col.denormalize_mode is not None\n            }\n            for metric in [\"mean\", \"std\", \"max\"]\n        ]\n\n        layer = layer_class(\n            input_cols,\n            input_stats,\n            output_cols,\n            output_stats,\n            backbone_dim=self.neurons_num,\n            backbone_depth=self.backbone_depth,\n            head_dim=self.neurons_num // 2,\n            head_depth=self.head_depth,\n        )\n\n        return layer\n\n    def forward(\n        self, x: torch.Tensor, u_t: torch.Tensor, e_t: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute state derivatives for the current batch.\n\n        Args:\n            x: State tensor.\n            u_t: Control tensor interpolated at current time.\n            e_t: Environment tensor interpolated at current time.\n\n        Returns:\n            Tensor of state derivatives assembled from architecture outputs.\n        \"\"\"\n\n        vects = torch.cat([x, u_t, e_t], dim=1)\n        vect_dict = dict()\n        for i, col in enumerate(self.x_cols + self.u_cols + self.e0_cols):\n            vect_dict[col] = vects[..., i]\n\n        for name in self.layers_name:\n            vect_dict = vect_dict | self.layers_dict[name](vect_dict)\n\n        ode_output = torch.stack(\n            [coeff * vect_dict[col] for coeff, col in self.dx_cols],\n            dim=1,\n        )\n\n        for col, vect in vect_dict.items():\n            if torch.isnan(vect).any():\n                pass\n            if col in self.history.keys():\n                self.history[col] = torch.cat(\n                    [self.history[col], vect.unsqueeze(1)], dim=1\n                )\n            else:\n                self.history[col] = vect.unsqueeze(1)\n\n        return ode_output\n</code></pre>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model.FlightDynamicsModel.__init__","title":"<code>__init__(architecture, stats_dict, model_cols, model_params=(2, 1, 48))</code>","text":"<p>Initialize the model with architecture definition and statistics.</p> <p>Parameters:</p> Name Type Description Default <code>architecture</code> <code>Sequence[Any]</code> <p>Iterable of layer definitions <code>(name, class, inputs, outputs, structured_flag)</code>.</p> required <code>stats_dict</code> <code>Dict[Any, Dict[str, float]]</code> <p>Mapping from column to normalization/denormalization statistics.</p> required <code>model_cols</code> <code>Tuple[Any, Any, Any, Any, Any]</code> <p>Tuple of model column groups (state, control, env, env_extra, derivatives).</p> required <code>model_params</code> <code>Sequence[int]</code> <p>Sequence defining backbone depth, head depth, and hidden width.</p> <code>(2, 1, 48)</code> Source code in <code>src/node_fdm/models/flight_dynamics_model.py</code> <pre><code>def __init__(\n    self,\n    architecture: Sequence[Any],\n    stats_dict: Dict[Any, Dict[str, float]],\n    model_cols: Tuple[Any, Any, Any, Any, Any],\n    model_params: Sequence[int] = (2, 1, 48),\n) -&gt; None:\n    \"\"\"Initialize the model with architecture definition and statistics.\n\n    Args:\n        architecture: Iterable of layer definitions `(name, class, inputs, outputs, structured_flag)`.\n        stats_dict: Mapping from column to normalization/denormalization statistics.\n        model_cols: Tuple of model column groups (state, control, env, env_extra, derivatives).\n        model_params: Sequence defining backbone depth, head depth, and hidden width.\n    \"\"\"\n    super().__init__()\n    self.architecture = architecture\n    self.stats_dict = stats_dict\n    self.x_cols, self.u_cols, self.e0_cols, self.e_cols, self.dx_cols = model_cols\n    self.backbone_depth, self.head_depth, self.neurons_num = model_params\n    self.layers_dict = nn.ModuleDict({})\n    self.layers_name = []\n\n    for name, layer_class, input_cols, ouput_cols, structured in self.architecture:\n        self.layers_name.append(name)\n        if structured:\n            self.layers_dict[name] = self.create_structured_layer(\n                input_cols,\n                ouput_cols,\n                layer_class=layer_class,\n            )\n        else:\n            self.layers_dict[name] = layer_class()\n</code></pre>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model.FlightDynamicsModel.create_structured_layer","title":"<code>create_structured_layer(input_cols, output_cols, layer_class=StructuredLayer)</code>","text":"<p>Build a structured layer with normalization and denormalization stats.</p> <p>Parameters:</p> Name Type Description Default <code>input_cols</code> <code>Sequence[Any]</code> <p>Columns consumed by the layer.</p> required <code>output_cols</code> <code>Sequence[Any]</code> <p>Columns produced by the layer.</p> required <code>layer_class</code> <code>Any</code> <p>Layer implementation to instantiate.</p> <code>StructuredLayer</code> <p>Returns:</p> Type Description <code>Module</code> <p>Configured structured layer instance.</p> Source code in <code>src/node_fdm/models/flight_dynamics_model.py</code> <pre><code>def create_structured_layer(\n    self,\n    input_cols: Sequence[Any],\n    output_cols: Sequence[Any],\n    layer_class: Any = StructuredLayer,\n) -&gt; nn.Module:\n    \"\"\"Build a structured layer with normalization and denormalization stats.\n\n    Args:\n        input_cols: Columns consumed by the layer.\n        output_cols: Columns produced by the layer.\n        layer_class: Layer implementation to instantiate.\n\n    Returns:\n        Configured structured layer instance.\n    \"\"\"\n    input_stats = [\n        {\n            col.col_name: self.stats_dict[col][metric]\n            for col in input_cols\n            if col.normalize_mode is not None\n        }\n        for metric in [\"mean\", \"std\"]\n    ]\n    output_stats = [\n        {\n            col.col_name: self.stats_dict[col][metric]\n            for col in output_cols\n            if col.denormalize_mode is not None\n        }\n        for metric in [\"mean\", \"std\", \"max\"]\n    ]\n\n    layer = layer_class(\n        input_cols,\n        input_stats,\n        output_cols,\n        output_stats,\n        backbone_dim=self.neurons_num,\n        backbone_depth=self.backbone_depth,\n        head_dim=self.neurons_num // 2,\n        head_depth=self.head_depth,\n    )\n\n    return layer\n</code></pre>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model.FlightDynamicsModel.forward","title":"<code>forward(x, u_t, e_t)</code>","text":"<p>Compute state derivatives for the current batch.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>State tensor.</p> required <code>u_t</code> <code>Tensor</code> <p>Control tensor interpolated at current time.</p> required <code>e_t</code> <code>Tensor</code> <p>Environment tensor interpolated at current time.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of state derivatives assembled from architecture outputs.</p> Source code in <code>src/node_fdm/models/flight_dynamics_model.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, u_t: torch.Tensor, e_t: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Compute state derivatives for the current batch.\n\n    Args:\n        x: State tensor.\n        u_t: Control tensor interpolated at current time.\n        e_t: Environment tensor interpolated at current time.\n\n    Returns:\n        Tensor of state derivatives assembled from architecture outputs.\n    \"\"\"\n\n    vects = torch.cat([x, u_t, e_t], dim=1)\n    vect_dict = dict()\n    for i, col in enumerate(self.x_cols + self.u_cols + self.e0_cols):\n        vect_dict[col] = vects[..., i]\n\n    for name in self.layers_name:\n        vect_dict = vect_dict | self.layers_dict[name](vect_dict)\n\n    ode_output = torch.stack(\n        [coeff * vect_dict[col] for coeff, col in self.dx_cols],\n        dim=1,\n    )\n\n    for col, vect in vect_dict.items():\n        if torch.isnan(vect).any():\n            pass\n        if col in self.history.keys():\n            self.history[col] = torch.cat(\n                [self.history[col], vect.unsqueeze(1)], dim=1\n            )\n        else:\n            self.history[col] = vect.unsqueeze(1)\n\n    return ode_output\n</code></pre>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model.FlightDynamicsModel.reset_history","title":"<code>reset_history()</code>","text":"<p>Reset internal history buffers.</p> <p>Clears stored layer outputs used for debugging or analysis between runs.</p> Source code in <code>src/node_fdm/models/flight_dynamics_model.py</code> <pre><code>def reset_history(self):\n    \"\"\"Reset internal history buffers.\n\n    Clears stored layer outputs used for debugging or analysis between runs.\n    \"\"\"\n    self.history = {}\n</code></pre>"},{"location":"reference/models/#batch-neural-ode","title":"Batch Neural ODE","text":"<p>The core utility responsible for solving the system of differential equations over batches of time sequences. It interfaces with the numerical solvers (Euler, RK4).</p>"},{"location":"reference/models/#node_fdm.models.batch_neural_ode","title":"<code>batch_neural_ode</code>","text":"<p>Batch-compatible Neural ODE wrapper that interpolates inputs over time.</p>"},{"location":"reference/models/#node_fdm.models.batch_neural_ode.BatchNeuralODE","title":"<code>BatchNeuralODE</code>","text":"<p>               Bases: <code>Module</code></p> <p>Wrap a neural ODE with batched control and environment inputs.</p> Source code in <code>src/node_fdm/models/batch_neural_ode.py</code> <pre><code>class BatchNeuralODE(nn.Module):\n    \"\"\"Wrap a neural ODE with batched control and environment inputs.\"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        u_seq: torch.Tensor,\n        e_seq: torch.Tensor,\n        t_grid: torch.Tensor,\n    ) -&gt; None:\n        \"\"\"Initialize the ODE wrapper and reset model history.\n\n        Args:\n            model: Base neural ODE model taking `(x, u_t, e_t)`.\n            u_seq: Control inputs over time with shape `(batch, time, features)`.\n            e_seq: Environment inputs over time with shape `(batch, time, features)`.\n            t_grid: Monotonic time grid corresponding to `u_seq` and `e_seq`.\n        \"\"\"\n        super().__init__()\n        self.model = model\n        self.model.reset_history()\n        self.u_seq = u_seq\n        self.e_seq = e_seq\n        self.t_grid = t_grid\n\n    def forward(self, t: torch.Tensor, x: torch.Tensor) -&gt; Any:\n        \"\"\"Evaluate the ODE dynamics at time `t` with linear interpolation.\n\n        Args:\n            t: Scalar tensor containing the evaluation time.\n            x: Current state tensor.\n\n        Returns:\n            Model output of the wrapped dynamics at time `t`.\n        \"\"\"\n        t = t.item()\n        idx = torch.searchsorted(\n            self.t_grid, torch.tensor(t, device=self.t_grid.device)\n        ).item()\n        idx0 = max(0, idx - 1)\n        idx1 = min(idx, self.t_grid.shape[0] - 1)\n\n        t0, t1 = self.t_grid[idx0].item(), self.t_grid[idx1].item()\n        alpha = 0 if t1 == t0 else (t - t0) / (t1 - t0)\n\n        u0, u1 = self.u_seq[:, idx0, :], self.u_seq[:, idx1, :]\n        e0, e1 = self.e_seq[:, idx0, :], self.e_seq[:, idx1, :]\n\n        u_t = (1 - alpha) * u0 + alpha * u1\n        e_t = (1 - alpha) * e0 + alpha * e1\n\n        return self.model(x, u_t, e_t)\n</code></pre>"},{"location":"reference/models/#node_fdm.models.batch_neural_ode.BatchNeuralODE.__init__","title":"<code>__init__(model, u_seq, e_seq, t_grid)</code>","text":"<p>Initialize the ODE wrapper and reset model history.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Base neural ODE model taking <code>(x, u_t, e_t)</code>.</p> required <code>u_seq</code> <code>Tensor</code> <p>Control inputs over time with shape <code>(batch, time, features)</code>.</p> required <code>e_seq</code> <code>Tensor</code> <p>Environment inputs over time with shape <code>(batch, time, features)</code>.</p> required <code>t_grid</code> <code>Tensor</code> <p>Monotonic time grid corresponding to <code>u_seq</code> and <code>e_seq</code>.</p> required Source code in <code>src/node_fdm/models/batch_neural_ode.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    u_seq: torch.Tensor,\n    e_seq: torch.Tensor,\n    t_grid: torch.Tensor,\n) -&gt; None:\n    \"\"\"Initialize the ODE wrapper and reset model history.\n\n    Args:\n        model: Base neural ODE model taking `(x, u_t, e_t)`.\n        u_seq: Control inputs over time with shape `(batch, time, features)`.\n        e_seq: Environment inputs over time with shape `(batch, time, features)`.\n        t_grid: Monotonic time grid corresponding to `u_seq` and `e_seq`.\n    \"\"\"\n    super().__init__()\n    self.model = model\n    self.model.reset_history()\n    self.u_seq = u_seq\n    self.e_seq = e_seq\n    self.t_grid = t_grid\n</code></pre>"},{"location":"reference/models/#node_fdm.models.batch_neural_ode.BatchNeuralODE.forward","title":"<code>forward(t, x)</code>","text":"<p>Evaluate the ODE dynamics at time <code>t</code> with linear interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>Scalar tensor containing the evaluation time.</p> required <code>x</code> <code>Tensor</code> <p>Current state tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Model output of the wrapped dynamics at time <code>t</code>.</p> Source code in <code>src/node_fdm/models/batch_neural_ode.py</code> <pre><code>def forward(self, t: torch.Tensor, x: torch.Tensor) -&gt; Any:\n    \"\"\"Evaluate the ODE dynamics at time `t` with linear interpolation.\n\n    Args:\n        t: Scalar tensor containing the evaluation time.\n        x: Current state tensor.\n\n    Returns:\n        Model output of the wrapped dynamics at time `t`.\n    \"\"\"\n    t = t.item()\n    idx = torch.searchsorted(\n        self.t_grid, torch.tensor(t, device=self.t_grid.device)\n    ).item()\n    idx0 = max(0, idx - 1)\n    idx1 = min(idx, self.t_grid.shape[0] - 1)\n\n    t0, t1 = self.t_grid[idx0].item(), self.t_grid[idx1].item()\n    alpha = 0 if t1 == t0 else (t - t0) / (t1 - t0)\n\n    u0, u1 = self.u_seq[:, idx0, :], self.u_seq[:, idx1, :]\n    e0, e1 = self.e_seq[:, idx0, :], self.e_seq[:, idx1, :]\n\n    u_t = (1 - alpha) * u0 + alpha * u1\n    e_t = (1 - alpha) * e0 + alpha * e1\n\n    return self.model(x, u_t, e_t)\n</code></pre>"},{"location":"reference/models/#production-model","title":"Production Model","text":"<p>An optimized wrapper designed strictly for inference environments. It streamlines the forward pass by removing training-specific hooks and overhead.</p>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model_prod","title":"<code>flight_dynamics_model_prod</code>","text":"<p>Production-ready flight dynamics model loader and evaluator.</p>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model_prod.FlightDynamicsModelProd","title":"<code>FlightDynamicsModelProd</code>","text":"<p>               Bases: <code>Module</code></p> <p>Load pretrained flight dynamics layers and expose an evaluation interface.</p> Source code in <code>src/node_fdm/models/flight_dynamics_model_prod.py</code> <pre><code>class FlightDynamicsModelProd(nn.Module):\n    \"\"\"Load pretrained flight dynamics layers and expose an evaluation interface.\"\"\"\n\n    def __init__(\n        self,\n        model_path: Any,\n    ) -&gt; None:\n        \"\"\"Initialize and load pretrained layers from a model directory.\n\n        Args:\n            model_path: Path-like pointing to the directory containing checkpoints and meta.json.\n        \"\"\"\n        super().__init__()\n        self.model_path = model_path\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        meta_path = model_path / \"meta.json\"\n        self.architecture, self.model_cols, model_params, self.stats_dict = (\n            get_architecture_params_from_meta(meta_path)\n        )\n        self.backbone_depth, self.head_depth, self.neurons_num = model_params\n        self.layers_dict = nn.ModuleDict({})\n        self.layers_name = []\n        for name, layer_class, input_cols, ouput_cols, structured in self.architecture:\n            self.layers_name.append(name)\n            if structured:\n                self.layers_dict[name] = self.create_structured_layer(\n                    input_cols,\n                    ouput_cols,\n                    layer_class=layer_class,\n                )\n            else:\n                self.layers_dict[name] = layer_class()\n            if name != \"trajectory\":\n                checkpoint = self.load_layer_checkpoint(name)\n                self.layers_dict[name].load_state_dict(\n                    checkpoint[\"layer_state\"], strict=False\n                )\n                self.layers_dict[name] = self.layers_dict[name].eval()\n\n    def load_layer_checkpoint(self, layer_name: str) -&gt; Any:\n        \"\"\"Load checkpoint for a given layer.\n\n        Args:\n            layer_name: Name of the layer whose weights should be loaded.\n\n        Returns:\n            Loaded checkpoint dictionary.\n        \"\"\"\n        path = os.path.join(self.model_path, f\"{layer_name}.pt\")\n        checkpoint = torch.load(path, map_location=self.device)\n        return checkpoint\n\n    def create_structured_layer(\n        self,\n        input_cols: Sequence[Any],\n        output_cols: Sequence[Any],\n        layer_class: Any = StructuredLayer,\n    ) -&gt; nn.Module:\n        \"\"\"Build a structured layer with normalization and denormalization stats.\n\n        Args:\n            input_cols: Columns consumed by the layer.\n            output_cols: Columns produced by the layer.\n            layer_class: Layer implementation to instantiate.\n\n        Returns:\n            Configured structured layer instance.\n        \"\"\"\n        input_stats = [\n            {\n                col.col_name: self.stats_dict[col][metric]\n                for col in input_cols\n                if col.normalize_mode is not None\n            }\n            for metric in [\"mean\", \"std\"]\n        ]\n        output_stats = [\n            {\n                col.col_name: self.stats_dict[col][metric]\n                for col in output_cols\n                if col.denormalize_mode is not None\n            }\n            for metric in [\"mean\", \"std\", \"max\"]\n        ]\n\n        layer = layer_class(\n            input_cols,\n            input_stats,\n            output_cols,\n            output_stats,\n            backbone_dim=self.neurons_num,\n            backbone_depth=self.backbone_depth,\n            head_dim=self.neurons_num // 2,\n            head_depth=self.head_depth,\n        )\n\n        return layer\n\n    def forward(self, vect_dict: dict) -&gt; dict:\n        \"\"\"Run a forward pass through all layers.\n\n        Args:\n            vect_dict: Mapping from column identifiers to tensors.\n\n        Returns:\n            Updated mapping with newly computed columns.\n        \"\"\"\n        for name in self.layers_name:\n            res = self.layers_dict[name](vect_dict)\n            vect_dict |= res\n\n        return vect_dict\n</code></pre>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model_prod.FlightDynamicsModelProd.__init__","title":"<code>__init__(model_path)</code>","text":"<p>Initialize and load pretrained layers from a model directory.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Any</code> <p>Path-like pointing to the directory containing checkpoints and meta.json.</p> required Source code in <code>src/node_fdm/models/flight_dynamics_model_prod.py</code> <pre><code>def __init__(\n    self,\n    model_path: Any,\n) -&gt; None:\n    \"\"\"Initialize and load pretrained layers from a model directory.\n\n    Args:\n        model_path: Path-like pointing to the directory containing checkpoints and meta.json.\n    \"\"\"\n    super().__init__()\n    self.model_path = model_path\n    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    meta_path = model_path / \"meta.json\"\n    self.architecture, self.model_cols, model_params, self.stats_dict = (\n        get_architecture_params_from_meta(meta_path)\n    )\n    self.backbone_depth, self.head_depth, self.neurons_num = model_params\n    self.layers_dict = nn.ModuleDict({})\n    self.layers_name = []\n    for name, layer_class, input_cols, ouput_cols, structured in self.architecture:\n        self.layers_name.append(name)\n        if structured:\n            self.layers_dict[name] = self.create_structured_layer(\n                input_cols,\n                ouput_cols,\n                layer_class=layer_class,\n            )\n        else:\n            self.layers_dict[name] = layer_class()\n        if name != \"trajectory\":\n            checkpoint = self.load_layer_checkpoint(name)\n            self.layers_dict[name].load_state_dict(\n                checkpoint[\"layer_state\"], strict=False\n            )\n            self.layers_dict[name] = self.layers_dict[name].eval()\n</code></pre>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model_prod.FlightDynamicsModelProd.create_structured_layer","title":"<code>create_structured_layer(input_cols, output_cols, layer_class=StructuredLayer)</code>","text":"<p>Build a structured layer with normalization and denormalization stats.</p> <p>Parameters:</p> Name Type Description Default <code>input_cols</code> <code>Sequence[Any]</code> <p>Columns consumed by the layer.</p> required <code>output_cols</code> <code>Sequence[Any]</code> <p>Columns produced by the layer.</p> required <code>layer_class</code> <code>Any</code> <p>Layer implementation to instantiate.</p> <code>StructuredLayer</code> <p>Returns:</p> Type Description <code>Module</code> <p>Configured structured layer instance.</p> Source code in <code>src/node_fdm/models/flight_dynamics_model_prod.py</code> <pre><code>def create_structured_layer(\n    self,\n    input_cols: Sequence[Any],\n    output_cols: Sequence[Any],\n    layer_class: Any = StructuredLayer,\n) -&gt; nn.Module:\n    \"\"\"Build a structured layer with normalization and denormalization stats.\n\n    Args:\n        input_cols: Columns consumed by the layer.\n        output_cols: Columns produced by the layer.\n        layer_class: Layer implementation to instantiate.\n\n    Returns:\n        Configured structured layer instance.\n    \"\"\"\n    input_stats = [\n        {\n            col.col_name: self.stats_dict[col][metric]\n            for col in input_cols\n            if col.normalize_mode is not None\n        }\n        for metric in [\"mean\", \"std\"]\n    ]\n    output_stats = [\n        {\n            col.col_name: self.stats_dict[col][metric]\n            for col in output_cols\n            if col.denormalize_mode is not None\n        }\n        for metric in [\"mean\", \"std\", \"max\"]\n    ]\n\n    layer = layer_class(\n        input_cols,\n        input_stats,\n        output_cols,\n        output_stats,\n        backbone_dim=self.neurons_num,\n        backbone_depth=self.backbone_depth,\n        head_dim=self.neurons_num // 2,\n        head_depth=self.head_depth,\n    )\n\n    return layer\n</code></pre>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model_prod.FlightDynamicsModelProd.forward","title":"<code>forward(vect_dict)</code>","text":"<p>Run a forward pass through all layers.</p> <p>Parameters:</p> Name Type Description Default <code>vect_dict</code> <code>dict</code> <p>Mapping from column identifiers to tensors.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Updated mapping with newly computed columns.</p> Source code in <code>src/node_fdm/models/flight_dynamics_model_prod.py</code> <pre><code>def forward(self, vect_dict: dict) -&gt; dict:\n    \"\"\"Run a forward pass through all layers.\n\n    Args:\n        vect_dict: Mapping from column identifiers to tensors.\n\n    Returns:\n        Updated mapping with newly computed columns.\n    \"\"\"\n    for name in self.layers_name:\n        res = self.layers_dict[name](vect_dict)\n        vect_dict |= res\n\n    return vect_dict\n</code></pre>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model_prod.FlightDynamicsModelProd.load_layer_checkpoint","title":"<code>load_layer_checkpoint(layer_name)</code>","text":"<p>Load checkpoint for a given layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer_name</code> <code>str</code> <p>Name of the layer whose weights should be loaded.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Loaded checkpoint dictionary.</p> Source code in <code>src/node_fdm/models/flight_dynamics_model_prod.py</code> <pre><code>def load_layer_checkpoint(self, layer_name: str) -&gt; Any:\n    \"\"\"Load checkpoint for a given layer.\n\n    Args:\n        layer_name: Name of the layer whose weights should be loaded.\n\n    Returns:\n        Loaded checkpoint dictionary.\n    \"\"\"\n    path = os.path.join(self.model_path, f\"{layer_name}.pt\")\n    checkpoint = torch.load(path, map_location=self.device)\n    return checkpoint\n</code></pre>"},{"location":"reference/node_fdm/","title":"\ud83d\udcda API Overview","text":"<p>The node_fdm package is designed with modularity in mind. It separates data handling, physical/neural architectures, and the training/inference engines into distinct namespaces.</p>"},{"location":"reference/node_fdm/#module-map","title":"\ud83d\uddfa\ufe0f Module Map","text":"<p>Here is a high-level view of how the sub-packages interact to form a complete pipeline:</p> <pre><code>graph LR\n    %% Data Flow\n    Data[node_fdm.data] --&gt; Trainer\n    Data --&gt; Predictor\n\n    %% Logic Flow\n    Arch[node_fdm.architectures] --&gt;|Defines| Model[node_fdm.models]\n\n    %% Execution Flow\n    Model --&gt;|Instantiated by| Trainer[node_fdm.ode_trainer]\n    Model --&gt;|Used by| Predictor[node_fdm.predictor]\n\n    %% Styling\n    classDef package fill:#e1f5fe,stroke:#01579b,stroke-width:2px;\n    class Data,Arch,Model,Trainer,Predictor package;</code></pre>"},{"location":"reference/node_fdm/#core-namespaces","title":"\ud83d\udce6 Core Namespaces","text":"Module Description Key Classes <code>node_fdm.ode_trainer</code> The Training Engine.Handles the training loop, validation, and PyTorch Lightning integration. <code>ODETrainer</code> <code>node_fdm.predictor</code> The Inference Engine.Wraps trained models to perform trajectory rollouts and simulations. <code>NodeFDMPredictor</code> <code>node_fdm.data</code> Data Pipeline.Tools for dataset construction, loading, and batching. <code>SeqDataset</code>, <code>FlightProcessor</code> <code>node_fdm.architectures</code> The Registry.Contains the built-in definitions (<code>opensky_2025</code>, <code>qar</code>) and the mapping logic. <code>mapping.py</code>, <code>columns.py</code> <code>node_fdm.models</code> Model Wrappers.The underlying PyTorch modules and ODE integration utilities. <code>StructuredLayer</code>, <code>PhysicsLayer</code>"},{"location":"reference/ode_trainer/","title":"\ud83c\udfcb\ufe0f ODETrainer API","text":"<p>The <code>ODETrainer</code> class acts as the high-level orchestrator for the training pipeline. It wraps PyTorch Lightning to provide a standardized interface for training Neural ODEs on flight data.</p> <p>It functions as the central bridge in the pipeline: it validates the configuration, retrieves the specific model architecture from the registry, initializes the training environment, and manages the lifecycle of model checkpoints and metadata artifacts.</p>"},{"location":"reference/ode_trainer/#class-reference","title":"\ud83d\udcd8 Class Reference","text":""},{"location":"reference/ode_trainer/#node_fdm.ode_trainer","title":"<code>ode_trainer</code>","text":"<p>Training utilities for neural ODE-based flight dynamics models.</p>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer","title":"<code>ODETrainer</code>","text":"<p>Handle data preparation, training loops, and checkpointing for ODE models.</p> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>class ODETrainer:\n    \"\"\"Handle data preparation, training loops, and checkpointing for ODE models.\"\"\"\n\n    def __init__(\n        self,\n        data_df: pd.DataFrame,\n        model_config: Dict[str, Any],\n        model_dir: Any,\n        num_workers: int = 4,\n        load_parallel: bool = True,\n        train_val_num: Tuple[int, int] = (5000, 500),\n    ) -&gt; None:\n        \"\"\"Initialize trainer with data, model configuration, and I/O paths.\n\n        Args:\n            data_df: DataFrame containing file paths and split labels.\n            model_config: Dictionary describing architecture, hyperparameters, and loader settings.\n            model_dir: Base directory to store checkpoints and metadata.\n            num_workers: Number of workers for DataLoaders.\n            load_parallel: Whether to load flights in parallel.\n            train_val_num: Tuple specifying how many train/val files to load.\n        \"\"\"\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        architecture, self.model_cols, custom_fn = get_architecture_from_name(\n            model_config[\"architecture_name\"]\n        )\n        self.x_cols, self.u_cols, self.e0_cols, self.e_cols, self.dx_cols = (\n            self.model_cols\n        )\n        self.model_dir = model_dir / model_config[\"model_name\"]\n        os.makedirs(self.model_dir, exist_ok=True)\n        self.architecture = architecture\n        self.model_config = model_config\n        self.architecture_name = model_config[\"architecture_name\"]\n        self.model_params = model_config[\"model_params\"]\n\n        self.train_dataset, self.val_dataset = get_train_val_data(\n            data_df,\n            self.model_cols,\n            shift=model_config[\"shift\"],\n            seq_len=model_config[\"seq_len\"],\n            custom_fn=custom_fn,\n            load_parallel=load_parallel,\n            train_val_num=train_val_num,\n        )\n        self.step = model_config[\"step\"]\n        self.num_workers = num_workers\n\n        self.stats_dict = self.train_dataset.stats_dict\n\n        self.model = self.get_or_create_model(*model_config[\"loading_args\"])\n\n        self.optimizer = torch.optim.AdamW(\n            self.model.parameters(),\n            lr=model_config[\"lr\"],\n            weight_decay=model_config[\"weight_decay\"],\n        )\n        self.epoch = 1\n        self.save_meta()\n\n    def get_or_create_model(\n        self, load: bool = False, load_loss: bool = False\n    ) -&gt; FlightDynamicsModel:\n        \"\"\"Instantiate a new model or load existing checkpoints.\n\n        Args:\n            load: Whether to attempt loading existing checkpoints.\n            load_loss: Whether to restore tracked best validation loss when loading.\n\n        Returns:\n            Initialized or restored `FlightDynamicsModel` instance.\n        \"\"\"\n        self.best_val_loss = float(\"inf\")\n        if load and os.path.exists(self.model_dir / \"meta.json\"):\n            model = self.load_best_checkpoint(load_loss=load_loss)\n        else:\n            print(\"Creating new model.\")\n            model = FlightDynamicsModel(\n                self.architecture,\n                self.stats_dict,\n                self.model_cols,\n                model_params=self.model_params,\n            ).to(self.device)\n        return model\n\n    def load_best_checkpoint(self, load_loss: bool = False) -&gt; FlightDynamicsModel:\n        \"\"\"Create and populate a model from saved checkpoints.\n\n        Args:\n            load_loss: Whether to restore tracked best validation loss.\n\n        Returns:\n            Model with layer weights loaded when available.\n        \"\"\"\n        model = FlightDynamicsModel(\n            self.architecture,\n            self.stats_dict,\n            self.model_cols,\n            model_params=self.model_params,\n        ).to(self.device)\n\n        for name in model.layers_name:\n            checkpoint = self.load_layer_checkpoint(name)\n            if checkpoint is not None:\n                model.layers_dict[name].load_state_dict(\n                    checkpoint[\"layer_state\"], strict=False\n                )\n                best_val_loss = checkpoint.get(\"best_val_loss\", float(\"inf\"))\n                self.epoch = checkpoint.get(\"epoch\", 0)\n            else:\n                best_val_loss = float(\"inf\")\n                self.epoch = 0\n\n        if load_loss:\n            self.best_val_loss = best_val_loss\n\n        print(\"Best val loss per layer:\", self.best_val_loss)\n        print(f\"Loaded modular model from {self.model_dir}\")\n\n        return model\n\n    def load_layer_checkpoint(self, layer_name: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Load checkpoint dictionary for a specific layer if available.\n\n        Args:\n            layer_name: Name of the layer to load.\n\n        Returns:\n            Checkpoint dictionary if found, otherwise None.\n        \"\"\"\n        path = os.path.join(self.model_dir, f\"{layer_name}.pt\")\n        if not os.path.exists(path):\n            print(f\"No checkpoint found for layer {layer_name}, skipping load.\")\n            return None\n        else:\n            print(f\"checkpoint found for layer {layer_name}\")\n        checkpoint = torch.load(path, map_location=self.device)\n        return checkpoint\n\n    def save_meta(self) -&gt; None:\n        \"\"\"Persist training metadata and statistics to disk.\n\n        Creates or updates `meta.json` within the model directory.\n        \"\"\"\n        saved_stats_dict = {str(col): value for col, value in self.stats_dict.items()}\n\n        meta_dict = {\n            \"architecture_name\": self.architecture_name,\n            \"model_params\": self.model_config[\"model_params\"],\n            \"step\": self.model_config[\"step\"],\n            \"shift\": self.model_config[\"shift\"],\n            \"lr\": self.model_config[\"lr\"],\n            \"seq_len\": self.model_config[\"seq_len\"],\n            \"batch_size\": self.model_config[\"batch_size\"],\n            \"stats_dict\": saved_stats_dict,\n        }\n        print(self.model_dir / \"meta.json\")\n        with open(self.model_dir / \"meta.json\", \"w\") as f:\n            json.dump(meta_dict, f, indent=4)\n\n    def save_layer_checkpoint(self, layer_name: str, epoch: int) -&gt; None:\n        \"\"\"Save checkpoint for an individual layer.\n\n        Args:\n            layer_name: Name of the layer to checkpoint.\n            epoch: Current epoch offset for tracking.\n        \"\"\"\n        layer = self.model.layers_dict[layer_name]\n        save_dict = {\n            \"layer_state\": layer.state_dict(),\n            \"optimizer_state\": self.optimizer.state_dict(),\n            \"best_val_loss\": self.best_val_loss,\n            \"epoch\": self.epoch + epoch,\n        }\n        torch.save(save_dict, self.model_dir / f\"{layer_name}.pt\")\n\n    def save_model(self, epoch: int) -&gt; None:\n        \"\"\"Save checkpoints for all layers.\n\n        Args:\n            epoch: Epoch index used when saving checkpoints.\n        \"\"\"\n        for name in self.model.layers_name:\n            self.save_layer_checkpoint(name, epoch)\n\n    def norm_vect(self, vect: torch.Tensor, col: Any) -&gt; torch.Tensor:\n        \"\"\"Normalize tensor using stored statistics for a column.\n\n        Args:\n            vect: Tensor to normalize.\n            col: Column identifier used to fetch statistics.\n\n        Returns:\n            Normalized tensor.\n        \"\"\"\n        return (vect - self.stats_dict[col][\"mean\"]) / (\n            self.stats_dict[col][\"std\"] + 1e-3\n        )\n\n    def cat_to_dict_vects(\n        self,\n        vect_list: Sequence[torch.Tensor],\n        col_list: Sequence[Any],\n        alpha_dict: Dict[Any, float],\n        normalize: bool = True,\n    ) -&gt; Dict[Any, torch.Tensor]:\n        \"\"\"Concatenate vectors and build a dict keyed by column definitions.\n\n        Args:\n            vect_list: Sequence of tensors to concatenate along the feature axis.\n            col_list: Column identifiers matching the concatenated tensors.\n            alpha_dict: Optional scaling factors applied per column.\n            normalize: Whether to normalize columns that request it.\n\n        Returns:\n            Dictionary mapping columns to (optionally) scaled and normalized tensors.\n        \"\"\"\n\n        def modifier(el: torch.Tensor, col: Any) -&gt; torch.Tensor:\n            if (col.normalize_mode == \"normal\") &amp; (normalize):\n                return self.norm_vect(el, col)\n            return el\n\n        coeff_list = [\n            alpha_dict[col] if col in alpha_dict.keys() else 0.0 for col in col_list\n        ]\n\n        vects = torch.cat(vect_list, dim=2)\n        vects_dict = {\n            col: coeff * modifier(vects[..., i], col).unsqueeze(-1)\n            for i, (col, coeff) in enumerate(zip(col_list, coeff_list))\n        }\n        return vects_dict\n\n    def ode_step(\n        self,\n        x_seq: torch.Tensor,\n        u_seq: torch.Tensor,\n        e_seq: torch.Tensor,\n        method: str,\n        alpha_dict: Dict[Any, float],\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, Sequence[Any]]:\n        \"\"\"Integrate one ODE step and return true/predicted trajectories.\n\n        Args:\n            x_seq: State sequences for the batch.\n            u_seq: Control sequences for the batch.\n            e_seq: Environment sequences for the batch.\n            method: ODE solver method passed to `odeint`.\n            alpha_dict: Scaling factors per monitored column.\n\n        Returns:\n            Tuple of (true trajectories, predicted trajectories, monitored columns).\n        \"\"\"\n        seq_len = x_seq.shape[1]\n\n        assert not torch.isnan(x_seq).any(), \"NaN in x_seq\"\n        assert not torch.isnan(u_seq).any(), \"NaN in u_seq\"\n        assert not torch.isnan(e_seq).any(), \"NaN in e_seq\"\n\n        x0 = x_seq[:, 0, :]\n\n        t_grid = torch.arange(\n            0, seq_len * self.step, self.step, dtype=torch.float32, device=self.device\n        )\n\n        func = BatchNeuralODE(self.model, u_seq, e_seq, t_grid)\n\n        odeint(func, x0, t_grid, method=method)\n\n        vects = torch.cat([x_seq, u_seq, e_seq], dim=2)\n        vect_dict = {\n            col: vects[..., i].unsqueeze(-1)\n            for i, col in enumerate(\n                self.x_cols + self.u_cols + self.e0_cols + self.e_cols\n            )\n        }\n\n        vects_dict = dict()\n\n        monitor_cols = self.x_cols + self.e_cols\n\n        for case in [\"true\", \"pred\"]:\n            if case == \"pred\":\n                vect_list = [\n                    self.model.history[col].unsqueeze(-1) for col in monitor_cols\n                ]\n            else:\n                vect_list = [vect_dict[col][:, 1:] for col in monitor_cols]\n\n            vects_dict[case] = self.cat_to_dict_vects(\n                vect_list,\n                monitor_cols,\n                alpha_dict=alpha_dict,\n            )\n        true_vect = torch.cat([vects_dict[\"true\"][col] for col in monitor_cols], dim=2)\n        pred_vect = torch.cat([vects_dict[\"pred\"][col] for col in monitor_cols], dim=2)\n        return true_vect, pred_vect, monitor_cols\n\n    def compute_loss_ode_step(\n        self,\n        batch: Sequence[torch.Tensor],\n        alpha_dict: Dict[Any, float],\n        method: str = \"rk4\",\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute loss for a single ODE rollout batch.\n\n        Args:\n            batch: Tuple of tensors `(x_seq, u_seq, e_seq, dx_seq)` from the DataLoader.\n            alpha_dict: Scaling factors per monitored column.\n            method: ODE solver method.\n\n        Returns:\n            Scalar loss tensor for the batch.\n        \"\"\"\n        x_seq, u_seq, e_seq, _ = [b.to(self.device) for b in batch]\n        true_vect, pred_vect, monitor_cols = self.ode_step(\n            x_seq,\n            u_seq,\n            e_seq,\n            method,\n            alpha_dict,\n        )\n\n        loss = 0.0\n        for i, col in enumerate(monitor_cols):\n            if col in alpha_dict.keys():\n                loss_fn = get_loss(col.loss_name)\n                assert not torch.isnan(pred_vect[..., i]).any(), \"NaN in pred_vect\"\n                assert not torch.isnan(true_vect[..., i]).any(), \"NaN in true_vect\"\n                res = loss_fn(pred_vect[..., i], true_vect[..., i])\n                loss += res\n\n        if torch.isnan(loss) or torch.isinf(loss):\n            print(\"NaN or Inf in loss!\")\n\n        return loss\n\n    def train(\n        self,\n        epochs: int = 800,\n        batch_size: int = 512,\n        val_batch_size: int = 10000,\n        scheduler: Optional[Any] = None,\n        method: str = \"rk4\",\n        alpha_dict: Optional[Dict[Any, float]] = None,\n    ) -&gt; None:\n        \"\"\"Train the ODE model and persist checkpoints/metrics.\n\n        Args:\n            epochs: Number of training epochs.\n            batch_size: Training batch size.\n            val_batch_size: Validation batch size.\n            scheduler: Optional learning-rate scheduler.\n            method: ODE solver method.\n            alpha_dict: Optional scaling factors per monitored column.\n        \"\"\"\n        self.train_loader = DataLoader(\n            self.train_dataset,\n            batch_size=batch_size,\n            shuffle=True,\n            num_workers=self.num_workers,\n        )\n        self.val_loader = DataLoader(\n            self.val_dataset,\n            batch_size=val_batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n        )\n\n        if alpha_dict is None:\n            alpha_dict = {col: 1.0 for col in self.x_cols}\n\n        self.stats_dict = self.train_dataset.stats_dict\n\n        losses = []\n        loss_csv_path = os.path.join(self.model_dir, \"training_losses.csv\")\n        fig_path = os.path.join(self.model_dir, \"training_curve.png\")\n\n        for epoch in range(epochs):\n            # --- TRAIN LOOP ---\n            self.model.train()\n            total_loss, total_batches = 0, 0\n            for batch in self.train_loader:\n                loss = self.compute_loss_ode_step(\n                    batch, alpha_dict=alpha_dict, method=method\n                )\n                self.optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                self.optimizer.step()\n                total_loss += loss.item()\n                total_batches += 1\n            avg_train_loss = total_loss / total_batches\n\n            # --- VALIDATION LOOP ---\n            self.model.eval()\n            val_loss, val_batches = 0, 0\n            with torch.no_grad():\n                for batch in self.val_loader:\n                    loss = self.compute_loss_ode_step(\n                        batch, alpha_dict=alpha_dict, method=method\n                    )\n                    val_loss += loss.item()\n                    val_batches += 1\n            avg_val_loss = val_loss / val_batches\n\n            if scheduler is not None:\n                scheduler.step(avg_val_loss)\n\n            losses.append(\n                {\n                    \"epoch\": epoch + 1,\n                    \"train_loss\": avg_train_loss,\n                    \"val_loss\": avg_val_loss,\n                }\n            )\n\n            print(\n                f\"Epoch {epoch+1}/{epochs} | train loss: {avg_train_loss:.5f} | val loss: {avg_val_loss:.5f}\"\n            )\n\n            # --- SAVE BEST MODEL ---\n            if avg_val_loss &lt; self.best_val_loss:\n                print(f\"  New best validation loss: {avg_val_loss:.5f}. Saving model.\")\n                self.best_val_loss = avg_val_loss\n                self.save_model(epoch)\n\n        df_losses = pd.DataFrame(losses)\n        df_losses.to_csv(loss_csv_path, index=False)\n        print(f\"\u2705 Saved training log to {loss_csv_path}\")\n\n        plt.figure(figsize=(7, 4))\n        plt.semilogy(\n            df_losses[\"epoch\"],\n            df_losses[\"train_loss\"],\n            label=\"Training loss\",\n            color=\"#1f77b4\",\n            linewidth=2,\n        )\n        plt.semilogy(\n            df_losses[\"epoch\"],\n            df_losses[\"val_loss\"],\n            label=\"Validation loss\",\n            color=\"#ff7f0e\",\n            linewidth=2,\n            linestyle=\"--\",\n        )\n\n        plt.title(\"Training and validation losses\", fontsize=13)\n        plt.xlabel(\"Epoch\", fontsize=11)\n        plt.ylabel(\"Loss (log scale)\", fontsize=11)\n        plt.grid(True, which=\"both\", linestyle=\":\", linewidth=0.8, alpha=0.7)\n        plt.legend(frameon=False, fontsize=10)\n        plt.tight_layout()\n        plt.savefig(fig_path, dpi=200)\n        plt.close()\n\n        print(f\"\u2705 Saved training curve to {fig_path}\")\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.__init__","title":"<code>__init__(data_df, model_config, model_dir, num_workers=4, load_parallel=True, train_val_num=(5000, 500))</code>","text":"<p>Initialize trainer with data, model configuration, and I/O paths.</p> <p>Parameters:</p> Name Type Description Default <code>data_df</code> <code>DataFrame</code> <p>DataFrame containing file paths and split labels.</p> required <code>model_config</code> <code>Dict[str, Any]</code> <p>Dictionary describing architecture, hyperparameters, and loader settings.</p> required <code>model_dir</code> <code>Any</code> <p>Base directory to store checkpoints and metadata.</p> required <code>num_workers</code> <code>int</code> <p>Number of workers for DataLoaders.</p> <code>4</code> <code>load_parallel</code> <code>bool</code> <p>Whether to load flights in parallel.</p> <code>True</code> <code>train_val_num</code> <code>Tuple[int, int]</code> <p>Tuple specifying how many train/val files to load.</p> <code>(5000, 500)</code> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def __init__(\n    self,\n    data_df: pd.DataFrame,\n    model_config: Dict[str, Any],\n    model_dir: Any,\n    num_workers: int = 4,\n    load_parallel: bool = True,\n    train_val_num: Tuple[int, int] = (5000, 500),\n) -&gt; None:\n    \"\"\"Initialize trainer with data, model configuration, and I/O paths.\n\n    Args:\n        data_df: DataFrame containing file paths and split labels.\n        model_config: Dictionary describing architecture, hyperparameters, and loader settings.\n        model_dir: Base directory to store checkpoints and metadata.\n        num_workers: Number of workers for DataLoaders.\n        load_parallel: Whether to load flights in parallel.\n        train_val_num: Tuple specifying how many train/val files to load.\n    \"\"\"\n\n    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    architecture, self.model_cols, custom_fn = get_architecture_from_name(\n        model_config[\"architecture_name\"]\n    )\n    self.x_cols, self.u_cols, self.e0_cols, self.e_cols, self.dx_cols = (\n        self.model_cols\n    )\n    self.model_dir = model_dir / model_config[\"model_name\"]\n    os.makedirs(self.model_dir, exist_ok=True)\n    self.architecture = architecture\n    self.model_config = model_config\n    self.architecture_name = model_config[\"architecture_name\"]\n    self.model_params = model_config[\"model_params\"]\n\n    self.train_dataset, self.val_dataset = get_train_val_data(\n        data_df,\n        self.model_cols,\n        shift=model_config[\"shift\"],\n        seq_len=model_config[\"seq_len\"],\n        custom_fn=custom_fn,\n        load_parallel=load_parallel,\n        train_val_num=train_val_num,\n    )\n    self.step = model_config[\"step\"]\n    self.num_workers = num_workers\n\n    self.stats_dict = self.train_dataset.stats_dict\n\n    self.model = self.get_or_create_model(*model_config[\"loading_args\"])\n\n    self.optimizer = torch.optim.AdamW(\n        self.model.parameters(),\n        lr=model_config[\"lr\"],\n        weight_decay=model_config[\"weight_decay\"],\n    )\n    self.epoch = 1\n    self.save_meta()\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.cat_to_dict_vects","title":"<code>cat_to_dict_vects(vect_list, col_list, alpha_dict, normalize=True)</code>","text":"<p>Concatenate vectors and build a dict keyed by column definitions.</p> <p>Parameters:</p> Name Type Description Default <code>vect_list</code> <code>Sequence[Tensor]</code> <p>Sequence of tensors to concatenate along the feature axis.</p> required <code>col_list</code> <code>Sequence[Any]</code> <p>Column identifiers matching the concatenated tensors.</p> required <code>alpha_dict</code> <code>Dict[Any, float]</code> <p>Optional scaling factors applied per column.</p> required <code>normalize</code> <code>bool</code> <p>Whether to normalize columns that request it.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[Any, Tensor]</code> <p>Dictionary mapping columns to (optionally) scaled and normalized tensors.</p> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def cat_to_dict_vects(\n    self,\n    vect_list: Sequence[torch.Tensor],\n    col_list: Sequence[Any],\n    alpha_dict: Dict[Any, float],\n    normalize: bool = True,\n) -&gt; Dict[Any, torch.Tensor]:\n    \"\"\"Concatenate vectors and build a dict keyed by column definitions.\n\n    Args:\n        vect_list: Sequence of tensors to concatenate along the feature axis.\n        col_list: Column identifiers matching the concatenated tensors.\n        alpha_dict: Optional scaling factors applied per column.\n        normalize: Whether to normalize columns that request it.\n\n    Returns:\n        Dictionary mapping columns to (optionally) scaled and normalized tensors.\n    \"\"\"\n\n    def modifier(el: torch.Tensor, col: Any) -&gt; torch.Tensor:\n        if (col.normalize_mode == \"normal\") &amp; (normalize):\n            return self.norm_vect(el, col)\n        return el\n\n    coeff_list = [\n        alpha_dict[col] if col in alpha_dict.keys() else 0.0 for col in col_list\n    ]\n\n    vects = torch.cat(vect_list, dim=2)\n    vects_dict = {\n        col: coeff * modifier(vects[..., i], col).unsqueeze(-1)\n        for i, (col, coeff) in enumerate(zip(col_list, coeff_list))\n    }\n    return vects_dict\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.compute_loss_ode_step","title":"<code>compute_loss_ode_step(batch, alpha_dict, method='rk4')</code>","text":"<p>Compute loss for a single ODE rollout batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Sequence[Tensor]</code> <p>Tuple of tensors <code>(x_seq, u_seq, e_seq, dx_seq)</code> from the DataLoader.</p> required <code>alpha_dict</code> <code>Dict[Any, float]</code> <p>Scaling factors per monitored column.</p> required <code>method</code> <code>str</code> <p>ODE solver method.</p> <code>'rk4'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Scalar loss tensor for the batch.</p> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def compute_loss_ode_step(\n    self,\n    batch: Sequence[torch.Tensor],\n    alpha_dict: Dict[Any, float],\n    method: str = \"rk4\",\n) -&gt; torch.Tensor:\n    \"\"\"Compute loss for a single ODE rollout batch.\n\n    Args:\n        batch: Tuple of tensors `(x_seq, u_seq, e_seq, dx_seq)` from the DataLoader.\n        alpha_dict: Scaling factors per monitored column.\n        method: ODE solver method.\n\n    Returns:\n        Scalar loss tensor for the batch.\n    \"\"\"\n    x_seq, u_seq, e_seq, _ = [b.to(self.device) for b in batch]\n    true_vect, pred_vect, monitor_cols = self.ode_step(\n        x_seq,\n        u_seq,\n        e_seq,\n        method,\n        alpha_dict,\n    )\n\n    loss = 0.0\n    for i, col in enumerate(monitor_cols):\n        if col in alpha_dict.keys():\n            loss_fn = get_loss(col.loss_name)\n            assert not torch.isnan(pred_vect[..., i]).any(), \"NaN in pred_vect\"\n            assert not torch.isnan(true_vect[..., i]).any(), \"NaN in true_vect\"\n            res = loss_fn(pred_vect[..., i], true_vect[..., i])\n            loss += res\n\n    if torch.isnan(loss) or torch.isinf(loss):\n        print(\"NaN or Inf in loss!\")\n\n    return loss\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.get_or_create_model","title":"<code>get_or_create_model(load=False, load_loss=False)</code>","text":"<p>Instantiate a new model or load existing checkpoints.</p> <p>Parameters:</p> Name Type Description Default <code>load</code> <code>bool</code> <p>Whether to attempt loading existing checkpoints.</p> <code>False</code> <code>load_loss</code> <code>bool</code> <p>Whether to restore tracked best validation loss when loading.</p> <code>False</code> <p>Returns:</p> Type Description <code>FlightDynamicsModel</code> <p>Initialized or restored <code>FlightDynamicsModel</code> instance.</p> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def get_or_create_model(\n    self, load: bool = False, load_loss: bool = False\n) -&gt; FlightDynamicsModel:\n    \"\"\"Instantiate a new model or load existing checkpoints.\n\n    Args:\n        load: Whether to attempt loading existing checkpoints.\n        load_loss: Whether to restore tracked best validation loss when loading.\n\n    Returns:\n        Initialized or restored `FlightDynamicsModel` instance.\n    \"\"\"\n    self.best_val_loss = float(\"inf\")\n    if load and os.path.exists(self.model_dir / \"meta.json\"):\n        model = self.load_best_checkpoint(load_loss=load_loss)\n    else:\n        print(\"Creating new model.\")\n        model = FlightDynamicsModel(\n            self.architecture,\n            self.stats_dict,\n            self.model_cols,\n            model_params=self.model_params,\n        ).to(self.device)\n    return model\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.load_best_checkpoint","title":"<code>load_best_checkpoint(load_loss=False)</code>","text":"<p>Create and populate a model from saved checkpoints.</p> <p>Parameters:</p> Name Type Description Default <code>load_loss</code> <code>bool</code> <p>Whether to restore tracked best validation loss.</p> <code>False</code> <p>Returns:</p> Type Description <code>FlightDynamicsModel</code> <p>Model with layer weights loaded when available.</p> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def load_best_checkpoint(self, load_loss: bool = False) -&gt; FlightDynamicsModel:\n    \"\"\"Create and populate a model from saved checkpoints.\n\n    Args:\n        load_loss: Whether to restore tracked best validation loss.\n\n    Returns:\n        Model with layer weights loaded when available.\n    \"\"\"\n    model = FlightDynamicsModel(\n        self.architecture,\n        self.stats_dict,\n        self.model_cols,\n        model_params=self.model_params,\n    ).to(self.device)\n\n    for name in model.layers_name:\n        checkpoint = self.load_layer_checkpoint(name)\n        if checkpoint is not None:\n            model.layers_dict[name].load_state_dict(\n                checkpoint[\"layer_state\"], strict=False\n            )\n            best_val_loss = checkpoint.get(\"best_val_loss\", float(\"inf\"))\n            self.epoch = checkpoint.get(\"epoch\", 0)\n        else:\n            best_val_loss = float(\"inf\")\n            self.epoch = 0\n\n    if load_loss:\n        self.best_val_loss = best_val_loss\n\n    print(\"Best val loss per layer:\", self.best_val_loss)\n    print(f\"Loaded modular model from {self.model_dir}\")\n\n    return model\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.load_layer_checkpoint","title":"<code>load_layer_checkpoint(layer_name)</code>","text":"<p>Load checkpoint dictionary for a specific layer if available.</p> <p>Parameters:</p> Name Type Description Default <code>layer_name</code> <code>str</code> <p>Name of the layer to load.</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Checkpoint dictionary if found, otherwise None.</p> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def load_layer_checkpoint(self, layer_name: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Load checkpoint dictionary for a specific layer if available.\n\n    Args:\n        layer_name: Name of the layer to load.\n\n    Returns:\n        Checkpoint dictionary if found, otherwise None.\n    \"\"\"\n    path = os.path.join(self.model_dir, f\"{layer_name}.pt\")\n    if not os.path.exists(path):\n        print(f\"No checkpoint found for layer {layer_name}, skipping load.\")\n        return None\n    else:\n        print(f\"checkpoint found for layer {layer_name}\")\n    checkpoint = torch.load(path, map_location=self.device)\n    return checkpoint\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.norm_vect","title":"<code>norm_vect(vect, col)</code>","text":"<p>Normalize tensor using stored statistics for a column.</p> <p>Parameters:</p> Name Type Description Default <code>vect</code> <code>Tensor</code> <p>Tensor to normalize.</p> required <code>col</code> <code>Any</code> <p>Column identifier used to fetch statistics.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor.</p> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def norm_vect(self, vect: torch.Tensor, col: Any) -&gt; torch.Tensor:\n    \"\"\"Normalize tensor using stored statistics for a column.\n\n    Args:\n        vect: Tensor to normalize.\n        col: Column identifier used to fetch statistics.\n\n    Returns:\n        Normalized tensor.\n    \"\"\"\n    return (vect - self.stats_dict[col][\"mean\"]) / (\n        self.stats_dict[col][\"std\"] + 1e-3\n    )\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.ode_step","title":"<code>ode_step(x_seq, u_seq, e_seq, method, alpha_dict)</code>","text":"<p>Integrate one ODE step and return true/predicted trajectories.</p> <p>Parameters:</p> Name Type Description Default <code>x_seq</code> <code>Tensor</code> <p>State sequences for the batch.</p> required <code>u_seq</code> <code>Tensor</code> <p>Control sequences for the batch.</p> required <code>e_seq</code> <code>Tensor</code> <p>Environment sequences for the batch.</p> required <code>method</code> <code>str</code> <p>ODE solver method passed to <code>odeint</code>.</p> required <code>alpha_dict</code> <code>Dict[Any, float]</code> <p>Scaling factors per monitored column.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Sequence[Any]]</code> <p>Tuple of (true trajectories, predicted trajectories, monitored columns).</p> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def ode_step(\n    self,\n    x_seq: torch.Tensor,\n    u_seq: torch.Tensor,\n    e_seq: torch.Tensor,\n    method: str,\n    alpha_dict: Dict[Any, float],\n) -&gt; Tuple[torch.Tensor, torch.Tensor, Sequence[Any]]:\n    \"\"\"Integrate one ODE step and return true/predicted trajectories.\n\n    Args:\n        x_seq: State sequences for the batch.\n        u_seq: Control sequences for the batch.\n        e_seq: Environment sequences for the batch.\n        method: ODE solver method passed to `odeint`.\n        alpha_dict: Scaling factors per monitored column.\n\n    Returns:\n        Tuple of (true trajectories, predicted trajectories, monitored columns).\n    \"\"\"\n    seq_len = x_seq.shape[1]\n\n    assert not torch.isnan(x_seq).any(), \"NaN in x_seq\"\n    assert not torch.isnan(u_seq).any(), \"NaN in u_seq\"\n    assert not torch.isnan(e_seq).any(), \"NaN in e_seq\"\n\n    x0 = x_seq[:, 0, :]\n\n    t_grid = torch.arange(\n        0, seq_len * self.step, self.step, dtype=torch.float32, device=self.device\n    )\n\n    func = BatchNeuralODE(self.model, u_seq, e_seq, t_grid)\n\n    odeint(func, x0, t_grid, method=method)\n\n    vects = torch.cat([x_seq, u_seq, e_seq], dim=2)\n    vect_dict = {\n        col: vects[..., i].unsqueeze(-1)\n        for i, col in enumerate(\n            self.x_cols + self.u_cols + self.e0_cols + self.e_cols\n        )\n    }\n\n    vects_dict = dict()\n\n    monitor_cols = self.x_cols + self.e_cols\n\n    for case in [\"true\", \"pred\"]:\n        if case == \"pred\":\n            vect_list = [\n                self.model.history[col].unsqueeze(-1) for col in monitor_cols\n            ]\n        else:\n            vect_list = [vect_dict[col][:, 1:] for col in monitor_cols]\n\n        vects_dict[case] = self.cat_to_dict_vects(\n            vect_list,\n            monitor_cols,\n            alpha_dict=alpha_dict,\n        )\n    true_vect = torch.cat([vects_dict[\"true\"][col] for col in monitor_cols], dim=2)\n    pred_vect = torch.cat([vects_dict[\"pred\"][col] for col in monitor_cols], dim=2)\n    return true_vect, pred_vect, monitor_cols\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.save_layer_checkpoint","title":"<code>save_layer_checkpoint(layer_name, epoch)</code>","text":"<p>Save checkpoint for an individual layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer_name</code> <code>str</code> <p>Name of the layer to checkpoint.</p> required <code>epoch</code> <code>int</code> <p>Current epoch offset for tracking.</p> required Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def save_layer_checkpoint(self, layer_name: str, epoch: int) -&gt; None:\n    \"\"\"Save checkpoint for an individual layer.\n\n    Args:\n        layer_name: Name of the layer to checkpoint.\n        epoch: Current epoch offset for tracking.\n    \"\"\"\n    layer = self.model.layers_dict[layer_name]\n    save_dict = {\n        \"layer_state\": layer.state_dict(),\n        \"optimizer_state\": self.optimizer.state_dict(),\n        \"best_val_loss\": self.best_val_loss,\n        \"epoch\": self.epoch + epoch,\n    }\n    torch.save(save_dict, self.model_dir / f\"{layer_name}.pt\")\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.save_meta","title":"<code>save_meta()</code>","text":"<p>Persist training metadata and statistics to disk.</p> <p>Creates or updates <code>meta.json</code> within the model directory.</p> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def save_meta(self) -&gt; None:\n    \"\"\"Persist training metadata and statistics to disk.\n\n    Creates or updates `meta.json` within the model directory.\n    \"\"\"\n    saved_stats_dict = {str(col): value for col, value in self.stats_dict.items()}\n\n    meta_dict = {\n        \"architecture_name\": self.architecture_name,\n        \"model_params\": self.model_config[\"model_params\"],\n        \"step\": self.model_config[\"step\"],\n        \"shift\": self.model_config[\"shift\"],\n        \"lr\": self.model_config[\"lr\"],\n        \"seq_len\": self.model_config[\"seq_len\"],\n        \"batch_size\": self.model_config[\"batch_size\"],\n        \"stats_dict\": saved_stats_dict,\n    }\n    print(self.model_dir / \"meta.json\")\n    with open(self.model_dir / \"meta.json\", \"w\") as f:\n        json.dump(meta_dict, f, indent=4)\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.save_model","title":"<code>save_model(epoch)</code>","text":"<p>Save checkpoints for all layers.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch index used when saving checkpoints.</p> required Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def save_model(self, epoch: int) -&gt; None:\n    \"\"\"Save checkpoints for all layers.\n\n    Args:\n        epoch: Epoch index used when saving checkpoints.\n    \"\"\"\n    for name in self.model.layers_name:\n        self.save_layer_checkpoint(name, epoch)\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.train","title":"<code>train(epochs=800, batch_size=512, val_batch_size=10000, scheduler=None, method='rk4', alpha_dict=None)</code>","text":"<p>Train the ODE model and persist checkpoints/metrics.</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>800</code> <code>batch_size</code> <code>int</code> <p>Training batch size.</p> <code>512</code> <code>val_batch_size</code> <code>int</code> <p>Validation batch size.</p> <code>10000</code> <code>scheduler</code> <code>Optional[Any]</code> <p>Optional learning-rate scheduler.</p> <code>None</code> <code>method</code> <code>str</code> <p>ODE solver method.</p> <code>'rk4'</code> <code>alpha_dict</code> <code>Optional[Dict[Any, float]]</code> <p>Optional scaling factors per monitored column.</p> <code>None</code> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def train(\n    self,\n    epochs: int = 800,\n    batch_size: int = 512,\n    val_batch_size: int = 10000,\n    scheduler: Optional[Any] = None,\n    method: str = \"rk4\",\n    alpha_dict: Optional[Dict[Any, float]] = None,\n) -&gt; None:\n    \"\"\"Train the ODE model and persist checkpoints/metrics.\n\n    Args:\n        epochs: Number of training epochs.\n        batch_size: Training batch size.\n        val_batch_size: Validation batch size.\n        scheduler: Optional learning-rate scheduler.\n        method: ODE solver method.\n        alpha_dict: Optional scaling factors per monitored column.\n    \"\"\"\n    self.train_loader = DataLoader(\n        self.train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n    )\n    self.val_loader = DataLoader(\n        self.val_dataset,\n        batch_size=val_batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n    )\n\n    if alpha_dict is None:\n        alpha_dict = {col: 1.0 for col in self.x_cols}\n\n    self.stats_dict = self.train_dataset.stats_dict\n\n    losses = []\n    loss_csv_path = os.path.join(self.model_dir, \"training_losses.csv\")\n    fig_path = os.path.join(self.model_dir, \"training_curve.png\")\n\n    for epoch in range(epochs):\n        # --- TRAIN LOOP ---\n        self.model.train()\n        total_loss, total_batches = 0, 0\n        for batch in self.train_loader:\n            loss = self.compute_loss_ode_step(\n                batch, alpha_dict=alpha_dict, method=method\n            )\n            self.optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n            self.optimizer.step()\n            total_loss += loss.item()\n            total_batches += 1\n        avg_train_loss = total_loss / total_batches\n\n        # --- VALIDATION LOOP ---\n        self.model.eval()\n        val_loss, val_batches = 0, 0\n        with torch.no_grad():\n            for batch in self.val_loader:\n                loss = self.compute_loss_ode_step(\n                    batch, alpha_dict=alpha_dict, method=method\n                )\n                val_loss += loss.item()\n                val_batches += 1\n        avg_val_loss = val_loss / val_batches\n\n        if scheduler is not None:\n            scheduler.step(avg_val_loss)\n\n        losses.append(\n            {\n                \"epoch\": epoch + 1,\n                \"train_loss\": avg_train_loss,\n                \"val_loss\": avg_val_loss,\n            }\n        )\n\n        print(\n            f\"Epoch {epoch+1}/{epochs} | train loss: {avg_train_loss:.5f} | val loss: {avg_val_loss:.5f}\"\n        )\n\n        # --- SAVE BEST MODEL ---\n        if avg_val_loss &lt; self.best_val_loss:\n            print(f\"  New best validation loss: {avg_val_loss:.5f}. Saving model.\")\n            self.best_val_loss = avg_val_loss\n            self.save_model(epoch)\n\n    df_losses = pd.DataFrame(losses)\n    df_losses.to_csv(loss_csv_path, index=False)\n    print(f\"\u2705 Saved training log to {loss_csv_path}\")\n\n    plt.figure(figsize=(7, 4))\n    plt.semilogy(\n        df_losses[\"epoch\"],\n        df_losses[\"train_loss\"],\n        label=\"Training loss\",\n        color=\"#1f77b4\",\n        linewidth=2,\n    )\n    plt.semilogy(\n        df_losses[\"epoch\"],\n        df_losses[\"val_loss\"],\n        label=\"Validation loss\",\n        color=\"#ff7f0e\",\n        linewidth=2,\n        linestyle=\"--\",\n    )\n\n    plt.title(\"Training and validation losses\", fontsize=13)\n    plt.xlabel(\"Epoch\", fontsize=11)\n    plt.ylabel(\"Loss (log scale)\", fontsize=11)\n    plt.grid(True, which=\"both\", linestyle=\":\", linewidth=0.8, alpha=0.7)\n    plt.legend(frameon=False, fontsize=10)\n    plt.tight_layout()\n    plt.savefig(fig_path, dpi=200)\n    plt.close()\n\n    print(f\"\u2705 Saved training curve to {fig_path}\")\n</code></pre>"},{"location":"reference/predictor/","title":"\ud83d\udd2e Predictor API","text":"<p>The <code>NodeFDMPredictor</code> serves as the inference engine for the framework.</p> <p>It is responsible for loading trained model artifacts (checkpoints and metadata), reconstructing the specific neural architecture, and performing full trajectory rollouts (simulations) by integrating the learned dynamics over time.</p>"},{"location":"reference/predictor/#class-reference","title":"\ud83d\udcd8 Class Reference","text":""},{"location":"reference/predictor/#node_fdm.predictor","title":"<code>predictor</code>","text":"<p>Prediction helper to roll out flight trajectories with trained models.</p>"},{"location":"reference/predictor/#node_fdm.predictor.NodeFDMPredictor","title":"<code>NodeFDMPredictor</code>","text":"<p>Predict flight trajectories using a pretrained FlightDynamicsModelProd.</p> Source code in <code>src/node_fdm/predictor.py</code> <pre><code>class NodeFDMPredictor:\n    \"\"\"Predict flight trajectories using a pretrained FlightDynamicsModelProd.\"\"\"\n\n    def __init__(\n        self,\n        model_cols: list,\n        model_path: Path,\n        dt: float = 4.0,\n        device: str = \"cuda:0\",\n    ):\n        \"\"\"Initialize predictor with model path and column definitions.\n\n        Args:\n            model_cols: Sequence of model column groups (state, control, env, env_extra, derivatives).\n            model_path: Directory containing pretrained model artifacts.\n            dt: Integration timestep used for state propagation.\n            device: Torch device string to run predictions on.\n        \"\"\"\n        self.model_path = Path(model_path)\n        self.x_cols, self.u_cols, self.e0_cols, self.e_cols, self.dx_cols = model_cols\n        self.dt = dt\n        self.device = torch.device(device)\n        self.model = FlightDynamicsModelProd(model_path).to(self.device)\n        self.model.eval()\n\n    @staticmethod\n    def _get_dict(f: pd.DataFrame, cols: List, i: int) -&gt; Dict:\n        \"\"\"Slice a DataFrame row into a dict of tensors keyed by column definitions.\n\n        Args:\n            f: Flight data DataFrame.\n            cols: Column identifiers to extract.\n            i: Row index to slice.\n\n        Returns:\n            Dictionary mapping column identifiers to 1-sample tensors.\n        \"\"\"\n        return {\n            col: torch.tensor(f[col].iloc[i : i + 1].values.astype(np.float32))\n            for col in cols\n        }\n\n    def _get_state(self, f: pd.DataFrame, i: int) -&gt; Dict:\n        \"\"\"Extract state columns at a specific timestep.\n\n        Returns:\n            Dictionary mapping state columns to tensors.\n        \"\"\"\n        return self._get_dict(f, self.x_cols, i)\n\n    def _get_ctrl(self, f: pd.DataFrame, i: int) -&gt; Dict:\n        \"\"\"Extract control columns at a specific timestep.\n\n        Returns:\n            Dictionary mapping control columns to tensors.\n        \"\"\"\n        return self._get_dict(f, self.u_cols, i)\n\n    def _get_env(self, f: pd.DataFrame, i: int) -&gt; Dict:\n        \"\"\"Extract environment columns at a specific timestep.\n\n        Returns:\n            Dictionary mapping environment columns to tensors.\n        \"\"\"\n        return self._get_dict(f, self.e0_cols, i)\n\n    def _next_state(self, current_state: Dict, res_dict: Dict) -&gt; Dict:\n        \"\"\"Advance state using predicted derivatives and configured timestep.\n\n        Args:\n            current_state: Mapping of current state tensors keyed by column.\n            res_dict: Model output containing derivative tensors.\n\n        Returns:\n            Updated state mapping after one integration step.\n        \"\"\"\n        new_state = dict()\n        for x_col, (coeff, dx_col) in zip(self.x_cols, self.dx_cols):\n            new_state[x_col] = current_state[x_col] + coeff * self.dt * res_dict[dx_col]\n        return new_state\n\n    def predict_flight(\n        self, flight_df: pd.DataFrame, add_cols: list = []\n    ) -&gt; pd.DataFrame:\n        \"\"\"Generate model predictions for an entire flight.\n\n        Args:\n            flight_df: Flight measurements DataFrame.\n            add_cols: Optional extra columns to return alongside state predictions.\n\n        Returns:\n            DataFrame containing predicted columns with `pred_` prefix.\n        \"\"\"\n        display_dict = {col: [] for col in self.x_cols + add_cols}\n\n        current_state = self._get_state(flight_df, 0)\n        current_state = {k: v.to(self.device) for k, v in current_state.items()}\n        for i in range(len(flight_df)):\n            input_dict = {\n                **current_state,\n                **self._get_ctrl(flight_df, i),\n                **self._get_env(flight_df, i),\n            }\n            input_dict = {k: v.to(self.device) for k, v in input_dict.items()}\n            res_dict = self.model.forward(input_dict)\n            for col in display_dict.keys():\n                display_dict[col].append(res_dict[col].cpu().detach().numpy())\n            current_state = self._next_state(current_state, res_dict)\n\n        pred_df = pd.DataFrame(\n            {f\"pred_{col}\": np.concatenate(display_dict[col]) for col in display_dict},\n            index=flight_df.index,\n        )\n        return pred_df\n</code></pre>"},{"location":"reference/predictor/#node_fdm.predictor.NodeFDMPredictor.__init__","title":"<code>__init__(model_cols, model_path, dt=4.0, device='cuda:0')</code>","text":"<p>Initialize predictor with model path and column definitions.</p> <p>Parameters:</p> Name Type Description Default <code>model_cols</code> <code>list</code> <p>Sequence of model column groups (state, control, env, env_extra, derivatives).</p> required <code>model_path</code> <code>Path</code> <p>Directory containing pretrained model artifacts.</p> required <code>dt</code> <code>float</code> <p>Integration timestep used for state propagation.</p> <code>4.0</code> <code>device</code> <code>str</code> <p>Torch device string to run predictions on.</p> <code>'cuda:0'</code> Source code in <code>src/node_fdm/predictor.py</code> <pre><code>def __init__(\n    self,\n    model_cols: list,\n    model_path: Path,\n    dt: float = 4.0,\n    device: str = \"cuda:0\",\n):\n    \"\"\"Initialize predictor with model path and column definitions.\n\n    Args:\n        model_cols: Sequence of model column groups (state, control, env, env_extra, derivatives).\n        model_path: Directory containing pretrained model artifacts.\n        dt: Integration timestep used for state propagation.\n        device: Torch device string to run predictions on.\n    \"\"\"\n    self.model_path = Path(model_path)\n    self.x_cols, self.u_cols, self.e0_cols, self.e_cols, self.dx_cols = model_cols\n    self.dt = dt\n    self.device = torch.device(device)\n    self.model = FlightDynamicsModelProd(model_path).to(self.device)\n    self.model.eval()\n</code></pre>"},{"location":"reference/predictor/#node_fdm.predictor.NodeFDMPredictor.predict_flight","title":"<code>predict_flight(flight_df, add_cols=[])</code>","text":"<p>Generate model predictions for an entire flight.</p> <p>Parameters:</p> Name Type Description Default <code>flight_df</code> <code>DataFrame</code> <p>Flight measurements DataFrame.</p> required <code>add_cols</code> <code>list</code> <p>Optional extra columns to return alongside state predictions.</p> <code>[]</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing predicted columns with <code>pred_</code> prefix.</p> Source code in <code>src/node_fdm/predictor.py</code> <pre><code>def predict_flight(\n    self, flight_df: pd.DataFrame, add_cols: list = []\n) -&gt; pd.DataFrame:\n    \"\"\"Generate model predictions for an entire flight.\n\n    Args:\n        flight_df: Flight measurements DataFrame.\n        add_cols: Optional extra columns to return alongside state predictions.\n\n    Returns:\n        DataFrame containing predicted columns with `pred_` prefix.\n    \"\"\"\n    display_dict = {col: [] for col in self.x_cols + add_cols}\n\n    current_state = self._get_state(flight_df, 0)\n    current_state = {k: v.to(self.device) for k, v in current_state.items()}\n    for i in range(len(flight_df)):\n        input_dict = {\n            **current_state,\n            **self._get_ctrl(flight_df, i),\n            **self._get_env(flight_df, i),\n        }\n        input_dict = {k: v.to(self.device) for k, v in input_dict.items()}\n        res_dict = self.model.forward(input_dict)\n        for col in display_dict.keys():\n            display_dict[col].append(res_dict[col].cpu().detach().numpy())\n        current_state = self._next_state(current_state, res_dict)\n\n    pred_df = pd.DataFrame(\n        {f\"pred_{col}\": np.concatenate(display_dict[col]) for col in display_dict},\n        index=flight_df.index,\n    )\n    return pred_df\n</code></pre>"}]}