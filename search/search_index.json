{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p> A physics-guided Neural Ordinary Differential Equation (Neural ODE) framework for aircraft flight dynamics. </p>"},{"location":"#node-fdm-at-a-glance","title":"\ud83c\udfaf node-fdm: At a Glance","text":"<p>node-fdm is a Python library designed for learning and simulation of aircraft flight dynamics.</p> <p>It couples the efficiency of Neural Ordinary Differential Equations (Neural ODE) with physical laws from aeronautics to:</p> <ul> <li>Reconstruct coherent aircraft trajectories from data (ADS-B or QAR).</li> <li>Simulate aircraft behavior through physically aware latent dynamics.</li> <li>Offer ready-to-use (OpenSky 2025, QAR) and customizable architectures.</li> <li>Enable benchmarking against established physical models such as BADA.</li> </ul> <p>This documentation will guide you through installation, core concepts, running pipelines, and extending the framework.</p>"},{"location":"#quick-start-navigation","title":"\ud83d\ude80 Quick Start &amp; Navigation","text":"<p>Start from installation, then follow the end-to-end pipelines mirrored in the repository layout.</p>"},{"location":"#start-here","title":"\ud83c\udf1f Start here","text":"<ul> <li>Installation: set up Python, optional extras, and editable installs.</li> <li>Quickstart: full workflow overview for any architecture plus the OpenSky 2025 example.</li> </ul>"},{"location":"#run-the-pipelines","title":"\ud83e\uddea Run the pipelines","text":"<ul> <li>Configure parameters: edit <code>scripts/opensky/config.yaml</code> or <code>scripts/qar/config.yaml</code> to set paths, typecodes, and hyperparameters.</li> <li>Train a model: launch <code>opensky_2025</code> or <code>qar</code> training via <code>ODETrainer</code> and monitor checkpoints.</li> <li>Run inference: load saved models with <code>NodeFDMPredictor</code>, roll out trajectories, and export predictions.</li> </ul>"},{"location":"#extend-or-customise","title":"\ud83d\udee0\ufe0f Extend or customise","text":"<ul> <li>Create an architecture: clone the OpenSky/QAR templates, declare columns, hooks, and layer stacks.</li> <li>Core concepts: learn about column groups, processing hooks, and architecture registration.</li> </ul>"},{"location":"#api-reference","title":"\ud83d\udcda API reference","text":"<ul> <li>Overview, Architectures, Data, Models, Trainer, Predictor, Package index.</li> </ul>"},{"location":"#legal-notice","title":"\ud83d\udccc Legal Notice","text":"<p>This project is distributed under the EUPL-1.2 license with specific EUROCONTROL amendments (see <code>AMENDMENT_TO_EUPL_license.md</code>).</p> <p>It is intended for research purposes only and must not be used as a regulatory or operational tool under any circumstances.</p>"},{"location":"concepts/","title":"\ud83e\udde0 Core Concepts","text":"<p>This section introduces the fundamental building blocks of node-fdm.  Understanding these concepts will help you navigate architectures, preprocess data, and extend the framework with your own models.</p>"},{"location":"concepts/#column-groups","title":"\ud83d\udd22 Column Groups","text":"<p>Each architecture organises its input and output features into well-defined column groups:</p> <ul> <li> <p><code>X_COLS</code> \u2014 State variables   Flight states used as ODE inputs and outputs.  </p> </li> <li> <p><code>U_COLS</code> \u2014 Control variables   Pilot or FMS selections, control inputs, or actuations.</p> </li> <li> <p><code>E0_COLS</code> \u2014 Environmental inputs   Exogenous variables such as wind, distances, temperatures, or air density.</p> </li> <li> <p><code>E_COLS</code> \u2014 Derived/environmental features   Outputs from physics or feature-extraction layers.</p> </li> <li> <p><code>DX_COLS</code> \u2014 Derivatives predicted by the ODE layer   Target derivatives for the Neural ODE (e.g. <code>dalt</code>, <code>dvz</code>, <code>dmass</code>).</p> </li> </ul> <p>These groups define the information flow inside an architecture.</p>"},{"location":"concepts/#architectures","title":"\ud83c\udfd7\ufe0f Architectures","text":"<p>An architecture combines physics and data-driven components:</p> <ul> <li> <p>Physics/feature layers   e.g., <code>TrajectoryLayer</code>, <code>EngineLayer</code>, or custom layers computing derived quantities.</p> </li> <li> <p>Neural ODE / Structured layers   e.g., <code>StructuredLayer</code>, which predicts the derivatives (<code>DX_COLS</code>).</p> </li> </ul> <p>Together, these layers form a stack defined in each architecture\u2019s <code>model.py</code>, where the order of layers and the mapping between column groups are specified.</p>"},{"location":"concepts/#processing-hooks","title":"\ud83d\udd27 Processing Hooks","text":"<p>Each architecture can provide two optional hooks:</p> <ul> <li> <p><code>flight_processing</code>   Augments raw data before training (e.g., computing <code>alt_diff</code>, smoothing, adding derived physics).</p> </li> <li> <p><code>segment_filtering</code>   Removes poor-quality segments or invalid training examples.</p> </li> </ul> <p>These hooks allow architectures to remain self-contained, they define not only the model, but also how the data should be prepared for it.</p>"},{"location":"concepts/#normalization-statistics","title":"\ud83d\udcd0 Normalization &amp; Statistics","text":"<p><code>SeqDataset</code> automatically computes:</p> <ul> <li>mean and standard deviation for each column  </li> <li>outlier-robust scaling (clipped at 99.5%)  </li> <li>metadata stored in <code>meta.json</code> for inference</li> </ul> <p>This ensures consistent training and inference, even when switching architectures.</p>"},{"location":"concepts/#registration-mechanism","title":"\ud83e\udde9 Registration Mechanism","text":"<p>For an architecture to be discoverable, it must be added to:</p> <p>node_fdm/architectures/mapping.py</p> <p>This mapping resolves each <code>architecture_name</code> to:</p> <ul> <li>its column groups  </li> <li>its layers  </li> <li>its processing hooks</li> </ul> <p>Without registration, trainers and predictors cannot locate your architecture.</p>"},{"location":"concepts/#creating-your-own-architecture","title":"\ud83d\ude80 Creating Your Own Architecture","text":"<p>Use the existing <code>opensky_2025</code> and <code>qar</code> folders as templates. Mirror their structure when defining:</p> <ul> <li>column groups (<code>columns.py</code>)  </li> <li>processing hooks (<code>flight_process.py</code>)  </li> <li>layer stack (<code>model.py</code>)  </li> <li>any extra physics or feature layers  </li> </ul>"},{"location":"guide/installation/","title":"\u2699\ufe0f Installation","text":"<p>This page explains how to install node-fdm, configure optional dependencies, and set up the directories used by the data pipelines.</p>"},{"location":"guide/installation/#prerequisites","title":"\ud83e\udde9 Prerequisites","text":"<p>Before installing, ensure you have:</p> <ul> <li>Python 3.11+</li> <li>OpenSky Trino access (required only if you plan to run the full OpenSky 2025 pipeline)</li> <li>Optional: BADA 4.2 model files   Set their location in the relevant <code>config.yaml</code> (<code>scripts/opensky/config.yaml</code>).</li> </ul>"},{"location":"guide/installation/#install-the-package","title":"\ud83d\udce6 Install the Package","text":"<p>From PyPI (recommended for users)</p> <ul> <li>Core library: <code>pip install node-fdm</code></li> <li>With optional deps (traffic, fastmeteo, viz): <code>pip install 'node-fdm[all]'</code></li> <li>BADA baseline support (pyBADA requirements are restrictive): <code>pip install pybada --ignore-requires-python --no-deps</code> <code>pip install simplekml 'xlsxwriter&gt;=3.2.5'</code>  # pyBADA dependency</li> </ul> <p>From source (contributors) <pre><code>pip install -e .[all]      # core + traffic + fastmeteo + click + tqdm + matplotlib\n</code></pre></p>"},{"location":"guide/installation/#project-directories","title":"\ud83d\udcc1 Project Directories","text":"<p>Each pipeline has its own <code>config.yaml</code> (<code>scripts/opensky/config.yaml</code>, <code>scripts/qar/config.yaml</code>) where you define:</p> <ul> <li><code>paths.data_dir</code> \u2014 root for all data artifacts  </li> <li><code>paths.download_dir</code>, <code>preprocess_dir</code>, <code>process_dir</code>, <code>predicted_dir</code>, <code>bada_dir</code>, <code>models_dir</code>, <code>figure_dir</code> </li> <li><code>paths.era5_cache_dir</code> \u2014 local cache for meteorological fields  </li> <li><code>bada.bada_4_2_dir</code> \u2014 required if you run the BADA baseline (<code>07_bada_prediction.py</code>)</li> </ul>"},{"location":"guide/installation/#quick-check","title":"\u2714\ufe0f Quick Check","text":"<p>Verify that <code>node_fdm</code> imports correctly:</p> <pre><code>python - &lt;&lt;'PY'\nimport torch\nimport node_fdm\nprint(\"Torch:\", torch.__version__)\nprint(\"node_fdm import OK\")\n</code></pre> <p>If both lines print successfully, your installation is complete.</p>"},{"location":"guide/quickstart/","title":"Quickstart","text":"<p>This guide provides a complete overview of how to run node-fdm end-to-end. It provide the general workflow used by all architectures and the full OpenSky 2025 pipeline example. All paths assume you are at the repository root; each pipeline ships its own <code>config.yaml</code> under <code>scripts/opensky/</code> or <code>scripts/qar/</code>.</p>"},{"location":"guide/quickstart/#general-pattern-any-architecture","title":"General Pattern (Any Architecture)","text":"<p>1) Collect and prepare raw data    Ensure your raw inputs can be mapped to the architecture\u2019s <code>Column</code> definitions.</p> <p>2) Decode, resample, and clean    Build consistent time steps, decode dependent messages, and remove invalid or unusable segments.</p> <p>3) Feature enrichment    Add environmental inputs (e.g., ERA5), smoothing, and architecture-specific derived quantities.</p> <p>4) Dataset splitting    Create a file list with <code>train</code>, <code>val</code>, and <code>test</code> assignments pointing to processed parquet files.</p> <p>5) Training    Use <code>ODETrainer</code>, setting <code>architecture_name</code> to your target architecture and loading its <code>model_params</code> from <code>model.py</code>.</p> <p>6) Inference    Load checkpoints with <code>NodeFDMPredictor</code>, reuse the architecture\u2019s preprocessing hooks, and write prediction parquet files.</p> <p>7) Evaluation and visualization    Compute metrics and generate overlays for the output columns relevant to your architecture.</p>"},{"location":"guide/quickstart/#opensky-2025-ads-b-example-pipeline","title":"OpenSky 2025 (ADS-B) Example Pipeline","text":"<p>1) Aircraft sampling <code>scripts/opensky/01_aircraft_list.py</code> builds <code>data/aircraft_db.csv</code> using OpenSky Trino and the <code>typecodes</code> declared in <code>scripts/opensky/config.yaml</code>.</p> <p>2) Download raw data <code>scripts/opensky/02_download_data.py</code> fetches history, flightlist, and extended tables into <code>data/downloaded_parquet/</code>.</p> <p>3) Decode and resample <code>scripts/opensky/03_preprocess_data.py &lt;history.parquet&gt;</code> decodes BDS 4/5/6, filters short flights, computes ADEP/ADES distances, and resamples to 4 seconds.</p> <p>4) Weather enrichment and smoothing <code>scripts/opensky/04_weather_spd_process_data.py</code> adds ERA5 wind/temperature (<code>era5_features</code>), applies <code>selected_param_config</code>, and writes enriched segments to <code>data/processed_flights/&lt;TYPECODE&gt;/</code> plus <code>dataset_split.csv</code>.</p> <p>5) Train Neural ODEs <code>scripts/opensky/05_training.py</code> trains the <code>opensky_2025</code> architecture via <code>ODETrainer</code>, saving checkpoints to <code>models/opensky_&lt;TYPECODE&gt;/</code>.</p> <p>6) Inference <code>scripts/opensky/06_flight_prediction.py</code> rolls out trajectories and writes <code>pred_*</code> columns to <code>data/predicted_flights/&lt;TYPECODE&gt;/</code>.</p> <p>7) Baselines and evaluation    - <code>scripts/opensky/07_bada_prediction.py</code> (requires <code>BADA_4_2_DIR</code>)    - <code>scripts/opensky/08_visualize_predictions.py</code> (overlays for truth vs. model vs. selections vs. BADA)    - <code>scripts/opensky/09_performance_aggregation.py</code> (MAE/MAPE/ME per phase)    - <code>scripts/opensky/10_dataset_stats.py</code> (coverage statistics)</p>"},{"location":"guide/quickstart/#general-tips","title":"General Tips","text":"<ul> <li>Use the pipeline <code>config.yaml</code> (<code>scripts/opensky/config.yaml</code> or <code>scripts/qar/config.yaml</code>) as the single source of paths, typecodes, architecture names, and shared parameters.  </li> <li>Make sure caches (e.g., <code>data/era5_cache</code>) exist to avoid repeated downloads of ERA5 fields.  </li> <li>For hardware constraints, adjust <code>batch_size</code>, <code>num_workers</code>, and <code>seq_len</code> inside <code>model_config</code>.</li> </ul>"},{"location":"howto/configure_params/","title":"Configure project paths and options","text":"<p>All runtime settings are now defined per pipeline:</p> <ul> <li><code>scripts/opensky/config.yaml</code> \u2014 OpenSky 2025 pipeline</li> <li><code>scripts/qar/config.yaml</code> \u2014 QAR pipeline</li> </ul> <p>Key fields (OpenSky example): <pre><code>paths:\n  data_dir: \"/path/to/data\"\n  download_dir: \"downloaded_parquet\"\n  preprocess_dir: \"preprocessed_parquet\"\n  process_dir: \"processed_flights\"\n  predicted_dir: \"predicted_flights\"\n  bada_dir: \"bada_flights\"\n  models_dir: \"models\"\n  figure_dir: \"figures\"\n  era5_cache_dir: \"era5_cache\"\n\nera5_features:\n  - u_component_of_wind\n  - v_component_of_wind\n  - temperature\n\ntypecodes:\n  - A320\n  - A20N\n  # ...\n\nbada:\n  bada_4_2_dir: \"/path/to/BADA/4.2.1\"\n</code></pre></p> <p>Tips:</p> <ul> <li>Keep <code>data_dir</code> absolute; subfolders are resolved relative to it.</li> <li>Adjust <code>typecodes</code> once in the relevant <code>config.yaml</code> instead of editing scripts.</li> <li>Ensure directories exist before running downloads/preprocessing (<code>mkdir -p ...</code>).</li> <li>Set <code>bada.bada_4_2_dir</code> if you plan to run <code>07_bada_prediction.py</code>.</li> <li>For QAR, only the needed paths are kept (<code>data_dir</code>, <code>predicted_dir</code>, <code>bada_dir</code>, <code>models_dir</code>, <code>figure_dir</code>) plus <code>typecodes</code> and <code>computing.default_cpu_count</code>.</li> </ul>"},{"location":"howto/create_architecture/","title":"Create a new architecture","text":"<p>Follow these steps to add and register a new architecture (see the README guidance):</p> <p>1) Copy a skeleton</p> <ul> <li>Duplicate <code>node_fdm/architectures/opensky_2025</code> (minimal) or <code>node_fdm/architectures/qar</code> (stacked layers) into <code>node_fdm/architectures/&lt;your_arch&gt;</code>.</li> <li>Keep the same file names: <code>columns.py</code>, <code>flight_process.py</code>, <code>model.py</code> (plus extra layers as needed).</li> </ul> <p>2) Declare columns (<code>columns.py</code>)</p> <ul> <li>Define state (<code>X_COLS</code>), controls (<code>U_COLS</code>), environment (<code>E0_COLS</code>), derived outputs (<code>E1_COLS</code>), and derivatives (<code>DX_COLS</code>), using <code>utils.data.column.Column</code> and units.</li> <li>Make sure derivative columns match the ODE targets you want to learn.</li> </ul> <p>3) Custom preprocessing (<code>flight_process.py</code>)</p> <ul> <li>Implement <code>flight_processing(df)</code> to add derived columns or smoothing.</li> <li>Optional <code>segment_filtering(df, start_idx, seq_len)</code> can reject bad segments (e.g., distance jumps).</li> <li>Expose any config you need (e.g., <code>selected_param_config</code>).</li> </ul> <p>4) Wire the model (<code>model.py</code>)</p> <ul> <li>Build <code>X_COLS</code>, <code>U_COLS</code>, <code>E0_COLS</code>, <code>E1_COLS</code>, <code>DX_COLS</code>, and <code>MODEL_COLS</code>.</li> <li>Define layers (e.g., a physics/feature layer, then an ODE/data layer) and assemble <code>ARCHITECTURE</code> as a list of layer specs.</li> <li>Set <code>MODEL_COLS</code> to match the ordering expected by <code>FlightProcessor</code> and <code>SeqDataset</code>.</li> </ul> <p>5) Add any custom layers</p> <ul> <li>Put them in the same folder (e.g., <code>trajectory_layer.py</code>, <code>engine_layer.py</code>).</li> <li>Export them via <code>__init__.py</code> if you need external imports.</li> </ul> <p>6) Register the name (<code>architectures/mapping.py</code>)</p> <ul> <li>Add your key to <code>valid_names</code>.</li> <li>Ensure <code>get_architecture_module</code> imports your <code>columns</code>, <code>flight_process</code>, and <code>model</code>.</li> </ul> <p>7) Test a tiny run</p> <ul> <li>Prepare a minimal processed dataset conforming to your columns.</li> <li>Run a short training with <code>ODETrainer</code> (small <code>seq_len</code>, small <code>batch_size</code>) to check shapes and stats.</li> <li>Verify inference with <code>NodeFDMPredictor</code> using your <code>MODEL_COLS</code> and <code>flight_processing</code>.</li> </ul> <p>Tips:</p> <ul> <li>Keep column names consistent between preprocessing and model definitions.</li> <li>Use <code>Column</code> units to avoid silent scale bugs.</li> <li>Update the relevant pipeline <code>config.yaml</code> (paths, <code>typecodes</code>) if your dataset layout differs from the existing pipelines.</li> </ul>"},{"location":"howto/run_inference/","title":"Run inference on flights (generic)","text":"<p><code>NodeFDMPredictor</code> needs:</p> <ul> <li>The architecture\u2019s <code>MODEL_COLS</code> and (optionally) its <code>flight_processing</code> function.</li> <li>A trained checkpoint directory containing <code>meta.json</code> and layer weights.</li> <li>Processed flights that match the architecture\u2019s preprocessing.</li> </ul> <p>Example (adapt the architecture import and paths): <pre><code>import json\nfrom pathlib import Path\nimport pandas as pd\nimport yaml\n\nfrom node_fdm.predictor import NodeFDMPredictor\nfrom node_fdm.data.flight_processor import FlightProcessor\nfrom node_fdm.architectures import mapping\n\ncfg = yaml.safe_load(open(\"config.yaml\"))  # run inside scripts/opensky or scripts/qar\npaths = cfg[\"paths\"]\nprocess_dir = Path(paths[\"data_dir\"]) / paths[\"process_dir\"]\nmodels_dir = Path(paths[\"data_dir\"]) / paths[\"models_dir\"]\n\n# Load meta to recover the architecture name\nmodel_path = models_dir / \"opensky_A320\"   # replace with your model folder\nmeta = json.loads((model_path / \"meta.json\").read_text())\narch_name = meta[\"architecture_name\"]\n\n# Get architecture-specific modules\n_, model_cols, custom_fn = mapping.get_architecture_from_name(arch_name)\ncustom_processing_fn, _ = custom_fn\n\n# Prepare one processed flight\nflight_path = process_dir / \"A320\" / \"20241001_A320_00001.parquet\"\nprocessor = FlightProcessor(model_cols, custom_processing_fn=custom_processing_fn)\nf = processor.process_flight(pd.read_parquet(flight_path))\n\n# Load predictor\npredictor = NodeFDMPredictor(\n    model_cols=model_cols,\n    model_path=model_path,\n    dt=meta.get(\"step\", 4.0),\n    device=\"cuda:0\",  # set to \"cpu\" if no GPU\n)\n\n# Predict full trajectory\npred_df = predictor.predict_flight(f)\nprint(pred_df.head())\n</code></pre></p> <p><code>predict_flight</code> returns <code>pred_&lt;column&gt;</code> for each state/environment output defined by the architecture.</p> <p>Batch runs: adapt <code>scripts/opensky/06_flight_prediction.py</code> or build a small loop over your test split, reusing <code>FlightProcessor</code> + <code>NodeFDMPredictor</code>.</p>"},{"location":"howto/train_model/","title":"Train a Neural ODE model (generic)","text":"<p>Prerequisites:</p> <ul> <li>A processed dataset compatible with the target architecture (columns and preprocessing must match).</li> <li>A file list DataFrame with at least <code>filepath</code> and <code>split</code> (<code>train</code> / <code>val</code> / <code>test</code>).</li> </ul>"},{"location":"howto/train_model/#minimal-training-script","title":"Minimal training script","text":"<p>Set the architecture name you want to use (e.g., <code>opensky_2025</code>, <code>qar</code>, or your custom one registered in <code>architectures.mapping.valid_names</code>). <pre><code>import pandas as pd\nfrom node_fdm.ode_trainer import ODETrainer\nimport yaml\nfrom pathlib import Path\n\ncfg = yaml.safe_load(open(\"config.yaml\"))  # run inside scripts/opensky or scripts/qar\npaths = cfg[\"paths\"]\nprocess_dir = Path(paths[\"data_dir\"]) / paths[\"process_dir\"]\nmodels_dir = Path(paths[\"data_dir\"]) / paths[\"models_dir\"]\n\nacft = \"A320\"  # or any grouping key you use\narch = \"opensky_2025\"  # replace with your architecture\n\nsplit_df = pd.read_csv(process_dir / \"dataset_split.csv\")\ndata_df = split_df[split_df.aircraft_type == acft]\n\nmodel_config = dict(\n    architecture_name=arch,\n    model_name=f\"{arch}_{acft}\",\n    step=4,           # sample period (seconds)\n    shift=60,         # stride for sliding windows\n    seq_len=60,       # window length (steps)\n    lr=1e-3,\n    weight_decay=1e-4,\n    model_params=[3, 2, 48],   # architecture-specific (see model.py)\n    loading_args=(False, False),  # (load, load_loss)\n    batch_size=512,\n    num_workers=4,\n)\n\ntrainer = ODETrainer(\n    data_df=data_df,\n    model_config=model_config,\n    model_dir=models_dir,\n    num_workers=model_config[\"num_workers\"],\n    load_parallel=True,\n)\n\ntrainer.train(\n    epochs=10,\n    batch_size=model_config[\"batch_size\"],\n    val_batch_size=10_000,\n    method=\"euler\",   # or \"rk4\"\n    alpha_dict=None,  # defaults to 1.0 per monitored column\n)\n</code></pre></p> <p>Outputs in <code>models/&lt;arch&gt;_&lt;group&gt;/</code>:</p> <ul> <li><code>meta.json</code> with architecture name, stats, hyperparameters.</li> <li>Layer checkpoints (e.g., <code>trajectory.pt</code>, <code>data_ode.pt</code>).</li> <li><code>training_losses.csv</code> and <code>training_curve.png</code>.</li> </ul>"},{"location":"howto/train_model/#tips","title":"Tips","text":"<ul> <li>For multiple architectures, run the loop per <code>architecture_name</code> and per data subset that matches its preprocessing/columns.</li> <li><code>model_params</code> is defined by each architecture\u2019s <code>model.py</code>; keep it in sync with your custom architecture.</li> <li>To resume training, set <code>loading_args=(True, True)</code>.</li> <li>Use <code>alpha_dict</code> in <code>trainer.train</code> to rebalance losses across monitored columns (<code>X_COLS + E_COLS</code>).</li> </ul>"},{"location":"reference/architectures/","title":"Architectures","text":""},{"location":"reference/architectures/#registry","title":"Registry","text":""},{"location":"reference/architectures/#node_fdm.architectures.mapping","title":"<code>mapping</code>","text":"<p>Helpers to dynamically load and assemble architecture components.</p>"},{"location":"reference/architectures/#node_fdm.architectures.mapping.get_architecture_from_name","title":"<code>get_architecture_from_name(architecture_name)</code>","text":"<p>Return architecture definition, model columns, and custom functions by name.</p> <p>Parameters:</p> Name Type Description Default <code>architecture_name</code> <code>str</code> <p>Name of the architecture to load.</p> required <p>Returns:</p> Type Description <code>Tuple[Any, Any, Any]</code> <p>Tuple of (architecture layers, model columns, custom functions).</p> Source code in <code>src/node_fdm/architectures/mapping.py</code> <pre><code>def get_architecture_from_name(architecture_name: str) -&gt; Tuple[Any, Any, Any]:\n    \"\"\"Return architecture definition, model columns, and custom functions by name.\n\n    Args:\n        architecture_name: Name of the architecture to load.\n\n    Returns:\n        Tuple of (architecture layers, model columns, custom functions).\n    \"\"\"\n    architecture_dict = get_architecture_module(architecture_name)\n    architecture = architecture_dict[\"model\"].ARCHITECTURE\n    model_cols = architecture_dict[\"model\"].MODEL_COLS\n    custom_fn = architecture_dict[\"custom_fn\"]\n    return architecture, model_cols, custom_fn\n</code></pre>"},{"location":"reference/architectures/#node_fdm.architectures.mapping.get_architecture_module","title":"<code>get_architecture_module(name)</code>","text":"<p>Dynamically import only the architecture requested.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Architecture package name to load.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing imported column, model, and custom function modules.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided architecture name is not supported.</p> Source code in <code>src/node_fdm/architectures/mapping.py</code> <pre><code>def get_architecture_module(name: str) -&gt; Dict[str, Any]:\n    \"\"\"Dynamically import only the architecture requested.\n\n    Args:\n        name: Architecture package name to load.\n\n    Returns:\n        Dictionary containing imported column, model, and custom function modules.\n\n    Raises:\n        ValueError: If the provided architecture name is not supported.\n    \"\"\"\n    valid_names = [\"opensky_2025\", \"qar\"]\n\n    if name not in valid_names:\n        raise ValueError(f\"Unknown architecture '{name}'. Valid names: {valid_names}\")\n\n    module_root = f\"node_fdm.architectures.{name}\"\n\n    columns = importlib.import_module(f\"{module_root}.columns\")\n    flight_process = importlib.import_module(f\"{module_root}.flight_process\")\n    model = importlib.import_module(f\"{module_root}.model\")\n\n    return {\n        \"columns\": columns,\n        \"custom_fn\": (\n            flight_process.flight_processing,\n            flight_process.segment_filtering,\n        ),\n        \"model\": model,\n    }\n</code></pre>"},{"location":"reference/architectures/#node_fdm.architectures.mapping.get_architecture_params_from_meta","title":"<code>get_architecture_params_from_meta(meta_path)</code>","text":"<p>Load architecture parameters and stats from a meta JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>meta_path</code> <code>str</code> <p>Path to the meta JSON file.</p> required <p>Returns:</p> Type Description <code>Tuple[Any, Any, Any, Dict[Any, Any]]</code> <p>Tuple containing architecture, model columns, model parameters, and stats dictionary.</p> Source code in <code>src/node_fdm/architectures/mapping.py</code> <pre><code>def get_architecture_params_from_meta(\n    meta_path: str,\n) -&gt; Tuple[Any, Any, Any, Dict[Any, Any]]:\n    \"\"\"Load architecture parameters and stats from a meta JSON file.\n\n    Args:\n        meta_path: Path to the meta JSON file.\n\n    Returns:\n        Tuple containing architecture, model columns, model parameters, and stats dictionary.\n    \"\"\"\n    with open(meta_path, \"r\") as f:\n        meta = json.load(f)\n\n    architecture, model_cols, _ = get_architecture_from_name(meta[\"architecture_name\"])\n    x_cols, u_cols, e0_cols, e_cols, _ = model_cols\n    deriv_cols = [col.derivative for col in x_cols]\n    model_cols2 = [x_cols, u_cols, e0_cols, e_cols, deriv_cols]\n\n    all_cols_dict = {str(col): col for cols in model_cols2 for col in cols}\n    stats_dict = {\n        all_cols_dict[str_col]: stats for str_col, stats in meta[\"stats_dict\"].items()\n    }\n\n    return architecture, model_cols, meta[\"model_params\"], stats_dict\n</code></pre>"},{"location":"reference/architectures/#opensky-2025","title":"OpenSky 2025","text":""},{"location":"reference/architectures/#node_fdm.architectures.opensky_2025.columns","title":"<code>columns</code>","text":"<p>Column definitions and unit mappings for the OpenSky 2025 architecture.</p>"},{"location":"reference/architectures/#node_fdm.architectures.opensky_2025.flight_process","title":"<code>flight_process</code>","text":"<p>Pre-processing utilities for OpenSky 2025 flight data.</p>"},{"location":"reference/architectures/#node_fdm.architectures.opensky_2025.flight_process.flight_processing","title":"<code>flight_processing(df)</code>","text":"<p>Prepare OpenSky flight data by computing altitude differences.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing flight measurements.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with altitude difference column added.</p> Source code in <code>src/node_fdm/architectures/opensky_2025/flight_process.py</code> <pre><code>def flight_processing(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Prepare OpenSky flight data by computing altitude differences.\n\n    Args:\n        df: Input DataFrame containing flight measurements.\n\n    Returns:\n        DataFrame with altitude difference column added.\n    \"\"\"\n    df[col_alt_diff] = df[col_alt_sel] - df[col_alt]\n\n    df[col_vz_sel] = df[col_vz_sel].fillna(0.0)\n    df[col_mach_sel] = df[col_mach_sel].fillna(0.0)\n    df[col_cas_sel] = df[col_cas_sel].fillna(0.0)\n\n    return df\n</code></pre>"},{"location":"reference/architectures/#node_fdm.architectures.opensky_2025.flight_process.segment_filtering","title":"<code>segment_filtering(f, start_idx, seq_len)</code>","text":"<p>Check whether a segment meets distance variation thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>DataFrame</code> <p>DataFrame containing flight measurements.</p> required <code>start_idx</code> <code>int</code> <p>Starting index of the segment to evaluate.</p> required <code>seq_len</code> <code>int</code> <p>Length of the segment to evaluate.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the segment stays within distance thresholds, otherwise False.</p> Source code in <code>src/node_fdm/architectures/opensky_2025/flight_process.py</code> <pre><code>def segment_filtering(f: pd.DataFrame, start_idx: int, seq_len: int) -&gt; bool:\n    \"\"\"Check whether a segment meets distance variation thresholds.\n\n    Args:\n        f: DataFrame containing flight measurements.\n        start_idx: Starting index of the segment to evaluate.\n        seq_len: Length of the segment to evaluate.\n\n    Returns:\n        True if the segment stays within distance thresholds, otherwise False.\n    \"\"\"\n    dist_diff = f[col_dist].diff(1)\n    seg = dist_diff.iloc[start_idx : start_idx + seq_len]\n    condition = len(seg[(seg &lt; LOW_THR) | (seg &gt; UPPER_THR)]) == 0\n    return condition\n</code></pre>"},{"location":"reference/architectures/#node_fdm.architectures.opensky_2025.model","title":"<code>model</code>","text":"<p>Layer configuration and column grouping for the OpenSky 2025 architecture.</p>"},{"location":"reference/architectures/#node_fdm.architectures.opensky_2025.trajectory_layer","title":"<code>trajectory_layer</code>","text":"<p>Torch module for computing basic trajectory features for OpenSky 2025 data.</p>"},{"location":"reference/architectures/#node_fdm.architectures.opensky_2025.trajectory_layer.TrajectoryLayer","title":"<code>TrajectoryLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Compute trajectory outputs such as vertical speed, Mach, and calibrated airspeed.</p> Source code in <code>src/node_fdm/architectures/opensky_2025/trajectory_layer.py</code> <pre><code>class TrajectoryLayer(nn.Module):\n    \"\"\"Compute trajectory outputs such as vertical speed, Mach, and calibrated airspeed.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the trajectory layer with base configuration.\"\"\"\n        super().__init__()\n        self.alpha = 40\n\n    def forward(self, x: Mapping[Any, torch.Tensor]) -&gt; Dict[Any, torch.Tensor]:\n        \"\"\"Compute derived trajectory quantities from the provided inputs.\n\n        Args:\n            x: Mapping from column identifiers to input tensors.\n\n        Returns:\n            Dictionary of derived tensors keyed by their column identifiers.\n        \"\"\"\n        output_dict = {}\n        for col in [col_tas, col_gamma, col_alt, col_long_wind_spd]:\n            x[col] = torch.nan_to_num(x[col], nan=0.0, posinf=1e6, neginf=-1e6)\n            x[col] = torch.clamp(x[col], min=-1e6, max=1e6)\n\n        tas = x[col_tas]\n        gamma = x[col_gamma]\n        long_wind = x[col_long_wind_spd]\n        alt = x[col_alt]\n\n        output_dict[col_vz] = tas * torch.sin(gamma)\n\n        temp = isa_temperature_torch(alt)\n\n        a = torch.sqrt(torch.clamp(gamma_ratio * R * temp, min=1e-6, max=1e8))\n\n        mach = tas / torch.clamp(a, min=1e-6, max=1e8)\n        output_dict[col_mach] = mach\n\n        output_dict[col_gs] = tas - long_wind\n\n        p = isa_pressure_torch(alt)\n\n        pt_over_p = torch.pow(\n            torch.clamp(1 + (gamma_ratio - 1) / 2 * mach**2, min=1e-6, max=1e6),\n            gamma_ratio / (gamma_ratio - 1),\n        )\n\n        qc_p0 = (torch.clamp(p, min=1.0) / p0) * (pt_over_p - 1.0)\n        qc_p0 = torch.clamp(qc_p0, min=-0.999, max=1e6)\n\n        CAS_term = torch.clamp(qc_p0 + 1.0, min=1e-8, max=1e6)\n        CAS = a0 * torch.sqrt(\n            (2.0 / (gamma_ratio - 1.0))\n            * (CAS_term ** ((gamma_ratio - 1.0) / gamma_ratio) - 1.0)\n        )\n        CAS = torch.nan_to_num(CAS, nan=0.0, posinf=1e4, neginf=0.0)\n        output_dict[col_cas] = CAS\n\n        # --- Reference differences ---\n        ref_alt = x[col_alt_sel]\n\n        alt_diff = ref_alt - alt\n\n        output_dict[col_alt_diff] = torch.nan_to_num(alt_diff, nan=0.0)\n\n        return output_dict\n</code></pre>"},{"location":"reference/architectures/#node_fdm.architectures.opensky_2025.trajectory_layer.TrajectoryLayer.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the trajectory layer with base configuration.</p> Source code in <code>src/node_fdm/architectures/opensky_2025/trajectory_layer.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the trajectory layer with base configuration.\"\"\"\n    super().__init__()\n    self.alpha = 40\n</code></pre>"},{"location":"reference/architectures/#node_fdm.architectures.opensky_2025.trajectory_layer.TrajectoryLayer.forward","title":"<code>forward(x)</code>","text":"<p>Compute derived trajectory quantities from the provided inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Mapping[Any, Tensor]</code> <p>Mapping from column identifiers to input tensors.</p> required <p>Returns:</p> Type Description <code>Dict[Any, Tensor]</code> <p>Dictionary of derived tensors keyed by their column identifiers.</p> Source code in <code>src/node_fdm/architectures/opensky_2025/trajectory_layer.py</code> <pre><code>def forward(self, x: Mapping[Any, torch.Tensor]) -&gt; Dict[Any, torch.Tensor]:\n    \"\"\"Compute derived trajectory quantities from the provided inputs.\n\n    Args:\n        x: Mapping from column identifiers to input tensors.\n\n    Returns:\n        Dictionary of derived tensors keyed by their column identifiers.\n    \"\"\"\n    output_dict = {}\n    for col in [col_tas, col_gamma, col_alt, col_long_wind_spd]:\n        x[col] = torch.nan_to_num(x[col], nan=0.0, posinf=1e6, neginf=-1e6)\n        x[col] = torch.clamp(x[col], min=-1e6, max=1e6)\n\n    tas = x[col_tas]\n    gamma = x[col_gamma]\n    long_wind = x[col_long_wind_spd]\n    alt = x[col_alt]\n\n    output_dict[col_vz] = tas * torch.sin(gamma)\n\n    temp = isa_temperature_torch(alt)\n\n    a = torch.sqrt(torch.clamp(gamma_ratio * R * temp, min=1e-6, max=1e8))\n\n    mach = tas / torch.clamp(a, min=1e-6, max=1e8)\n    output_dict[col_mach] = mach\n\n    output_dict[col_gs] = tas - long_wind\n\n    p = isa_pressure_torch(alt)\n\n    pt_over_p = torch.pow(\n        torch.clamp(1 + (gamma_ratio - 1) / 2 * mach**2, min=1e-6, max=1e6),\n        gamma_ratio / (gamma_ratio - 1),\n    )\n\n    qc_p0 = (torch.clamp(p, min=1.0) / p0) * (pt_over_p - 1.0)\n    qc_p0 = torch.clamp(qc_p0, min=-0.999, max=1e6)\n\n    CAS_term = torch.clamp(qc_p0 + 1.0, min=1e-8, max=1e6)\n    CAS = a0 * torch.sqrt(\n        (2.0 / (gamma_ratio - 1.0))\n        * (CAS_term ** ((gamma_ratio - 1.0) / gamma_ratio) - 1.0)\n    )\n    CAS = torch.nan_to_num(CAS, nan=0.0, posinf=1e4, neginf=0.0)\n    output_dict[col_cas] = CAS\n\n    # --- Reference differences ---\n    ref_alt = x[col_alt_sel]\n\n    alt_diff = ref_alt - alt\n\n    output_dict[col_alt_diff] = torch.nan_to_num(alt_diff, nan=0.0)\n\n    return output_dict\n</code></pre>"},{"location":"reference/data/","title":"Data pipeline","text":""},{"location":"reference/data/#node_fdm.data.dataset","title":"<code>dataset</code>","text":"<p>Dataset utilities for loading and segmenting flight data sequences.</p>"},{"location":"reference/data/#node_fdm.data.dataset.SeqDataset","title":"<code>SeqDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Sequence dataset that loads flight segments for model training.</p> Source code in <code>src/node_fdm/data/dataset.py</code> <pre><code>class SeqDataset(Dataset):\n    \"\"\"Sequence dataset that loads flight segments for model training.\"\"\"\n\n    def __init__(\n        self,\n        flights_path_list: Sequence[str],\n        model_cols: Tuple[Any, Any, Any, Any, Any],\n        seq_len: int = 60,\n        shift: int = 60,\n        n_jobs: int = 35,\n        load_parallel: bool = True,\n        custom_fn: Tuple[\n            Optional[Callable[[pd.DataFrame], pd.DataFrame]],\n            Optional[Callable[..., bool]],\n        ] = (None, None),\n    ) -&gt; None:\n        \"\"\"Initialize the dataset with flight paths and model column definitions.\n\n        Args:\n            flights_path_list: Iterable of flight parquet file paths.\n            model_cols: Tuple containing model column groups (state, control, env, etc.).\n            seq_len: Sequence length to extract from each flight.\n            shift: Step size when sliding the sequence window.\n            n_jobs: Number of parallel workers to use when loading flights.\n            load_parallel: Whether to load flights concurrently.\n            custom_fn: Tuple of optional processing and segment-filtering callables.\n        \"\"\"\n        self.flights_path_list = flights_path_list\n        self.shift = shift\n        self.seq_len = seq_len\n        self.x_cols, self.u_cols, self.e0_cols, self.e_cols, self.dx_cols = model_cols\n        self.deriv_cols = [col.derivative for col in self.x_cols]\n        self.model_cols = model_cols\n        self.load_parallel = load_parallel\n        self.n_jobs = n_jobs\n        custom_processing_fn, custom_segment_filtering_fn = custom_fn\n        self.processor = FlightProcessor(\n            model_cols, custom_processing_fn=custom_processing_fn\n        )\n        self.custom_segment_filtering_fn = custom_segment_filtering_fn\n        self.init_flight_date()\n\n    def init_flight_date(self) -&gt; None:\n        \"\"\"Load all flights, build sequence cache, and compute aggregate statistics.\n\n        Populates internal sequence list and per-column statistics used for normalization.\n        \"\"\"\n        if self.load_parallel:\n            results = Parallel(n_jobs=self.n_jobs)(\n                delayed(self.process_one_flight)(\n                    flight,\n                )\n                for flight in tqdm(self.flights_path_list, desc=\"Loading flights\")\n            )\n        else:\n            results = [\n                self.process_one_flight(flight)\n                for flight in tqdm(self.flights_path_list, desc=\"Loading flights\")\n            ]\n\n        self.sequences = []\n        for seqs in results:\n            self.sequences.extend(seqs)\n\n        all_data = np.concatenate(\n            [\n                np.concatenate([seq[0], seq[1], seq[2], seq[3]], axis=1)\n                for seq in self.sequences\n            ],\n            axis=0,\n        )\n\n        all_cols = (\n            self.x_cols + self.u_cols + self.e0_cols + self.e_cols + self.deriv_cols\n        )\n\n        self.stats_dict = dict()\n\n        for i, col in enumerate(all_cols):\n            vals = all_data[:, i].astype(float)\n            self.stats_dict[col] = {\n                \"mean\": vals.mean(),\n                \"std\": vals.std() + 1e-6,\n                \"max\": np.percentile(np.abs(vals), 99.5),\n            }\n\n    def process_one_flight(\n        self, flight_path: str\n    ) -&gt; List[Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]]:\n        \"\"\"Process a single flight file into clean, nan-free sequences.\n\n        Args:\n            flight_path: Path to a flight parquet file.\n\n        Returns:\n            List of tuples containing state, control, environment, and derivative arrays.\n        \"\"\"\n        f = self.read_flight(flight_path)\n        seqs = []\n        N = len(f)\n        if N &gt; self.seq_len:\n            x_seq = f[self.x_cols].values.astype(np.float32)\n            u_seq = f[self.u_cols].values.astype(np.float32)\n            e_seq = f[self.e0_cols + self.e_cols].values.astype(np.float32)\n            dx_seq = f[self.deriv_cols].values.astype(np.float32)\n\n            for start in range(0, N - self.seq_len + 1, self.shift):\n                custom_segment_filtering_bool = True\n                if self.custom_segment_filtering_fn is not None:\n                    custom_segment_filtering_bool = self.custom_segment_filtering_fn(\n                        f, start, self.seq_len\n                    )\n                nans = sum(\n                    [\n                        np.isnan(seq[start : start + self.seq_len]).sum()\n                        for seq in [x_seq, u_seq, e_seq, dx_seq]\n                    ]\n                )\n                if (custom_segment_filtering_bool) &amp; (nans == 0):\n                    seqs.append(\n                        (\n                            x_seq[start : start + self.seq_len],\n                            u_seq[start : start + self.seq_len],\n                            e_seq[start : start + self.seq_len],\n                            dx_seq[start : start + self.seq_len],\n                        )\n                    )\n        return seqs\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return number of available sequences.\n\n        Returns:\n            Count of cached flight sequences.\n        \"\"\"\n        return len(self.sequences)\n\n    def __getitem__(\n        self, idx: int\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Return tensors for a specific sequence index.\n\n        Args:\n            idx: Index of the sequence to retrieve.\n\n        Returns:\n            Tuple of tensors for state, control, environment, and derivative slices.\n        \"\"\"\n        x_seq, u_seq, e_seq, dxdt_seq = self.sequences[idx]\n        return (\n            torch.tensor(x_seq, dtype=torch.float32),\n            torch.tensor(u_seq, dtype=torch.float32),\n            torch.tensor(e_seq, dtype=torch.float32),\n            torch.tensor(dxdt_seq, dtype=torch.float32),\n        )\n\n    def read_flight(self, flight_path: str) -&gt; pd.DataFrame:\n        \"\"\"Read a flight parquet file and apply base processing.\n\n        Args:\n            flight_path: Path to a parquet file containing flight data.\n\n        Returns:\n            Processed DataFrame with standardized columns.\n        \"\"\"\n        f = pd.read_parquet(flight_path)\n        return self.processor.process_flight(f)\n\n    def get_full_flight(\n        self, flight_idx: int\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, pd.DataFrame]:\n        \"\"\"Return full arrays for a specific flight index.\n\n        Args:\n            flight_idx: Index of the flight in the provided flight list.\n\n        Returns:\n            Tuple of state, control, environment, derivative arrays, and the full DataFrame.\n        \"\"\"\n        flight_path = self.flights_path_list[flight_idx]\n        f = self.read_flight(flight_path)\n        x_seq = f[self.x_cols].values.astype(np.float32)\n        u_seq = f[self.u_cols].values.astype(np.float32)\n        e0_seq = f[self.e0_cols + self.e_cols].values.astype(np.float32)\n        dx_seq = f[self.deriv_cols].values.astype(np.float32)\n        return x_seq, u_seq, e0_seq, dx_seq, f\n</code></pre>"},{"location":"reference/data/#node_fdm.data.dataset.SeqDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Return tensors for a specific sequence index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the sequence to retrieve.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>Tuple of tensors for state, control, environment, and derivative slices.</p> Source code in <code>src/node_fdm/data/dataset.py</code> <pre><code>def __getitem__(\n    self, idx: int\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Return tensors for a specific sequence index.\n\n    Args:\n        idx: Index of the sequence to retrieve.\n\n    Returns:\n        Tuple of tensors for state, control, environment, and derivative slices.\n    \"\"\"\n    x_seq, u_seq, e_seq, dxdt_seq = self.sequences[idx]\n    return (\n        torch.tensor(x_seq, dtype=torch.float32),\n        torch.tensor(u_seq, dtype=torch.float32),\n        torch.tensor(e_seq, dtype=torch.float32),\n        torch.tensor(dxdt_seq, dtype=torch.float32),\n    )\n</code></pre>"},{"location":"reference/data/#node_fdm.data.dataset.SeqDataset.__init__","title":"<code>__init__(flights_path_list, model_cols, seq_len=60, shift=60, n_jobs=35, load_parallel=True, custom_fn=(None, None))</code>","text":"<p>Initialize the dataset with flight paths and model column definitions.</p> <p>Parameters:</p> Name Type Description Default <code>flights_path_list</code> <code>Sequence[str]</code> <p>Iterable of flight parquet file paths.</p> required <code>model_cols</code> <code>Tuple[Any, Any, Any, Any, Any]</code> <p>Tuple containing model column groups (state, control, env, etc.).</p> required <code>seq_len</code> <code>int</code> <p>Sequence length to extract from each flight.</p> <code>60</code> <code>shift</code> <code>int</code> <p>Step size when sliding the sequence window.</p> <code>60</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel workers to use when loading flights.</p> <code>35</code> <code>load_parallel</code> <code>bool</code> <p>Whether to load flights concurrently.</p> <code>True</code> <code>custom_fn</code> <code>Tuple[Optional[Callable[[DataFrame], DataFrame]], Optional[Callable[..., bool]]]</code> <p>Tuple of optional processing and segment-filtering callables.</p> <code>(None, None)</code> Source code in <code>src/node_fdm/data/dataset.py</code> <pre><code>def __init__(\n    self,\n    flights_path_list: Sequence[str],\n    model_cols: Tuple[Any, Any, Any, Any, Any],\n    seq_len: int = 60,\n    shift: int = 60,\n    n_jobs: int = 35,\n    load_parallel: bool = True,\n    custom_fn: Tuple[\n        Optional[Callable[[pd.DataFrame], pd.DataFrame]],\n        Optional[Callable[..., bool]],\n    ] = (None, None),\n) -&gt; None:\n    \"\"\"Initialize the dataset with flight paths and model column definitions.\n\n    Args:\n        flights_path_list: Iterable of flight parquet file paths.\n        model_cols: Tuple containing model column groups (state, control, env, etc.).\n        seq_len: Sequence length to extract from each flight.\n        shift: Step size when sliding the sequence window.\n        n_jobs: Number of parallel workers to use when loading flights.\n        load_parallel: Whether to load flights concurrently.\n        custom_fn: Tuple of optional processing and segment-filtering callables.\n    \"\"\"\n    self.flights_path_list = flights_path_list\n    self.shift = shift\n    self.seq_len = seq_len\n    self.x_cols, self.u_cols, self.e0_cols, self.e_cols, self.dx_cols = model_cols\n    self.deriv_cols = [col.derivative for col in self.x_cols]\n    self.model_cols = model_cols\n    self.load_parallel = load_parallel\n    self.n_jobs = n_jobs\n    custom_processing_fn, custom_segment_filtering_fn = custom_fn\n    self.processor = FlightProcessor(\n        model_cols, custom_processing_fn=custom_processing_fn\n    )\n    self.custom_segment_filtering_fn = custom_segment_filtering_fn\n    self.init_flight_date()\n</code></pre>"},{"location":"reference/data/#node_fdm.data.dataset.SeqDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return number of available sequences.</p> <p>Returns:</p> Type Description <code>int</code> <p>Count of cached flight sequences.</p> Source code in <code>src/node_fdm/data/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return number of available sequences.\n\n    Returns:\n        Count of cached flight sequences.\n    \"\"\"\n    return len(self.sequences)\n</code></pre>"},{"location":"reference/data/#node_fdm.data.dataset.SeqDataset.get_full_flight","title":"<code>get_full_flight(flight_idx)</code>","text":"<p>Return full arrays for a specific flight index.</p> <p>Parameters:</p> Name Type Description Default <code>flight_idx</code> <code>int</code> <p>Index of the flight in the provided flight list.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray, ndarray, DataFrame]</code> <p>Tuple of state, control, environment, derivative arrays, and the full DataFrame.</p> Source code in <code>src/node_fdm/data/dataset.py</code> <pre><code>def get_full_flight(\n    self, flight_idx: int\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, pd.DataFrame]:\n    \"\"\"Return full arrays for a specific flight index.\n\n    Args:\n        flight_idx: Index of the flight in the provided flight list.\n\n    Returns:\n        Tuple of state, control, environment, derivative arrays, and the full DataFrame.\n    \"\"\"\n    flight_path = self.flights_path_list[flight_idx]\n    f = self.read_flight(flight_path)\n    x_seq = f[self.x_cols].values.astype(np.float32)\n    u_seq = f[self.u_cols].values.astype(np.float32)\n    e0_seq = f[self.e0_cols + self.e_cols].values.astype(np.float32)\n    dx_seq = f[self.deriv_cols].values.astype(np.float32)\n    return x_seq, u_seq, e0_seq, dx_seq, f\n</code></pre>"},{"location":"reference/data/#node_fdm.data.dataset.SeqDataset.init_flight_date","title":"<code>init_flight_date()</code>","text":"<p>Load all flights, build sequence cache, and compute aggregate statistics.</p> <p>Populates internal sequence list and per-column statistics used for normalization.</p> Source code in <code>src/node_fdm/data/dataset.py</code> <pre><code>def init_flight_date(self) -&gt; None:\n    \"\"\"Load all flights, build sequence cache, and compute aggregate statistics.\n\n    Populates internal sequence list and per-column statistics used for normalization.\n    \"\"\"\n    if self.load_parallel:\n        results = Parallel(n_jobs=self.n_jobs)(\n            delayed(self.process_one_flight)(\n                flight,\n            )\n            for flight in tqdm(self.flights_path_list, desc=\"Loading flights\")\n        )\n    else:\n        results = [\n            self.process_one_flight(flight)\n            for flight in tqdm(self.flights_path_list, desc=\"Loading flights\")\n        ]\n\n    self.sequences = []\n    for seqs in results:\n        self.sequences.extend(seqs)\n\n    all_data = np.concatenate(\n        [\n            np.concatenate([seq[0], seq[1], seq[2], seq[3]], axis=1)\n            for seq in self.sequences\n        ],\n        axis=0,\n    )\n\n    all_cols = (\n        self.x_cols + self.u_cols + self.e0_cols + self.e_cols + self.deriv_cols\n    )\n\n    self.stats_dict = dict()\n\n    for i, col in enumerate(all_cols):\n        vals = all_data[:, i].astype(float)\n        self.stats_dict[col] = {\n            \"mean\": vals.mean(),\n            \"std\": vals.std() + 1e-6,\n            \"max\": np.percentile(np.abs(vals), 99.5),\n        }\n</code></pre>"},{"location":"reference/data/#node_fdm.data.dataset.SeqDataset.process_one_flight","title":"<code>process_one_flight(flight_path)</code>","text":"<p>Process a single flight file into clean, nan-free sequences.</p> <p>Parameters:</p> Name Type Description Default <code>flight_path</code> <code>str</code> <p>Path to a flight parquet file.</p> required <p>Returns:</p> Type Description <code>List[Tuple[ndarray, ndarray, ndarray, ndarray]]</code> <p>List of tuples containing state, control, environment, and derivative arrays.</p> Source code in <code>src/node_fdm/data/dataset.py</code> <pre><code>def process_one_flight(\n    self, flight_path: str\n) -&gt; List[Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]]:\n    \"\"\"Process a single flight file into clean, nan-free sequences.\n\n    Args:\n        flight_path: Path to a flight parquet file.\n\n    Returns:\n        List of tuples containing state, control, environment, and derivative arrays.\n    \"\"\"\n    f = self.read_flight(flight_path)\n    seqs = []\n    N = len(f)\n    if N &gt; self.seq_len:\n        x_seq = f[self.x_cols].values.astype(np.float32)\n        u_seq = f[self.u_cols].values.astype(np.float32)\n        e_seq = f[self.e0_cols + self.e_cols].values.astype(np.float32)\n        dx_seq = f[self.deriv_cols].values.astype(np.float32)\n\n        for start in range(0, N - self.seq_len + 1, self.shift):\n            custom_segment_filtering_bool = True\n            if self.custom_segment_filtering_fn is not None:\n                custom_segment_filtering_bool = self.custom_segment_filtering_fn(\n                    f, start, self.seq_len\n                )\n            nans = sum(\n                [\n                    np.isnan(seq[start : start + self.seq_len]).sum()\n                    for seq in [x_seq, u_seq, e_seq, dx_seq]\n                ]\n            )\n            if (custom_segment_filtering_bool) &amp; (nans == 0):\n                seqs.append(\n                    (\n                        x_seq[start : start + self.seq_len],\n                        u_seq[start : start + self.seq_len],\n                        e_seq[start : start + self.seq_len],\n                        dx_seq[start : start + self.seq_len],\n                    )\n                )\n    return seqs\n</code></pre>"},{"location":"reference/data/#node_fdm.data.dataset.SeqDataset.read_flight","title":"<code>read_flight(flight_path)</code>","text":"<p>Read a flight parquet file and apply base processing.</p> <p>Parameters:</p> Name Type Description Default <code>flight_path</code> <code>str</code> <p>Path to a parquet file containing flight data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Processed DataFrame with standardized columns.</p> Source code in <code>src/node_fdm/data/dataset.py</code> <pre><code>def read_flight(self, flight_path: str) -&gt; pd.DataFrame:\n    \"\"\"Read a flight parquet file and apply base processing.\n\n    Args:\n        flight_path: Path to a parquet file containing flight data.\n\n    Returns:\n        Processed DataFrame with standardized columns.\n    \"\"\"\n    f = pd.read_parquet(flight_path)\n    return self.processor.process_flight(f)\n</code></pre>"},{"location":"reference/data/#node_fdm.data.loader","title":"<code>loader</code>","text":"<p>Helper for building train/validation datasets.</p>"},{"location":"reference/data/#node_fdm.data.loader.get_train_val_data","title":"<code>get_train_val_data(data_df, model_cols, shift=60, seq_len=60, custom_fn=(None, None), load_parallel=True, train_val_num=(5000, 500))</code>","text":"<p>Create training and validation datasets from a labeled file list.</p> <p>Parameters:</p> Name Type Description Default <code>data_df</code> <code>DataFrame</code> <p>DataFrame containing file paths with a <code>split</code> column.</p> required <code>model_cols</code> <p>Tuple containing model column groups.</p> required <code>shift</code> <code>int</code> <p>Window shift used when generating sequences.</p> <code>60</code> <code>seq_len</code> <code>int</code> <p>Sequence length for each sample.</p> <code>60</code> <code>custom_fn</code> <code>Tuple[Optional[Callable[[DataFrame], DataFrame]], Optional[Callable[..., bool]]]</code> <p>Tuple of optional processing and segment-filtering callables.</p> <code>(None, None)</code> <code>load_parallel</code> <code>bool</code> <p>Whether to load flights concurrently.</p> <code>True</code> <code>train_val_num</code> <code>Tuple[int, int]</code> <p>Maximum number of train and validation files to load.</p> <code>(5000, 500)</code> <p>Returns:</p> Type Description <code>Tuple[SeqDataset, SeqDataset]</code> <p>Tuple of training and validation SeqDataset instances.</p> Source code in <code>src/node_fdm/data/loader.py</code> <pre><code>def get_train_val_data(\n    data_df: pd.DataFrame,\n    model_cols,\n    shift: int = 60,\n    seq_len: int = 60,\n    custom_fn: Tuple[\n        Optional[Callable[[pd.DataFrame], pd.DataFrame]], Optional[Callable[..., bool]]\n    ] = (None, None),\n    load_parallel: bool = True,\n    train_val_num: Tuple[int, int] = (5000, 500),\n) -&gt; Tuple[SeqDataset, SeqDataset]:\n    \"\"\"Create training and validation datasets from a labeled file list.\n\n    Args:\n        data_df: DataFrame containing file paths with a `split` column.\n        model_cols: Tuple containing model column groups.\n        shift: Window shift used when generating sequences.\n        seq_len: Sequence length for each sample.\n        custom_fn: Tuple of optional processing and segment-filtering callables.\n        load_parallel: Whether to load flights concurrently.\n        train_val_num: Maximum number of train and validation files to load.\n\n    Returns:\n        Tuple of training and validation SeqDataset instances.\n    \"\"\"\n\n    train_files = data_df[data_df.split == \"train\"].filepath.tolist()\n    validation_files = data_df[data_df.split == \"val\"].filepath.tolist()\n\n    train_dataset = SeqDataset(\n        train_files[: train_val_num[0]],\n        model_cols,\n        seq_len=seq_len,\n        shift=shift,\n        custom_fn=custom_fn,\n        load_parallel=load_parallel,\n    )\n    val_dataset = SeqDataset(\n        validation_files[: train_val_num[1]],\n        model_cols,\n        seq_len=seq_len,\n        shift=shift,\n        custom_fn=custom_fn,\n        load_parallel=load_parallel,\n    )\n    return train_dataset, val_dataset\n</code></pre>"},{"location":"reference/data/#node_fdm.data.flight_processor","title":"<code>flight_processor</code>","text":"<p>Flight preprocessing pipeline for converting raw data into model-ready columns.</p>"},{"location":"reference/data/#node_fdm.data.flight_processor.FlightProcessor","title":"<code>FlightProcessor</code>","text":"<p>Flexible flight data processor with a customizable post-processing hook.</p> Source code in <code>src/node_fdm/data/flight_processor.py</code> <pre><code>class FlightProcessor:\n    \"\"\"Flexible flight data processor with a customizable post-processing hook.\"\"\"\n\n    def __init__(\n        self,\n        model_cols: Tuple[Any, Any, Any, Any, Any],\n        custom_processing_fn: Optional[Callable[[Any], Any]] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the processor with model column configuration and hooks.\n\n        Args:\n            model_cols: Tuple of model column groups (state, control, env, etc.).\n            custom_processing_fn: Optional callable applied after base processing; uses Any for flexibility with DataFrame-like inputs.\n        \"\"\"\n        (\n            self.x_cols,\n            self.u_cols,\n            self.e0_cols,\n            self.e_cols,\n            self.dx_cols,\n        ) = model_cols\n        self.dx_cols = [col.derivative for col in self.x_cols]\n        self.custom_processing_fn = custom_processing_fn\n\n    # ------------------------------------------------------------------\n    def process_flight(self, df: Any) -&gt; DataFrameWrapper:\n        \"\"\"Run the main flight preprocessing pipeline.\n\n        Args:\n            df: DataFrame-like object containing raw flight data. Uses Any for flexibility across wrappers.\n\n        Returns:\n            Processed DataFrameWrapper filtered to model-relevant columns.\n        \"\"\"\n\n        df = DataFrameWrapper(df)\n\n        for col in Column.get_all():\n            raw_col = col.raw_name\n            gold_col = col.col_name\n            if raw_col is not None and raw_col in df.columns:\n                df[gold_col] = col.unit.convert(df[raw_col])\n\n        for col in self.x_cols:\n            df[col.derivative] = df[col].diff(1).bfill()\n\n        if self.custom_processing_fn is not None:\n            df = self.custom_processing_fn(df)\n\n        return df[self.x_cols + self.u_cols + self.e0_cols + self.e_cols + self.dx_cols]\n</code></pre>"},{"location":"reference/data/#node_fdm.data.flight_processor.FlightProcessor.__init__","title":"<code>__init__(model_cols, custom_processing_fn=None)</code>","text":"<p>Initialize the processor with model column configuration and hooks.</p> <p>Parameters:</p> Name Type Description Default <code>model_cols</code> <code>Tuple[Any, Any, Any, Any, Any]</code> <p>Tuple of model column groups (state, control, env, etc.).</p> required <code>custom_processing_fn</code> <code>Optional[Callable[[Any], Any]]</code> <p>Optional callable applied after base processing; uses Any for flexibility with DataFrame-like inputs.</p> <code>None</code> Source code in <code>src/node_fdm/data/flight_processor.py</code> <pre><code>def __init__(\n    self,\n    model_cols: Tuple[Any, Any, Any, Any, Any],\n    custom_processing_fn: Optional[Callable[[Any], Any]] = None,\n) -&gt; None:\n    \"\"\"Initialize the processor with model column configuration and hooks.\n\n    Args:\n        model_cols: Tuple of model column groups (state, control, env, etc.).\n        custom_processing_fn: Optional callable applied after base processing; uses Any for flexibility with DataFrame-like inputs.\n    \"\"\"\n    (\n        self.x_cols,\n        self.u_cols,\n        self.e0_cols,\n        self.e_cols,\n        self.dx_cols,\n    ) = model_cols\n    self.dx_cols = [col.derivative for col in self.x_cols]\n    self.custom_processing_fn = custom_processing_fn\n</code></pre>"},{"location":"reference/data/#node_fdm.data.flight_processor.FlightProcessor.process_flight","title":"<code>process_flight(df)</code>","text":"<p>Run the main flight preprocessing pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Any</code> <p>DataFrame-like object containing raw flight data. Uses Any for flexibility across wrappers.</p> required <p>Returns:</p> Type Description <code>DataFrameWrapper</code> <p>Processed DataFrameWrapper filtered to model-relevant columns.</p> Source code in <code>src/node_fdm/data/flight_processor.py</code> <pre><code>def process_flight(self, df: Any) -&gt; DataFrameWrapper:\n    \"\"\"Run the main flight preprocessing pipeline.\n\n    Args:\n        df: DataFrame-like object containing raw flight data. Uses Any for flexibility across wrappers.\n\n    Returns:\n        Processed DataFrameWrapper filtered to model-relevant columns.\n    \"\"\"\n\n    df = DataFrameWrapper(df)\n\n    for col in Column.get_all():\n        raw_col = col.raw_name\n        gold_col = col.col_name\n        if raw_col is not None and raw_col in df.columns:\n            df[gold_col] = col.unit.convert(df[raw_col])\n\n    for col in self.x_cols:\n        df[col.derivative] = df[col].diff(1).bfill()\n\n    if self.custom_processing_fn is not None:\n        df = self.custom_processing_fn(df)\n\n    return df[self.x_cols + self.u_cols + self.e0_cols + self.e_cols + self.dx_cols]\n</code></pre>"},{"location":"reference/models/","title":"Model wrappers","text":""},{"location":"reference/models/#node_fdm.models.flight_dynamics_model","title":"<code>flight_dynamics_model</code>","text":"<p>Neural flight dynamics model assembled from architecture layers.</p>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model.FlightDynamicsModel","title":"<code>FlightDynamicsModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Compute state derivatives using a layered flight dynamics architecture.</p> Source code in <code>src/node_fdm/models/flight_dynamics_model.py</code> <pre><code>class FlightDynamicsModel(nn.Module):\n    \"\"\"Compute state derivatives using a layered flight dynamics architecture.\"\"\"\n\n    def __init__(\n        self,\n        architecture: Sequence[Any],\n        stats_dict: Dict[Any, Dict[str, float]],\n        model_cols: Tuple[Any, Any, Any, Any, Any],\n        model_params: Sequence[int] = (2, 1, 48),\n    ) -&gt; None:\n        \"\"\"Initialize the model with architecture definition and statistics.\n\n        Args:\n            architecture: Iterable of layer definitions `(name, class, inputs, outputs, structured_flag)`.\n            stats_dict: Mapping from column to normalization/denormalization statistics.\n            model_cols: Tuple of model column groups (state, control, env, env_extra, derivatives).\n            model_params: Sequence defining backbone depth, head depth, and hidden width.\n        \"\"\"\n        super().__init__()\n        self.architecture = architecture\n        self.stats_dict = stats_dict\n        self.x_cols, self.u_cols, self.e0_cols, self.e_cols, self.dx_cols = model_cols\n        self.backbone_depth, self.head_depth, self.neurons_num = model_params\n        self.layers_dict = nn.ModuleDict({})\n        self.layers_name = []\n\n        for name, layer_class, input_cols, ouput_cols, structured in self.architecture:\n            self.layers_name.append(name)\n            if structured:\n                self.layers_dict[name] = self.create_structured_layer(\n                    input_cols,\n                    ouput_cols,\n                    layer_class=layer_class,\n                )\n            else:\n                self.layers_dict[name] = layer_class()\n\n    def reset_history(self):\n        \"\"\"Reset internal history buffers.\n\n        Clears stored layer outputs used for debugging or analysis between runs.\n        \"\"\"\n        self.history = {}\n\n    def create_structured_layer(\n        self,\n        input_cols: Sequence[Any],\n        output_cols: Sequence[Any],\n        layer_class: Any = StructuredLayer,\n    ) -&gt; nn.Module:\n        \"\"\"Build a structured layer with normalization and denormalization stats.\n\n        Args:\n            input_cols: Columns consumed by the layer.\n            output_cols: Columns produced by the layer.\n            layer_class: Layer implementation to instantiate.\n\n        Returns:\n            Configured structured layer instance.\n        \"\"\"\n        input_stats = [\n            {\n                col.col_name: self.stats_dict[col][metric]\n                for col in input_cols\n                if col.normalize_mode is not None\n            }\n            for metric in [\"mean\", \"std\"]\n        ]\n        output_stats = [\n            {\n                col.col_name: self.stats_dict[col][metric]\n                for col in output_cols\n                if col.denormalize_mode is not None\n            }\n            for metric in [\"mean\", \"std\", \"max\"]\n        ]\n\n        layer = layer_class(\n            input_cols,\n            input_stats,\n            output_cols,\n            output_stats,\n            backbone_dim=self.neurons_num,\n            backbone_depth=self.backbone_depth,\n            head_dim=self.neurons_num // 2,\n            head_depth=self.head_depth,\n        )\n\n        return layer\n\n    def forward(\n        self, x: torch.Tensor, u_t: torch.Tensor, e_t: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute state derivatives for the current batch.\n\n        Args:\n            x: State tensor.\n            u_t: Control tensor interpolated at current time.\n            e_t: Environment tensor interpolated at current time.\n\n        Returns:\n            Tensor of state derivatives assembled from architecture outputs.\n        \"\"\"\n\n        vects = torch.cat([x, u_t, e_t], dim=1)\n        vect_dict = dict()\n        for i, col in enumerate(self.x_cols + self.u_cols + self.e0_cols):\n            vect_dict[col] = vects[..., i]\n\n        for name in self.layers_name:\n            vect_dict = vect_dict | self.layers_dict[name](vect_dict)\n\n        ode_output = torch.stack(\n            [coeff * vect_dict[col] for coeff, col in self.dx_cols],\n            dim=1,\n        )\n\n        for col, vect in vect_dict.items():\n            if torch.isnan(vect).any():\n                pass\n            if col in self.history.keys():\n                self.history[col] = torch.cat(\n                    [self.history[col], vect.unsqueeze(1)], dim=1\n                )\n            else:\n                self.history[col] = vect.unsqueeze(1)\n\n        return ode_output\n</code></pre>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model.FlightDynamicsModel.__init__","title":"<code>__init__(architecture, stats_dict, model_cols, model_params=(2, 1, 48))</code>","text":"<p>Initialize the model with architecture definition and statistics.</p> <p>Parameters:</p> Name Type Description Default <code>architecture</code> <code>Sequence[Any]</code> <p>Iterable of layer definitions <code>(name, class, inputs, outputs, structured_flag)</code>.</p> required <code>stats_dict</code> <code>Dict[Any, Dict[str, float]]</code> <p>Mapping from column to normalization/denormalization statistics.</p> required <code>model_cols</code> <code>Tuple[Any, Any, Any, Any, Any]</code> <p>Tuple of model column groups (state, control, env, env_extra, derivatives).</p> required <code>model_params</code> <code>Sequence[int]</code> <p>Sequence defining backbone depth, head depth, and hidden width.</p> <code>(2, 1, 48)</code> Source code in <code>src/node_fdm/models/flight_dynamics_model.py</code> <pre><code>def __init__(\n    self,\n    architecture: Sequence[Any],\n    stats_dict: Dict[Any, Dict[str, float]],\n    model_cols: Tuple[Any, Any, Any, Any, Any],\n    model_params: Sequence[int] = (2, 1, 48),\n) -&gt; None:\n    \"\"\"Initialize the model with architecture definition and statistics.\n\n    Args:\n        architecture: Iterable of layer definitions `(name, class, inputs, outputs, structured_flag)`.\n        stats_dict: Mapping from column to normalization/denormalization statistics.\n        model_cols: Tuple of model column groups (state, control, env, env_extra, derivatives).\n        model_params: Sequence defining backbone depth, head depth, and hidden width.\n    \"\"\"\n    super().__init__()\n    self.architecture = architecture\n    self.stats_dict = stats_dict\n    self.x_cols, self.u_cols, self.e0_cols, self.e_cols, self.dx_cols = model_cols\n    self.backbone_depth, self.head_depth, self.neurons_num = model_params\n    self.layers_dict = nn.ModuleDict({})\n    self.layers_name = []\n\n    for name, layer_class, input_cols, ouput_cols, structured in self.architecture:\n        self.layers_name.append(name)\n        if structured:\n            self.layers_dict[name] = self.create_structured_layer(\n                input_cols,\n                ouput_cols,\n                layer_class=layer_class,\n            )\n        else:\n            self.layers_dict[name] = layer_class()\n</code></pre>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model.FlightDynamicsModel.create_structured_layer","title":"<code>create_structured_layer(input_cols, output_cols, layer_class=StructuredLayer)</code>","text":"<p>Build a structured layer with normalization and denormalization stats.</p> <p>Parameters:</p> Name Type Description Default <code>input_cols</code> <code>Sequence[Any]</code> <p>Columns consumed by the layer.</p> required <code>output_cols</code> <code>Sequence[Any]</code> <p>Columns produced by the layer.</p> required <code>layer_class</code> <code>Any</code> <p>Layer implementation to instantiate.</p> <code>StructuredLayer</code> <p>Returns:</p> Type Description <code>Module</code> <p>Configured structured layer instance.</p> Source code in <code>src/node_fdm/models/flight_dynamics_model.py</code> <pre><code>def create_structured_layer(\n    self,\n    input_cols: Sequence[Any],\n    output_cols: Sequence[Any],\n    layer_class: Any = StructuredLayer,\n) -&gt; nn.Module:\n    \"\"\"Build a structured layer with normalization and denormalization stats.\n\n    Args:\n        input_cols: Columns consumed by the layer.\n        output_cols: Columns produced by the layer.\n        layer_class: Layer implementation to instantiate.\n\n    Returns:\n        Configured structured layer instance.\n    \"\"\"\n    input_stats = [\n        {\n            col.col_name: self.stats_dict[col][metric]\n            for col in input_cols\n            if col.normalize_mode is not None\n        }\n        for metric in [\"mean\", \"std\"]\n    ]\n    output_stats = [\n        {\n            col.col_name: self.stats_dict[col][metric]\n            for col in output_cols\n            if col.denormalize_mode is not None\n        }\n        for metric in [\"mean\", \"std\", \"max\"]\n    ]\n\n    layer = layer_class(\n        input_cols,\n        input_stats,\n        output_cols,\n        output_stats,\n        backbone_dim=self.neurons_num,\n        backbone_depth=self.backbone_depth,\n        head_dim=self.neurons_num // 2,\n        head_depth=self.head_depth,\n    )\n\n    return layer\n</code></pre>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model.FlightDynamicsModel.forward","title":"<code>forward(x, u_t, e_t)</code>","text":"<p>Compute state derivatives for the current batch.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>State tensor.</p> required <code>u_t</code> <code>Tensor</code> <p>Control tensor interpolated at current time.</p> required <code>e_t</code> <code>Tensor</code> <p>Environment tensor interpolated at current time.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of state derivatives assembled from architecture outputs.</p> Source code in <code>src/node_fdm/models/flight_dynamics_model.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, u_t: torch.Tensor, e_t: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Compute state derivatives for the current batch.\n\n    Args:\n        x: State tensor.\n        u_t: Control tensor interpolated at current time.\n        e_t: Environment tensor interpolated at current time.\n\n    Returns:\n        Tensor of state derivatives assembled from architecture outputs.\n    \"\"\"\n\n    vects = torch.cat([x, u_t, e_t], dim=1)\n    vect_dict = dict()\n    for i, col in enumerate(self.x_cols + self.u_cols + self.e0_cols):\n        vect_dict[col] = vects[..., i]\n\n    for name in self.layers_name:\n        vect_dict = vect_dict | self.layers_dict[name](vect_dict)\n\n    ode_output = torch.stack(\n        [coeff * vect_dict[col] for coeff, col in self.dx_cols],\n        dim=1,\n    )\n\n    for col, vect in vect_dict.items():\n        if torch.isnan(vect).any():\n            pass\n        if col in self.history.keys():\n            self.history[col] = torch.cat(\n                [self.history[col], vect.unsqueeze(1)], dim=1\n            )\n        else:\n            self.history[col] = vect.unsqueeze(1)\n\n    return ode_output\n</code></pre>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model.FlightDynamicsModel.reset_history","title":"<code>reset_history()</code>","text":"<p>Reset internal history buffers.</p> <p>Clears stored layer outputs used for debugging or analysis between runs.</p> Source code in <code>src/node_fdm/models/flight_dynamics_model.py</code> <pre><code>def reset_history(self):\n    \"\"\"Reset internal history buffers.\n\n    Clears stored layer outputs used for debugging or analysis between runs.\n    \"\"\"\n    self.history = {}\n</code></pre>"},{"location":"reference/models/#node_fdm.models.batch_neural_ode","title":"<code>batch_neural_ode</code>","text":"<p>Batch-compatible Neural ODE wrapper that interpolates inputs over time.</p>"},{"location":"reference/models/#node_fdm.models.batch_neural_ode.BatchNeuralODE","title":"<code>BatchNeuralODE</code>","text":"<p>               Bases: <code>Module</code></p> <p>Wrap a neural ODE with batched control and environment inputs.</p> Source code in <code>src/node_fdm/models/batch_neural_ode.py</code> <pre><code>class BatchNeuralODE(nn.Module):\n    \"\"\"Wrap a neural ODE with batched control and environment inputs.\"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        u_seq: torch.Tensor,\n        e_seq: torch.Tensor,\n        t_grid: torch.Tensor,\n    ) -&gt; None:\n        \"\"\"Initialize the ODE wrapper and reset model history.\n\n        Args:\n            model: Base neural ODE model taking `(x, u_t, e_t)`.\n            u_seq: Control inputs over time with shape `(batch, time, features)`.\n            e_seq: Environment inputs over time with shape `(batch, time, features)`.\n            t_grid: Monotonic time grid corresponding to `u_seq` and `e_seq`.\n        \"\"\"\n        super().__init__()\n        self.model = model\n        self.model.reset_history()\n        self.u_seq = u_seq\n        self.e_seq = e_seq\n        self.t_grid = t_grid\n\n    def forward(self, t: torch.Tensor, x: torch.Tensor) -&gt; Any:\n        \"\"\"Evaluate the ODE dynamics at time `t` with linear interpolation.\n\n        Args:\n            t: Scalar tensor containing the evaluation time.\n            x: Current state tensor.\n\n        Returns:\n            Model output of the wrapped dynamics at time `t`.\n        \"\"\"\n        t = t.item()\n        idx = torch.searchsorted(\n            self.t_grid, torch.tensor(t, device=self.t_grid.device)\n        ).item()\n        idx0 = max(0, idx - 1)\n        idx1 = min(idx, self.t_grid.shape[0] - 1)\n\n        t0, t1 = self.t_grid[idx0].item(), self.t_grid[idx1].item()\n        alpha = 0 if t1 == t0 else (t - t0) / (t1 - t0)\n\n        u0, u1 = self.u_seq[:, idx0, :], self.u_seq[:, idx1, :]\n        e0, e1 = self.e_seq[:, idx0, :], self.e_seq[:, idx1, :]\n\n        u_t = (1 - alpha) * u0 + alpha * u1\n        e_t = (1 - alpha) * e0 + alpha * e1\n\n        return self.model(x, u_t, e_t)\n</code></pre>"},{"location":"reference/models/#node_fdm.models.batch_neural_ode.BatchNeuralODE.__init__","title":"<code>__init__(model, u_seq, e_seq, t_grid)</code>","text":"<p>Initialize the ODE wrapper and reset model history.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Base neural ODE model taking <code>(x, u_t, e_t)</code>.</p> required <code>u_seq</code> <code>Tensor</code> <p>Control inputs over time with shape <code>(batch, time, features)</code>.</p> required <code>e_seq</code> <code>Tensor</code> <p>Environment inputs over time with shape <code>(batch, time, features)</code>.</p> required <code>t_grid</code> <code>Tensor</code> <p>Monotonic time grid corresponding to <code>u_seq</code> and <code>e_seq</code>.</p> required Source code in <code>src/node_fdm/models/batch_neural_ode.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    u_seq: torch.Tensor,\n    e_seq: torch.Tensor,\n    t_grid: torch.Tensor,\n) -&gt; None:\n    \"\"\"Initialize the ODE wrapper and reset model history.\n\n    Args:\n        model: Base neural ODE model taking `(x, u_t, e_t)`.\n        u_seq: Control inputs over time with shape `(batch, time, features)`.\n        e_seq: Environment inputs over time with shape `(batch, time, features)`.\n        t_grid: Monotonic time grid corresponding to `u_seq` and `e_seq`.\n    \"\"\"\n    super().__init__()\n    self.model = model\n    self.model.reset_history()\n    self.u_seq = u_seq\n    self.e_seq = e_seq\n    self.t_grid = t_grid\n</code></pre>"},{"location":"reference/models/#node_fdm.models.batch_neural_ode.BatchNeuralODE.forward","title":"<code>forward(t, x)</code>","text":"<p>Evaluate the ODE dynamics at time <code>t</code> with linear interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Tensor</code> <p>Scalar tensor containing the evaluation time.</p> required <code>x</code> <code>Tensor</code> <p>Current state tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Model output of the wrapped dynamics at time <code>t</code>.</p> Source code in <code>src/node_fdm/models/batch_neural_ode.py</code> <pre><code>def forward(self, t: torch.Tensor, x: torch.Tensor) -&gt; Any:\n    \"\"\"Evaluate the ODE dynamics at time `t` with linear interpolation.\n\n    Args:\n        t: Scalar tensor containing the evaluation time.\n        x: Current state tensor.\n\n    Returns:\n        Model output of the wrapped dynamics at time `t`.\n    \"\"\"\n    t = t.item()\n    idx = torch.searchsorted(\n        self.t_grid, torch.tensor(t, device=self.t_grid.device)\n    ).item()\n    idx0 = max(0, idx - 1)\n    idx1 = min(idx, self.t_grid.shape[0] - 1)\n\n    t0, t1 = self.t_grid[idx0].item(), self.t_grid[idx1].item()\n    alpha = 0 if t1 == t0 else (t - t0) / (t1 - t0)\n\n    u0, u1 = self.u_seq[:, idx0, :], self.u_seq[:, idx1, :]\n    e0, e1 = self.e_seq[:, idx0, :], self.e_seq[:, idx1, :]\n\n    u_t = (1 - alpha) * u0 + alpha * u1\n    e_t = (1 - alpha) * e0 + alpha * e1\n\n    return self.model(x, u_t, e_t)\n</code></pre>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model_prod","title":"<code>flight_dynamics_model_prod</code>","text":"<p>Production-ready flight dynamics model loader and evaluator.</p>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model_prod.FlightDynamicsModelProd","title":"<code>FlightDynamicsModelProd</code>","text":"<p>               Bases: <code>Module</code></p> <p>Load pretrained flight dynamics layers and expose an evaluation interface.</p> Source code in <code>src/node_fdm/models/flight_dynamics_model_prod.py</code> <pre><code>class FlightDynamicsModelProd(nn.Module):\n    \"\"\"Load pretrained flight dynamics layers and expose an evaluation interface.\"\"\"\n\n    def __init__(\n        self,\n        model_path: Any,\n    ) -&gt; None:\n        \"\"\"Initialize and load pretrained layers from a model directory.\n\n        Args:\n            model_path: Path-like pointing to the directory containing checkpoints and meta.json.\n        \"\"\"\n        super().__init__()\n        self.model_path = model_path\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        meta_path = model_path / \"meta.json\"\n        self.architecture, self.model_cols, model_params, self.stats_dict = (\n            get_architecture_params_from_meta(meta_path)\n        )\n        self.backbone_depth, self.head_depth, self.neurons_num = model_params\n        self.layers_dict = nn.ModuleDict({})\n        self.layers_name = []\n        for name, layer_class, input_cols, ouput_cols, structured in self.architecture:\n            self.layers_name.append(name)\n            if structured:\n                self.layers_dict[name] = self.create_structured_layer(\n                    input_cols,\n                    ouput_cols,\n                    layer_class=layer_class,\n                )\n            else:\n                self.layers_dict[name] = layer_class()\n            if name != \"trajectory\":\n                checkpoint = self.load_layer_checkpoint(name)\n                self.layers_dict[name].load_state_dict(\n                    checkpoint[\"layer_state\"], strict=False\n                )\n                self.layers_dict[name] = self.layers_dict[name].eval()\n\n    def load_layer_checkpoint(self, layer_name: str) -&gt; Any:\n        \"\"\"Load checkpoint for a given layer.\n\n        Args:\n            layer_name: Name of the layer whose weights should be loaded.\n\n        Returns:\n            Loaded checkpoint dictionary.\n        \"\"\"\n        path = os.path.join(self.model_path, f\"{layer_name}.pt\")\n        checkpoint = torch.load(path, map_location=self.device)\n        return checkpoint\n\n    def create_structured_layer(\n        self,\n        input_cols: Sequence[Any],\n        output_cols: Sequence[Any],\n        layer_class: Any = StructuredLayer,\n    ) -&gt; nn.Module:\n        \"\"\"Build a structured layer with normalization and denormalization stats.\n\n        Args:\n            input_cols: Columns consumed by the layer.\n            output_cols: Columns produced by the layer.\n            layer_class: Layer implementation to instantiate.\n\n        Returns:\n            Configured structured layer instance.\n        \"\"\"\n        input_stats = [\n            {\n                col.col_name: self.stats_dict[col][metric]\n                for col in input_cols\n                if col.normalize_mode is not None\n            }\n            for metric in [\"mean\", \"std\"]\n        ]\n        output_stats = [\n            {\n                col.col_name: self.stats_dict[col][metric]\n                for col in output_cols\n                if col.denormalize_mode is not None\n            }\n            for metric in [\"mean\", \"std\", \"max\"]\n        ]\n\n        layer = layer_class(\n            input_cols,\n            input_stats,\n            output_cols,\n            output_stats,\n            backbone_dim=self.neurons_num,\n            backbone_depth=self.backbone_depth,\n            head_dim=self.neurons_num // 2,\n            head_depth=self.head_depth,\n        )\n\n        return layer\n\n    def forward(self, vect_dict: dict) -&gt; dict:\n        \"\"\"Run a forward pass through all layers.\n\n        Args:\n            vect_dict: Mapping from column identifiers to tensors.\n\n        Returns:\n            Updated mapping with newly computed columns.\n        \"\"\"\n        for name in self.layers_name:\n            res = self.layers_dict[name](vect_dict)\n            vect_dict |= res\n\n        return vect_dict\n</code></pre>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model_prod.FlightDynamicsModelProd.__init__","title":"<code>__init__(model_path)</code>","text":"<p>Initialize and load pretrained layers from a model directory.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Any</code> <p>Path-like pointing to the directory containing checkpoints and meta.json.</p> required Source code in <code>src/node_fdm/models/flight_dynamics_model_prod.py</code> <pre><code>def __init__(\n    self,\n    model_path: Any,\n) -&gt; None:\n    \"\"\"Initialize and load pretrained layers from a model directory.\n\n    Args:\n        model_path: Path-like pointing to the directory containing checkpoints and meta.json.\n    \"\"\"\n    super().__init__()\n    self.model_path = model_path\n    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    meta_path = model_path / \"meta.json\"\n    self.architecture, self.model_cols, model_params, self.stats_dict = (\n        get_architecture_params_from_meta(meta_path)\n    )\n    self.backbone_depth, self.head_depth, self.neurons_num = model_params\n    self.layers_dict = nn.ModuleDict({})\n    self.layers_name = []\n    for name, layer_class, input_cols, ouput_cols, structured in self.architecture:\n        self.layers_name.append(name)\n        if structured:\n            self.layers_dict[name] = self.create_structured_layer(\n                input_cols,\n                ouput_cols,\n                layer_class=layer_class,\n            )\n        else:\n            self.layers_dict[name] = layer_class()\n        if name != \"trajectory\":\n            checkpoint = self.load_layer_checkpoint(name)\n            self.layers_dict[name].load_state_dict(\n                checkpoint[\"layer_state\"], strict=False\n            )\n            self.layers_dict[name] = self.layers_dict[name].eval()\n</code></pre>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model_prod.FlightDynamicsModelProd.create_structured_layer","title":"<code>create_structured_layer(input_cols, output_cols, layer_class=StructuredLayer)</code>","text":"<p>Build a structured layer with normalization and denormalization stats.</p> <p>Parameters:</p> Name Type Description Default <code>input_cols</code> <code>Sequence[Any]</code> <p>Columns consumed by the layer.</p> required <code>output_cols</code> <code>Sequence[Any]</code> <p>Columns produced by the layer.</p> required <code>layer_class</code> <code>Any</code> <p>Layer implementation to instantiate.</p> <code>StructuredLayer</code> <p>Returns:</p> Type Description <code>Module</code> <p>Configured structured layer instance.</p> Source code in <code>src/node_fdm/models/flight_dynamics_model_prod.py</code> <pre><code>def create_structured_layer(\n    self,\n    input_cols: Sequence[Any],\n    output_cols: Sequence[Any],\n    layer_class: Any = StructuredLayer,\n) -&gt; nn.Module:\n    \"\"\"Build a structured layer with normalization and denormalization stats.\n\n    Args:\n        input_cols: Columns consumed by the layer.\n        output_cols: Columns produced by the layer.\n        layer_class: Layer implementation to instantiate.\n\n    Returns:\n        Configured structured layer instance.\n    \"\"\"\n    input_stats = [\n        {\n            col.col_name: self.stats_dict[col][metric]\n            for col in input_cols\n            if col.normalize_mode is not None\n        }\n        for metric in [\"mean\", \"std\"]\n    ]\n    output_stats = [\n        {\n            col.col_name: self.stats_dict[col][metric]\n            for col in output_cols\n            if col.denormalize_mode is not None\n        }\n        for metric in [\"mean\", \"std\", \"max\"]\n    ]\n\n    layer = layer_class(\n        input_cols,\n        input_stats,\n        output_cols,\n        output_stats,\n        backbone_dim=self.neurons_num,\n        backbone_depth=self.backbone_depth,\n        head_dim=self.neurons_num // 2,\n        head_depth=self.head_depth,\n    )\n\n    return layer\n</code></pre>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model_prod.FlightDynamicsModelProd.forward","title":"<code>forward(vect_dict)</code>","text":"<p>Run a forward pass through all layers.</p> <p>Parameters:</p> Name Type Description Default <code>vect_dict</code> <code>dict</code> <p>Mapping from column identifiers to tensors.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Updated mapping with newly computed columns.</p> Source code in <code>src/node_fdm/models/flight_dynamics_model_prod.py</code> <pre><code>def forward(self, vect_dict: dict) -&gt; dict:\n    \"\"\"Run a forward pass through all layers.\n\n    Args:\n        vect_dict: Mapping from column identifiers to tensors.\n\n    Returns:\n        Updated mapping with newly computed columns.\n    \"\"\"\n    for name in self.layers_name:\n        res = self.layers_dict[name](vect_dict)\n        vect_dict |= res\n\n    return vect_dict\n</code></pre>"},{"location":"reference/models/#node_fdm.models.flight_dynamics_model_prod.FlightDynamicsModelProd.load_layer_checkpoint","title":"<code>load_layer_checkpoint(layer_name)</code>","text":"<p>Load checkpoint for a given layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer_name</code> <code>str</code> <p>Name of the layer whose weights should be loaded.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Loaded checkpoint dictionary.</p> Source code in <code>src/node_fdm/models/flight_dynamics_model_prod.py</code> <pre><code>def load_layer_checkpoint(self, layer_name: str) -&gt; Any:\n    \"\"\"Load checkpoint for a given layer.\n\n    Args:\n        layer_name: Name of the layer whose weights should be loaded.\n\n    Returns:\n        Loaded checkpoint dictionary.\n    \"\"\"\n    path = os.path.join(self.model_path, f\"{layer_name}.pt\")\n    checkpoint = torch.load(path, map_location=self.device)\n    return checkpoint\n</code></pre>"},{"location":"reference/node_fdm/","title":"node_fdm package overview","text":"<p>Core namespaces and their responsibilities (all architectures share these): - <code>node_fdm.ode_trainer</code> \u2014 training loop and checkpointing for modular Neural ODEs. - <code>node_fdm.predictor</code> \u2014 inference helper to roll out trajectories. - <code>node_fdm.data</code> \u2014 dataset construction, preprocessing hooks, loaders. - <code>node_fdm.architectures</code> \u2014 architecture registry and built-in definitions (<code>opensky_2025</code>, <code>qar</code>, add your own). - <code>node_fdm.models</code> \u2014 model wrappers and ODE integration utilities.</p> <p>Use the dedicated pages in this section for full API details. If you need a top-level view of package exports:</p>"},{"location":"reference/node_fdm/#node_fdm","title":"<code>node_fdm</code>","text":"<p>Package initialization for node_fdm utilities and models.</p>"},{"location":"reference/ode_trainer/","title":"ODE trainer","text":""},{"location":"reference/ode_trainer/#node_fdm.ode_trainer","title":"<code>ode_trainer</code>","text":"<p>Training utilities for neural ODE-based flight dynamics models.</p>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer","title":"<code>ODETrainer</code>","text":"<p>Handle data preparation, training loops, and checkpointing for ODE models.</p> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>class ODETrainer:\n    \"\"\"Handle data preparation, training loops, and checkpointing for ODE models.\"\"\"\n\n    def __init__(\n        self,\n        data_df: pd.DataFrame,\n        model_config: Dict[str, Any],\n        model_dir: Any,\n        num_workers: int = 4,\n        load_parallel: bool = True,\n        train_val_num: Tuple[int, int] = (5000, 500),\n    ) -&gt; None:\n        \"\"\"Initialize trainer with data, model configuration, and I/O paths.\n\n        Args:\n            data_df: DataFrame containing file paths and split labels.\n            model_config: Dictionary describing architecture, hyperparameters, and loader settings.\n            model_dir: Base directory to store checkpoints and metadata.\n            num_workers: Number of workers for DataLoaders.\n            load_parallel: Whether to load flights in parallel.\n            train_val_num: Tuple specifying how many train/val files to load.\n        \"\"\"\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        architecture, self.model_cols, custom_fn = get_architecture_from_name(\n            model_config[\"architecture_name\"]\n        )\n        self.x_cols, self.u_cols, self.e0_cols, self.e_cols, self.dx_cols = (\n            self.model_cols\n        )\n        self.model_dir = model_dir / model_config[\"model_name\"]\n        os.makedirs(self.model_dir, exist_ok=True)\n        self.architecture = architecture\n        self.model_config = model_config\n        self.architecture_name = model_config[\"architecture_name\"]\n        self.model_params = model_config[\"model_params\"]\n\n        self.train_dataset, self.val_dataset = get_train_val_data(\n            data_df,\n            self.model_cols,\n            shift=model_config[\"shift\"],\n            seq_len=model_config[\"seq_len\"],\n            custom_fn=custom_fn,\n            load_parallel=load_parallel,\n            train_val_num=train_val_num,\n        )\n        self.step = model_config[\"step\"]\n        self.num_workers = num_workers\n\n        self.stats_dict = self.train_dataset.stats_dict\n\n        self.model = self.get_or_create_model(*model_config[\"loading_args\"])\n\n        self.optimizer = torch.optim.AdamW(\n            self.model.parameters(),\n            lr=model_config[\"lr\"],\n            weight_decay=model_config[\"weight_decay\"],\n        )\n        self.epoch = 1\n        self.save_meta()\n\n    def get_or_create_model(\n        self, load: bool = False, load_loss: bool = False\n    ) -&gt; FlightDynamicsModel:\n        \"\"\"Instantiate a new model or load existing checkpoints.\n\n        Args:\n            load: Whether to attempt loading existing checkpoints.\n            load_loss: Whether to restore tracked best validation loss when loading.\n\n        Returns:\n            Initialized or restored `FlightDynamicsModel` instance.\n        \"\"\"\n        self.best_val_loss = float(\"inf\")\n        if load and os.path.exists(self.model_dir / \"meta.json\"):\n            model = self.load_best_checkpoint(load_loss=load_loss)\n        else:\n            print(\"Creating new model.\")\n            model = FlightDynamicsModel(\n                self.architecture,\n                self.stats_dict,\n                self.model_cols,\n                model_params=self.model_params,\n            ).to(self.device)\n        return model\n\n    def load_best_checkpoint(self, load_loss: bool = False) -&gt; FlightDynamicsModel:\n        \"\"\"Create and populate a model from saved checkpoints.\n\n        Args:\n            load_loss: Whether to restore tracked best validation loss.\n\n        Returns:\n            Model with layer weights loaded when available.\n        \"\"\"\n        model = FlightDynamicsModel(\n            self.architecture,\n            self.stats_dict,\n            self.model_cols,\n            model_params=self.model_params,\n        ).to(self.device)\n\n        for name in model.layers_name:\n            checkpoint = self.load_layer_checkpoint(name)\n            if checkpoint is not None:\n                model.layers_dict[name].load_state_dict(\n                    checkpoint[\"layer_state\"], strict=False\n                )\n                best_val_loss = checkpoint.get(\"best_val_loss\", float(\"inf\"))\n                self.epoch = checkpoint.get(\"epoch\", 0)\n            else:\n                best_val_loss = float(\"inf\")\n                self.epoch = 0\n\n        if load_loss:\n            self.best_val_loss = best_val_loss\n\n        print(\"Best val loss per layer:\", self.best_val_loss)\n        print(f\"Loaded modular model from {self.model_dir}\")\n\n        return model\n\n    def load_layer_checkpoint(self, layer_name: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Load checkpoint dictionary for a specific layer if available.\n\n        Args:\n            layer_name: Name of the layer to load.\n\n        Returns:\n            Checkpoint dictionary if found, otherwise None.\n        \"\"\"\n        path = os.path.join(self.model_dir, f\"{layer_name}.pt\")\n        if not os.path.exists(path):\n            print(f\"No checkpoint found for layer {layer_name}, skipping load.\")\n            return None\n        else:\n            print(f\"checkpoint found for layer {layer_name}\")\n        checkpoint = torch.load(path, map_location=self.device)\n        return checkpoint\n\n    def save_meta(self) -&gt; None:\n        \"\"\"Persist training metadata and statistics to disk.\n\n        Creates or updates `meta.json` within the model directory.\n        \"\"\"\n        saved_stats_dict = {str(col): value for col, value in self.stats_dict.items()}\n\n        meta_dict = {\n            \"architecture_name\": self.architecture_name,\n            \"model_params\": self.model_config[\"model_params\"],\n            \"step\": self.model_config[\"step\"],\n            \"shift\": self.model_config[\"shift\"],\n            \"lr\": self.model_config[\"lr\"],\n            \"seq_len\": self.model_config[\"seq_len\"],\n            \"batch_size\": self.model_config[\"batch_size\"],\n            \"stats_dict\": saved_stats_dict,\n        }\n        print(self.model_dir / \"meta.json\")\n        with open(self.model_dir / \"meta.json\", \"w\") as f:\n            json.dump(meta_dict, f, indent=4)\n\n    def save_layer_checkpoint(self, layer_name: str, epoch: int) -&gt; None:\n        \"\"\"Save checkpoint for an individual layer.\n\n        Args:\n            layer_name: Name of the layer to checkpoint.\n            epoch: Current epoch offset for tracking.\n        \"\"\"\n        layer = self.model.layers_dict[layer_name]\n        save_dict = {\n            \"layer_state\": layer.state_dict(),\n            \"optimizer_state\": self.optimizer.state_dict(),\n            \"best_val_loss\": self.best_val_loss,\n            \"epoch\": self.epoch + epoch,\n        }\n        torch.save(save_dict, self.model_dir / f\"{layer_name}.pt\")\n\n    def save_model(self, epoch: int) -&gt; None:\n        \"\"\"Save checkpoints for all layers.\n\n        Args:\n            epoch: Epoch index used when saving checkpoints.\n        \"\"\"\n        for name in self.model.layers_name:\n            self.save_layer_checkpoint(name, epoch)\n\n    def norm_vect(self, vect: torch.Tensor, col: Any) -&gt; torch.Tensor:\n        \"\"\"Normalize tensor using stored statistics for a column.\n\n        Args:\n            vect: Tensor to normalize.\n            col: Column identifier used to fetch statistics.\n\n        Returns:\n            Normalized tensor.\n        \"\"\"\n        return (vect - self.stats_dict[col][\"mean\"]) / (\n            self.stats_dict[col][\"std\"] + 1e-3\n        )\n\n    def cat_to_dict_vects(\n        self,\n        vect_list: Sequence[torch.Tensor],\n        col_list: Sequence[Any],\n        alpha_dict: Dict[Any, float],\n        normalize: bool = True,\n    ) -&gt; Dict[Any, torch.Tensor]:\n        \"\"\"Concatenate vectors and build a dict keyed by column definitions.\n\n        Args:\n            vect_list: Sequence of tensors to concatenate along the feature axis.\n            col_list: Column identifiers matching the concatenated tensors.\n            alpha_dict: Optional scaling factors applied per column.\n            normalize: Whether to normalize columns that request it.\n\n        Returns:\n            Dictionary mapping columns to (optionally) scaled and normalized tensors.\n        \"\"\"\n\n        def modifier(el: torch.Tensor, col: Any) -&gt; torch.Tensor:\n            if (col.normalize_mode == \"normal\") &amp; (normalize):\n                return self.norm_vect(el, col)\n            return el\n\n        coeff_list = [\n            alpha_dict[col] if col in alpha_dict.keys() else 0.0 for col in col_list\n        ]\n\n        vects = torch.cat(vect_list, dim=2)\n        vects_dict = {\n            col: coeff * modifier(vects[..., i], col).unsqueeze(-1)\n            for i, (col, coeff) in enumerate(zip(col_list, coeff_list))\n        }\n        return vects_dict\n\n    def ode_step(\n        self,\n        x_seq: torch.Tensor,\n        u_seq: torch.Tensor,\n        e_seq: torch.Tensor,\n        method: str,\n        alpha_dict: Dict[Any, float],\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, Sequence[Any]]:\n        \"\"\"Integrate one ODE step and return true/predicted trajectories.\n\n        Args:\n            x_seq: State sequences for the batch.\n            u_seq: Control sequences for the batch.\n            e_seq: Environment sequences for the batch.\n            method: ODE solver method passed to `odeint`.\n            alpha_dict: Scaling factors per monitored column.\n\n        Returns:\n            Tuple of (true trajectories, predicted trajectories, monitored columns).\n        \"\"\"\n        seq_len = x_seq.shape[1]\n\n        assert not torch.isnan(x_seq).any(), \"NaN in x_seq\"\n        assert not torch.isnan(u_seq).any(), \"NaN in u_seq\"\n        assert not torch.isnan(e_seq).any(), \"NaN in e_seq\"\n\n        x0 = x_seq[:, 0, :]\n\n        t_grid = torch.arange(\n            0, seq_len * self.step, self.step, dtype=torch.float32, device=self.device\n        )\n\n        func = BatchNeuralODE(self.model, u_seq, e_seq, t_grid)\n\n        odeint(func, x0, t_grid, method=method)\n\n        vects = torch.cat([x_seq, u_seq, e_seq], dim=2)\n        vect_dict = {\n            col: vects[..., i].unsqueeze(-1)\n            for i, col in enumerate(\n                self.x_cols + self.u_cols + self.e0_cols + self.e_cols\n            )\n        }\n\n        vects_dict = dict()\n\n        monitor_cols = self.x_cols + self.e_cols\n\n        for case in [\"true\", \"pred\"]:\n            if case == \"pred\":\n                vect_list = [\n                    self.model.history[col].unsqueeze(-1) for col in monitor_cols\n                ]\n            else:\n                vect_list = [vect_dict[col][:, 1:] for col in monitor_cols]\n\n            vects_dict[case] = self.cat_to_dict_vects(\n                vect_list,\n                monitor_cols,\n                alpha_dict=alpha_dict,\n            )\n        true_vect = torch.cat([vects_dict[\"true\"][col] for col in monitor_cols], dim=2)\n        pred_vect = torch.cat([vects_dict[\"pred\"][col] for col in monitor_cols], dim=2)\n        return true_vect, pred_vect, monitor_cols\n\n    def compute_loss_ode_step(\n        self,\n        batch: Sequence[torch.Tensor],\n        alpha_dict: Dict[Any, float],\n        method: str = \"rk4\",\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute loss for a single ODE rollout batch.\n\n        Args:\n            batch: Tuple of tensors `(x_seq, u_seq, e_seq, dx_seq)` from the DataLoader.\n            alpha_dict: Scaling factors per monitored column.\n            method: ODE solver method.\n\n        Returns:\n            Scalar loss tensor for the batch.\n        \"\"\"\n        x_seq, u_seq, e_seq, _ = [b.to(self.device) for b in batch]\n        true_vect, pred_vect, monitor_cols = self.ode_step(\n            x_seq,\n            u_seq,\n            e_seq,\n            method,\n            alpha_dict,\n        )\n\n        loss = 0.0\n        for i, col in enumerate(monitor_cols):\n            if col in alpha_dict.keys():\n                loss_fn = get_loss(col.loss_name)\n                assert not torch.isnan(pred_vect[..., i]).any(), \"NaN in pred_vect\"\n                assert not torch.isnan(true_vect[..., i]).any(), \"NaN in true_vect\"\n                res = loss_fn(pred_vect[..., i], true_vect[..., i])\n                loss += res\n\n        if torch.isnan(loss) or torch.isinf(loss):\n            print(\"NaN or Inf in loss!\")\n\n        return loss\n\n    def train(\n        self,\n        epochs: int = 800,\n        batch_size: int = 512,\n        val_batch_size: int = 10000,\n        scheduler: Optional[Any] = None,\n        method: str = \"rk4\",\n        alpha_dict: Optional[Dict[Any, float]] = None,\n    ) -&gt; None:\n        \"\"\"Train the ODE model and persist checkpoints/metrics.\n\n        Args:\n            epochs: Number of training epochs.\n            batch_size: Training batch size.\n            val_batch_size: Validation batch size.\n            scheduler: Optional learning-rate scheduler.\n            method: ODE solver method.\n            alpha_dict: Optional scaling factors per monitored column.\n        \"\"\"\n        self.train_loader = DataLoader(\n            self.train_dataset,\n            batch_size=batch_size,\n            shuffle=True,\n            num_workers=self.num_workers,\n        )\n        self.val_loader = DataLoader(\n            self.val_dataset,\n            batch_size=val_batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n        )\n\n        if alpha_dict is None:\n            alpha_dict = {col: 1.0 for col in self.x_cols}\n\n        self.stats_dict = self.train_dataset.stats_dict\n\n        losses = []\n        loss_csv_path = os.path.join(self.model_dir, \"training_losses.csv\")\n        fig_path = os.path.join(self.model_dir, \"training_curve.png\")\n\n        for epoch in range(epochs):\n            # --- TRAIN LOOP ---\n            self.model.train()\n            total_loss, total_batches = 0, 0\n            for batch in self.train_loader:\n                loss = self.compute_loss_ode_step(\n                    batch, alpha_dict=alpha_dict, method=method\n                )\n                self.optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                self.optimizer.step()\n                total_loss += loss.item()\n                total_batches += 1\n            avg_train_loss = total_loss / total_batches\n\n            # --- VALIDATION LOOP ---\n            self.model.eval()\n            val_loss, val_batches = 0, 0\n            with torch.no_grad():\n                for batch in self.val_loader:\n                    loss = self.compute_loss_ode_step(\n                        batch, alpha_dict=alpha_dict, method=method\n                    )\n                    val_loss += loss.item()\n                    val_batches += 1\n            avg_val_loss = val_loss / val_batches\n\n            if scheduler is not None:\n                scheduler.step(avg_val_loss)\n\n            losses.append(\n                {\n                    \"epoch\": epoch + 1,\n                    \"train_loss\": avg_train_loss,\n                    \"val_loss\": avg_val_loss,\n                }\n            )\n\n            print(\n                f\"Epoch {epoch+1}/{epochs} | train loss: {avg_train_loss:.5f} | val loss: {avg_val_loss:.5f}\"\n            )\n\n            # --- SAVE BEST MODEL ---\n            if avg_val_loss &lt; self.best_val_loss:\n                print(f\"  New best validation loss: {avg_val_loss:.5f}. Saving model.\")\n                self.best_val_loss = avg_val_loss\n                self.save_model(epoch)\n\n        df_losses = pd.DataFrame(losses)\n        df_losses.to_csv(loss_csv_path, index=False)\n        print(f\"\u2705 Saved training log to {loss_csv_path}\")\n\n        plt.figure(figsize=(7, 4))\n        plt.semilogy(\n            df_losses[\"epoch\"],\n            df_losses[\"train_loss\"],\n            label=\"Training loss\",\n            color=\"#1f77b4\",\n            linewidth=2,\n        )\n        plt.semilogy(\n            df_losses[\"epoch\"],\n            df_losses[\"val_loss\"],\n            label=\"Validation loss\",\n            color=\"#ff7f0e\",\n            linewidth=2,\n            linestyle=\"--\",\n        )\n\n        plt.title(\"Training and validation losses\", fontsize=13)\n        plt.xlabel(\"Epoch\", fontsize=11)\n        plt.ylabel(\"Loss (log scale)\", fontsize=11)\n        plt.grid(True, which=\"both\", linestyle=\":\", linewidth=0.8, alpha=0.7)\n        plt.legend(frameon=False, fontsize=10)\n        plt.tight_layout()\n        plt.savefig(fig_path, dpi=200)\n        plt.close()\n\n        print(f\"\u2705 Saved training curve to {fig_path}\")\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.__init__","title":"<code>__init__(data_df, model_config, model_dir, num_workers=4, load_parallel=True, train_val_num=(5000, 500))</code>","text":"<p>Initialize trainer with data, model configuration, and I/O paths.</p> <p>Parameters:</p> Name Type Description Default <code>data_df</code> <code>DataFrame</code> <p>DataFrame containing file paths and split labels.</p> required <code>model_config</code> <code>Dict[str, Any]</code> <p>Dictionary describing architecture, hyperparameters, and loader settings.</p> required <code>model_dir</code> <code>Any</code> <p>Base directory to store checkpoints and metadata.</p> required <code>num_workers</code> <code>int</code> <p>Number of workers for DataLoaders.</p> <code>4</code> <code>load_parallel</code> <code>bool</code> <p>Whether to load flights in parallel.</p> <code>True</code> <code>train_val_num</code> <code>Tuple[int, int]</code> <p>Tuple specifying how many train/val files to load.</p> <code>(5000, 500)</code> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def __init__(\n    self,\n    data_df: pd.DataFrame,\n    model_config: Dict[str, Any],\n    model_dir: Any,\n    num_workers: int = 4,\n    load_parallel: bool = True,\n    train_val_num: Tuple[int, int] = (5000, 500),\n) -&gt; None:\n    \"\"\"Initialize trainer with data, model configuration, and I/O paths.\n\n    Args:\n        data_df: DataFrame containing file paths and split labels.\n        model_config: Dictionary describing architecture, hyperparameters, and loader settings.\n        model_dir: Base directory to store checkpoints and metadata.\n        num_workers: Number of workers for DataLoaders.\n        load_parallel: Whether to load flights in parallel.\n        train_val_num: Tuple specifying how many train/val files to load.\n    \"\"\"\n\n    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    architecture, self.model_cols, custom_fn = get_architecture_from_name(\n        model_config[\"architecture_name\"]\n    )\n    self.x_cols, self.u_cols, self.e0_cols, self.e_cols, self.dx_cols = (\n        self.model_cols\n    )\n    self.model_dir = model_dir / model_config[\"model_name\"]\n    os.makedirs(self.model_dir, exist_ok=True)\n    self.architecture = architecture\n    self.model_config = model_config\n    self.architecture_name = model_config[\"architecture_name\"]\n    self.model_params = model_config[\"model_params\"]\n\n    self.train_dataset, self.val_dataset = get_train_val_data(\n        data_df,\n        self.model_cols,\n        shift=model_config[\"shift\"],\n        seq_len=model_config[\"seq_len\"],\n        custom_fn=custom_fn,\n        load_parallel=load_parallel,\n        train_val_num=train_val_num,\n    )\n    self.step = model_config[\"step\"]\n    self.num_workers = num_workers\n\n    self.stats_dict = self.train_dataset.stats_dict\n\n    self.model = self.get_or_create_model(*model_config[\"loading_args\"])\n\n    self.optimizer = torch.optim.AdamW(\n        self.model.parameters(),\n        lr=model_config[\"lr\"],\n        weight_decay=model_config[\"weight_decay\"],\n    )\n    self.epoch = 1\n    self.save_meta()\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.cat_to_dict_vects","title":"<code>cat_to_dict_vects(vect_list, col_list, alpha_dict, normalize=True)</code>","text":"<p>Concatenate vectors and build a dict keyed by column definitions.</p> <p>Parameters:</p> Name Type Description Default <code>vect_list</code> <code>Sequence[Tensor]</code> <p>Sequence of tensors to concatenate along the feature axis.</p> required <code>col_list</code> <code>Sequence[Any]</code> <p>Column identifiers matching the concatenated tensors.</p> required <code>alpha_dict</code> <code>Dict[Any, float]</code> <p>Optional scaling factors applied per column.</p> required <code>normalize</code> <code>bool</code> <p>Whether to normalize columns that request it.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[Any, Tensor]</code> <p>Dictionary mapping columns to (optionally) scaled and normalized tensors.</p> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def cat_to_dict_vects(\n    self,\n    vect_list: Sequence[torch.Tensor],\n    col_list: Sequence[Any],\n    alpha_dict: Dict[Any, float],\n    normalize: bool = True,\n) -&gt; Dict[Any, torch.Tensor]:\n    \"\"\"Concatenate vectors and build a dict keyed by column definitions.\n\n    Args:\n        vect_list: Sequence of tensors to concatenate along the feature axis.\n        col_list: Column identifiers matching the concatenated tensors.\n        alpha_dict: Optional scaling factors applied per column.\n        normalize: Whether to normalize columns that request it.\n\n    Returns:\n        Dictionary mapping columns to (optionally) scaled and normalized tensors.\n    \"\"\"\n\n    def modifier(el: torch.Tensor, col: Any) -&gt; torch.Tensor:\n        if (col.normalize_mode == \"normal\") &amp; (normalize):\n            return self.norm_vect(el, col)\n        return el\n\n    coeff_list = [\n        alpha_dict[col] if col in alpha_dict.keys() else 0.0 for col in col_list\n    ]\n\n    vects = torch.cat(vect_list, dim=2)\n    vects_dict = {\n        col: coeff * modifier(vects[..., i], col).unsqueeze(-1)\n        for i, (col, coeff) in enumerate(zip(col_list, coeff_list))\n    }\n    return vects_dict\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.compute_loss_ode_step","title":"<code>compute_loss_ode_step(batch, alpha_dict, method='rk4')</code>","text":"<p>Compute loss for a single ODE rollout batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Sequence[Tensor]</code> <p>Tuple of tensors <code>(x_seq, u_seq, e_seq, dx_seq)</code> from the DataLoader.</p> required <code>alpha_dict</code> <code>Dict[Any, float]</code> <p>Scaling factors per monitored column.</p> required <code>method</code> <code>str</code> <p>ODE solver method.</p> <code>'rk4'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Scalar loss tensor for the batch.</p> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def compute_loss_ode_step(\n    self,\n    batch: Sequence[torch.Tensor],\n    alpha_dict: Dict[Any, float],\n    method: str = \"rk4\",\n) -&gt; torch.Tensor:\n    \"\"\"Compute loss for a single ODE rollout batch.\n\n    Args:\n        batch: Tuple of tensors `(x_seq, u_seq, e_seq, dx_seq)` from the DataLoader.\n        alpha_dict: Scaling factors per monitored column.\n        method: ODE solver method.\n\n    Returns:\n        Scalar loss tensor for the batch.\n    \"\"\"\n    x_seq, u_seq, e_seq, _ = [b.to(self.device) for b in batch]\n    true_vect, pred_vect, monitor_cols = self.ode_step(\n        x_seq,\n        u_seq,\n        e_seq,\n        method,\n        alpha_dict,\n    )\n\n    loss = 0.0\n    for i, col in enumerate(monitor_cols):\n        if col in alpha_dict.keys():\n            loss_fn = get_loss(col.loss_name)\n            assert not torch.isnan(pred_vect[..., i]).any(), \"NaN in pred_vect\"\n            assert not torch.isnan(true_vect[..., i]).any(), \"NaN in true_vect\"\n            res = loss_fn(pred_vect[..., i], true_vect[..., i])\n            loss += res\n\n    if torch.isnan(loss) or torch.isinf(loss):\n        print(\"NaN or Inf in loss!\")\n\n    return loss\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.get_or_create_model","title":"<code>get_or_create_model(load=False, load_loss=False)</code>","text":"<p>Instantiate a new model or load existing checkpoints.</p> <p>Parameters:</p> Name Type Description Default <code>load</code> <code>bool</code> <p>Whether to attempt loading existing checkpoints.</p> <code>False</code> <code>load_loss</code> <code>bool</code> <p>Whether to restore tracked best validation loss when loading.</p> <code>False</code> <p>Returns:</p> Type Description <code>FlightDynamicsModel</code> <p>Initialized or restored <code>FlightDynamicsModel</code> instance.</p> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def get_or_create_model(\n    self, load: bool = False, load_loss: bool = False\n) -&gt; FlightDynamicsModel:\n    \"\"\"Instantiate a new model or load existing checkpoints.\n\n    Args:\n        load: Whether to attempt loading existing checkpoints.\n        load_loss: Whether to restore tracked best validation loss when loading.\n\n    Returns:\n        Initialized or restored `FlightDynamicsModel` instance.\n    \"\"\"\n    self.best_val_loss = float(\"inf\")\n    if load and os.path.exists(self.model_dir / \"meta.json\"):\n        model = self.load_best_checkpoint(load_loss=load_loss)\n    else:\n        print(\"Creating new model.\")\n        model = FlightDynamicsModel(\n            self.architecture,\n            self.stats_dict,\n            self.model_cols,\n            model_params=self.model_params,\n        ).to(self.device)\n    return model\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.load_best_checkpoint","title":"<code>load_best_checkpoint(load_loss=False)</code>","text":"<p>Create and populate a model from saved checkpoints.</p> <p>Parameters:</p> Name Type Description Default <code>load_loss</code> <code>bool</code> <p>Whether to restore tracked best validation loss.</p> <code>False</code> <p>Returns:</p> Type Description <code>FlightDynamicsModel</code> <p>Model with layer weights loaded when available.</p> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def load_best_checkpoint(self, load_loss: bool = False) -&gt; FlightDynamicsModel:\n    \"\"\"Create and populate a model from saved checkpoints.\n\n    Args:\n        load_loss: Whether to restore tracked best validation loss.\n\n    Returns:\n        Model with layer weights loaded when available.\n    \"\"\"\n    model = FlightDynamicsModel(\n        self.architecture,\n        self.stats_dict,\n        self.model_cols,\n        model_params=self.model_params,\n    ).to(self.device)\n\n    for name in model.layers_name:\n        checkpoint = self.load_layer_checkpoint(name)\n        if checkpoint is not None:\n            model.layers_dict[name].load_state_dict(\n                checkpoint[\"layer_state\"], strict=False\n            )\n            best_val_loss = checkpoint.get(\"best_val_loss\", float(\"inf\"))\n            self.epoch = checkpoint.get(\"epoch\", 0)\n        else:\n            best_val_loss = float(\"inf\")\n            self.epoch = 0\n\n    if load_loss:\n        self.best_val_loss = best_val_loss\n\n    print(\"Best val loss per layer:\", self.best_val_loss)\n    print(f\"Loaded modular model from {self.model_dir}\")\n\n    return model\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.load_layer_checkpoint","title":"<code>load_layer_checkpoint(layer_name)</code>","text":"<p>Load checkpoint dictionary for a specific layer if available.</p> <p>Parameters:</p> Name Type Description Default <code>layer_name</code> <code>str</code> <p>Name of the layer to load.</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Checkpoint dictionary if found, otherwise None.</p> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def load_layer_checkpoint(self, layer_name: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Load checkpoint dictionary for a specific layer if available.\n\n    Args:\n        layer_name: Name of the layer to load.\n\n    Returns:\n        Checkpoint dictionary if found, otherwise None.\n    \"\"\"\n    path = os.path.join(self.model_dir, f\"{layer_name}.pt\")\n    if not os.path.exists(path):\n        print(f\"No checkpoint found for layer {layer_name}, skipping load.\")\n        return None\n    else:\n        print(f\"checkpoint found for layer {layer_name}\")\n    checkpoint = torch.load(path, map_location=self.device)\n    return checkpoint\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.norm_vect","title":"<code>norm_vect(vect, col)</code>","text":"<p>Normalize tensor using stored statistics for a column.</p> <p>Parameters:</p> Name Type Description Default <code>vect</code> <code>Tensor</code> <p>Tensor to normalize.</p> required <code>col</code> <code>Any</code> <p>Column identifier used to fetch statistics.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor.</p> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def norm_vect(self, vect: torch.Tensor, col: Any) -&gt; torch.Tensor:\n    \"\"\"Normalize tensor using stored statistics for a column.\n\n    Args:\n        vect: Tensor to normalize.\n        col: Column identifier used to fetch statistics.\n\n    Returns:\n        Normalized tensor.\n    \"\"\"\n    return (vect - self.stats_dict[col][\"mean\"]) / (\n        self.stats_dict[col][\"std\"] + 1e-3\n    )\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.ode_step","title":"<code>ode_step(x_seq, u_seq, e_seq, method, alpha_dict)</code>","text":"<p>Integrate one ODE step and return true/predicted trajectories.</p> <p>Parameters:</p> Name Type Description Default <code>x_seq</code> <code>Tensor</code> <p>State sequences for the batch.</p> required <code>u_seq</code> <code>Tensor</code> <p>Control sequences for the batch.</p> required <code>e_seq</code> <code>Tensor</code> <p>Environment sequences for the batch.</p> required <code>method</code> <code>str</code> <p>ODE solver method passed to <code>odeint</code>.</p> required <code>alpha_dict</code> <code>Dict[Any, float]</code> <p>Scaling factors per monitored column.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Sequence[Any]]</code> <p>Tuple of (true trajectories, predicted trajectories, monitored columns).</p> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def ode_step(\n    self,\n    x_seq: torch.Tensor,\n    u_seq: torch.Tensor,\n    e_seq: torch.Tensor,\n    method: str,\n    alpha_dict: Dict[Any, float],\n) -&gt; Tuple[torch.Tensor, torch.Tensor, Sequence[Any]]:\n    \"\"\"Integrate one ODE step and return true/predicted trajectories.\n\n    Args:\n        x_seq: State sequences for the batch.\n        u_seq: Control sequences for the batch.\n        e_seq: Environment sequences for the batch.\n        method: ODE solver method passed to `odeint`.\n        alpha_dict: Scaling factors per monitored column.\n\n    Returns:\n        Tuple of (true trajectories, predicted trajectories, monitored columns).\n    \"\"\"\n    seq_len = x_seq.shape[1]\n\n    assert not torch.isnan(x_seq).any(), \"NaN in x_seq\"\n    assert not torch.isnan(u_seq).any(), \"NaN in u_seq\"\n    assert not torch.isnan(e_seq).any(), \"NaN in e_seq\"\n\n    x0 = x_seq[:, 0, :]\n\n    t_grid = torch.arange(\n        0, seq_len * self.step, self.step, dtype=torch.float32, device=self.device\n    )\n\n    func = BatchNeuralODE(self.model, u_seq, e_seq, t_grid)\n\n    odeint(func, x0, t_grid, method=method)\n\n    vects = torch.cat([x_seq, u_seq, e_seq], dim=2)\n    vect_dict = {\n        col: vects[..., i].unsqueeze(-1)\n        for i, col in enumerate(\n            self.x_cols + self.u_cols + self.e0_cols + self.e_cols\n        )\n    }\n\n    vects_dict = dict()\n\n    monitor_cols = self.x_cols + self.e_cols\n\n    for case in [\"true\", \"pred\"]:\n        if case == \"pred\":\n            vect_list = [\n                self.model.history[col].unsqueeze(-1) for col in monitor_cols\n            ]\n        else:\n            vect_list = [vect_dict[col][:, 1:] for col in monitor_cols]\n\n        vects_dict[case] = self.cat_to_dict_vects(\n            vect_list,\n            monitor_cols,\n            alpha_dict=alpha_dict,\n        )\n    true_vect = torch.cat([vects_dict[\"true\"][col] for col in monitor_cols], dim=2)\n    pred_vect = torch.cat([vects_dict[\"pred\"][col] for col in monitor_cols], dim=2)\n    return true_vect, pred_vect, monitor_cols\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.save_layer_checkpoint","title":"<code>save_layer_checkpoint(layer_name, epoch)</code>","text":"<p>Save checkpoint for an individual layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer_name</code> <code>str</code> <p>Name of the layer to checkpoint.</p> required <code>epoch</code> <code>int</code> <p>Current epoch offset for tracking.</p> required Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def save_layer_checkpoint(self, layer_name: str, epoch: int) -&gt; None:\n    \"\"\"Save checkpoint for an individual layer.\n\n    Args:\n        layer_name: Name of the layer to checkpoint.\n        epoch: Current epoch offset for tracking.\n    \"\"\"\n    layer = self.model.layers_dict[layer_name]\n    save_dict = {\n        \"layer_state\": layer.state_dict(),\n        \"optimizer_state\": self.optimizer.state_dict(),\n        \"best_val_loss\": self.best_val_loss,\n        \"epoch\": self.epoch + epoch,\n    }\n    torch.save(save_dict, self.model_dir / f\"{layer_name}.pt\")\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.save_meta","title":"<code>save_meta()</code>","text":"<p>Persist training metadata and statistics to disk.</p> <p>Creates or updates <code>meta.json</code> within the model directory.</p> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def save_meta(self) -&gt; None:\n    \"\"\"Persist training metadata and statistics to disk.\n\n    Creates or updates `meta.json` within the model directory.\n    \"\"\"\n    saved_stats_dict = {str(col): value for col, value in self.stats_dict.items()}\n\n    meta_dict = {\n        \"architecture_name\": self.architecture_name,\n        \"model_params\": self.model_config[\"model_params\"],\n        \"step\": self.model_config[\"step\"],\n        \"shift\": self.model_config[\"shift\"],\n        \"lr\": self.model_config[\"lr\"],\n        \"seq_len\": self.model_config[\"seq_len\"],\n        \"batch_size\": self.model_config[\"batch_size\"],\n        \"stats_dict\": saved_stats_dict,\n    }\n    print(self.model_dir / \"meta.json\")\n    with open(self.model_dir / \"meta.json\", \"w\") as f:\n        json.dump(meta_dict, f, indent=4)\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.save_model","title":"<code>save_model(epoch)</code>","text":"<p>Save checkpoints for all layers.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch index used when saving checkpoints.</p> required Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def save_model(self, epoch: int) -&gt; None:\n    \"\"\"Save checkpoints for all layers.\n\n    Args:\n        epoch: Epoch index used when saving checkpoints.\n    \"\"\"\n    for name in self.model.layers_name:\n        self.save_layer_checkpoint(name, epoch)\n</code></pre>"},{"location":"reference/ode_trainer/#node_fdm.ode_trainer.ODETrainer.train","title":"<code>train(epochs=800, batch_size=512, val_batch_size=10000, scheduler=None, method='rk4', alpha_dict=None)</code>","text":"<p>Train the ODE model and persist checkpoints/metrics.</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>800</code> <code>batch_size</code> <code>int</code> <p>Training batch size.</p> <code>512</code> <code>val_batch_size</code> <code>int</code> <p>Validation batch size.</p> <code>10000</code> <code>scheduler</code> <code>Optional[Any]</code> <p>Optional learning-rate scheduler.</p> <code>None</code> <code>method</code> <code>str</code> <p>ODE solver method.</p> <code>'rk4'</code> <code>alpha_dict</code> <code>Optional[Dict[Any, float]]</code> <p>Optional scaling factors per monitored column.</p> <code>None</code> Source code in <code>src/node_fdm/ode_trainer.py</code> <pre><code>def train(\n    self,\n    epochs: int = 800,\n    batch_size: int = 512,\n    val_batch_size: int = 10000,\n    scheduler: Optional[Any] = None,\n    method: str = \"rk4\",\n    alpha_dict: Optional[Dict[Any, float]] = None,\n) -&gt; None:\n    \"\"\"Train the ODE model and persist checkpoints/metrics.\n\n    Args:\n        epochs: Number of training epochs.\n        batch_size: Training batch size.\n        val_batch_size: Validation batch size.\n        scheduler: Optional learning-rate scheduler.\n        method: ODE solver method.\n        alpha_dict: Optional scaling factors per monitored column.\n    \"\"\"\n    self.train_loader = DataLoader(\n        self.train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n    )\n    self.val_loader = DataLoader(\n        self.val_dataset,\n        batch_size=val_batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n    )\n\n    if alpha_dict is None:\n        alpha_dict = {col: 1.0 for col in self.x_cols}\n\n    self.stats_dict = self.train_dataset.stats_dict\n\n    losses = []\n    loss_csv_path = os.path.join(self.model_dir, \"training_losses.csv\")\n    fig_path = os.path.join(self.model_dir, \"training_curve.png\")\n\n    for epoch in range(epochs):\n        # --- TRAIN LOOP ---\n        self.model.train()\n        total_loss, total_batches = 0, 0\n        for batch in self.train_loader:\n            loss = self.compute_loss_ode_step(\n                batch, alpha_dict=alpha_dict, method=method\n            )\n            self.optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n            self.optimizer.step()\n            total_loss += loss.item()\n            total_batches += 1\n        avg_train_loss = total_loss / total_batches\n\n        # --- VALIDATION LOOP ---\n        self.model.eval()\n        val_loss, val_batches = 0, 0\n        with torch.no_grad():\n            for batch in self.val_loader:\n                loss = self.compute_loss_ode_step(\n                    batch, alpha_dict=alpha_dict, method=method\n                )\n                val_loss += loss.item()\n                val_batches += 1\n        avg_val_loss = val_loss / val_batches\n\n        if scheduler is not None:\n            scheduler.step(avg_val_loss)\n\n        losses.append(\n            {\n                \"epoch\": epoch + 1,\n                \"train_loss\": avg_train_loss,\n                \"val_loss\": avg_val_loss,\n            }\n        )\n\n        print(\n            f\"Epoch {epoch+1}/{epochs} | train loss: {avg_train_loss:.5f} | val loss: {avg_val_loss:.5f}\"\n        )\n\n        # --- SAVE BEST MODEL ---\n        if avg_val_loss &lt; self.best_val_loss:\n            print(f\"  New best validation loss: {avg_val_loss:.5f}. Saving model.\")\n            self.best_val_loss = avg_val_loss\n            self.save_model(epoch)\n\n    df_losses = pd.DataFrame(losses)\n    df_losses.to_csv(loss_csv_path, index=False)\n    print(f\"\u2705 Saved training log to {loss_csv_path}\")\n\n    plt.figure(figsize=(7, 4))\n    plt.semilogy(\n        df_losses[\"epoch\"],\n        df_losses[\"train_loss\"],\n        label=\"Training loss\",\n        color=\"#1f77b4\",\n        linewidth=2,\n    )\n    plt.semilogy(\n        df_losses[\"epoch\"],\n        df_losses[\"val_loss\"],\n        label=\"Validation loss\",\n        color=\"#ff7f0e\",\n        linewidth=2,\n        linestyle=\"--\",\n    )\n\n    plt.title(\"Training and validation losses\", fontsize=13)\n    plt.xlabel(\"Epoch\", fontsize=11)\n    plt.ylabel(\"Loss (log scale)\", fontsize=11)\n    plt.grid(True, which=\"both\", linestyle=\":\", linewidth=0.8, alpha=0.7)\n    plt.legend(frameon=False, fontsize=10)\n    plt.tight_layout()\n    plt.savefig(fig_path, dpi=200)\n    plt.close()\n\n    print(f\"\u2705 Saved training curve to {fig_path}\")\n</code></pre>"},{"location":"reference/predictor/","title":"Predictor","text":""},{"location":"reference/predictor/#node_fdm.predictor","title":"<code>predictor</code>","text":"<p>Prediction helper to roll out flight trajectories with trained models.</p>"},{"location":"reference/predictor/#node_fdm.predictor.NodeFDMPredictor","title":"<code>NodeFDMPredictor</code>","text":"<p>Predict flight trajectories using a pretrained FlightDynamicsModelProd.</p> Source code in <code>src/node_fdm/predictor.py</code> <pre><code>class NodeFDMPredictor:\n    \"\"\"Predict flight trajectories using a pretrained FlightDynamicsModelProd.\"\"\"\n\n    def __init__(\n        self,\n        model_cols: list,\n        model_path: Path,\n        dt: float = 4.0,\n        device: str = \"cuda:0\",\n    ):\n        \"\"\"Initialize predictor with model path and column definitions.\n\n        Args:\n            model_cols: Sequence of model column groups (state, control, env, env_extra, derivatives).\n            model_path: Directory containing pretrained model artifacts.\n            dt: Integration timestep used for state propagation.\n            device: Torch device string to run predictions on.\n        \"\"\"\n        self.model_path = Path(model_path)\n        self.x_cols, self.u_cols, self.e0_cols, self.e_cols, self.dx_cols = model_cols\n        self.dt = dt\n        self.device = torch.device(device)\n        self.model = FlightDynamicsModelProd(model_path).to(self.device)\n        self.model.eval()\n\n    @staticmethod\n    def _get_dict(f: pd.DataFrame, cols: List, i: int) -&gt; Dict:\n        \"\"\"Slice a DataFrame row into a dict of tensors keyed by column definitions.\n\n        Args:\n            f: Flight data DataFrame.\n            cols: Column identifiers to extract.\n            i: Row index to slice.\n\n        Returns:\n            Dictionary mapping column identifiers to 1-sample tensors.\n        \"\"\"\n        return {\n            col: torch.tensor(f[col].iloc[i : i + 1].values.astype(np.float32))\n            for col in cols\n        }\n\n    def _get_state(self, f: pd.DataFrame, i: int) -&gt; Dict:\n        \"\"\"Extract state columns at a specific timestep.\n\n        Returns:\n            Dictionary mapping state columns to tensors.\n        \"\"\"\n        return self._get_dict(f, self.x_cols, i)\n\n    def _get_ctrl(self, f: pd.DataFrame, i: int) -&gt; Dict:\n        \"\"\"Extract control columns at a specific timestep.\n\n        Returns:\n            Dictionary mapping control columns to tensors.\n        \"\"\"\n        return self._get_dict(f, self.u_cols, i)\n\n    def _get_env(self, f: pd.DataFrame, i: int) -&gt; Dict:\n        \"\"\"Extract environment columns at a specific timestep.\n\n        Returns:\n            Dictionary mapping environment columns to tensors.\n        \"\"\"\n        return self._get_dict(f, self.e0_cols, i)\n\n    def _next_state(self, current_state: Dict, res_dict: Dict) -&gt; Dict:\n        \"\"\"Advance state using predicted derivatives and configured timestep.\n\n        Args:\n            current_state: Mapping of current state tensors keyed by column.\n            res_dict: Model output containing derivative tensors.\n\n        Returns:\n            Updated state mapping after one integration step.\n        \"\"\"\n        new_state = dict()\n        for x_col, (coeff, dx_col) in zip(self.x_cols, self.dx_cols):\n            new_state[x_col] = current_state[x_col] + coeff * self.dt * res_dict[dx_col]\n        return new_state\n\n    def predict_flight(\n        self, flight_df: pd.DataFrame, add_cols: list = []\n    ) -&gt; pd.DataFrame:\n        \"\"\"Generate model predictions for an entire flight.\n\n        Args:\n            flight_df: Flight measurements DataFrame.\n            add_cols: Optional extra columns to return alongside state predictions.\n\n        Returns:\n            DataFrame containing predicted columns with `pred_` prefix.\n        \"\"\"\n        display_dict = {col: [] for col in self.x_cols + add_cols}\n\n        current_state = self._get_state(flight_df, 0)\n        current_state = {k: v.to(self.device) for k, v in current_state.items()}\n        for i in range(len(flight_df)):\n            input_dict = {\n                **current_state,\n                **self._get_ctrl(flight_df, i),\n                **self._get_env(flight_df, i),\n            }\n            input_dict = {k: v.to(self.device) for k, v in input_dict.items()}\n            res_dict = self.model.forward(input_dict)\n            for col in display_dict.keys():\n                display_dict[col].append(res_dict[col].cpu().detach().numpy())\n            current_state = self._next_state(current_state, res_dict)\n\n        pred_df = pd.DataFrame(\n            {f\"pred_{col}\": np.concatenate(display_dict[col]) for col in display_dict},\n            index=flight_df.index,\n        )\n        return pred_df\n</code></pre>"},{"location":"reference/predictor/#node_fdm.predictor.NodeFDMPredictor.__init__","title":"<code>__init__(model_cols, model_path, dt=4.0, device='cuda:0')</code>","text":"<p>Initialize predictor with model path and column definitions.</p> <p>Parameters:</p> Name Type Description Default <code>model_cols</code> <code>list</code> <p>Sequence of model column groups (state, control, env, env_extra, derivatives).</p> required <code>model_path</code> <code>Path</code> <p>Directory containing pretrained model artifacts.</p> required <code>dt</code> <code>float</code> <p>Integration timestep used for state propagation.</p> <code>4.0</code> <code>device</code> <code>str</code> <p>Torch device string to run predictions on.</p> <code>'cuda:0'</code> Source code in <code>src/node_fdm/predictor.py</code> <pre><code>def __init__(\n    self,\n    model_cols: list,\n    model_path: Path,\n    dt: float = 4.0,\n    device: str = \"cuda:0\",\n):\n    \"\"\"Initialize predictor with model path and column definitions.\n\n    Args:\n        model_cols: Sequence of model column groups (state, control, env, env_extra, derivatives).\n        model_path: Directory containing pretrained model artifacts.\n        dt: Integration timestep used for state propagation.\n        device: Torch device string to run predictions on.\n    \"\"\"\n    self.model_path = Path(model_path)\n    self.x_cols, self.u_cols, self.e0_cols, self.e_cols, self.dx_cols = model_cols\n    self.dt = dt\n    self.device = torch.device(device)\n    self.model = FlightDynamicsModelProd(model_path).to(self.device)\n    self.model.eval()\n</code></pre>"},{"location":"reference/predictor/#node_fdm.predictor.NodeFDMPredictor.predict_flight","title":"<code>predict_flight(flight_df, add_cols=[])</code>","text":"<p>Generate model predictions for an entire flight.</p> <p>Parameters:</p> Name Type Description Default <code>flight_df</code> <code>DataFrame</code> <p>Flight measurements DataFrame.</p> required <code>add_cols</code> <code>list</code> <p>Optional extra columns to return alongside state predictions.</p> <code>[]</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing predicted columns with <code>pred_</code> prefix.</p> Source code in <code>src/node_fdm/predictor.py</code> <pre><code>def predict_flight(\n    self, flight_df: pd.DataFrame, add_cols: list = []\n) -&gt; pd.DataFrame:\n    \"\"\"Generate model predictions for an entire flight.\n\n    Args:\n        flight_df: Flight measurements DataFrame.\n        add_cols: Optional extra columns to return alongside state predictions.\n\n    Returns:\n        DataFrame containing predicted columns with `pred_` prefix.\n    \"\"\"\n    display_dict = {col: [] for col in self.x_cols + add_cols}\n\n    current_state = self._get_state(flight_df, 0)\n    current_state = {k: v.to(self.device) for k, v in current_state.items()}\n    for i in range(len(flight_df)):\n        input_dict = {\n            **current_state,\n            **self._get_ctrl(flight_df, i),\n            **self._get_env(flight_df, i),\n        }\n        input_dict = {k: v.to(self.device) for k, v in input_dict.items()}\n        res_dict = self.model.forward(input_dict)\n        for col in display_dict.keys():\n            display_dict[col].append(res_dict[col].cpu().detach().numpy())\n        current_state = self._next_state(current_state, res_dict)\n\n    pred_df = pd.DataFrame(\n        {f\"pred_{col}\": np.concatenate(display_dict[col]) for col in display_dict},\n        index=flight_df.index,\n    )\n    return pred_df\n</code></pre>"}]}